<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yorozuya</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://invkrh.me/"/>
  <updated>2019-06-09T23:36:51.523Z</updated>
  <id>http://invkrh.me/</id>
  
  <author>
    <name>Hao Ren</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Expectation-Maximization Algorithm Explained</title>
    <link href="http://invkrh.me/2019/05/14/expectation-maximization-explained/"/>
    <id>http://invkrh.me/2019/05/14/expectation-maximization-explained/</id>
    <published>2019-05-14T21:49:26.000Z</published>
    <updated>2019-06-09T23:36:51.523Z</updated>
    
    <content type="html"><![CDATA[<p>The EM algorithm is used to find (local) maximum likelihood parameters of a statistical model in cases where the equations cannot be solved directly. Typically these models involve latent variables in addition to unknown parameters and known data observations. In this post, we will start with how the algorithm works and then prove its correctness, finally we will show a concrete yet the most common use case where the algorithm is applied.</p><a id="more"></a><h1 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h1><p>The typical setting of the EM algorithm is a parameter estimation problem in which we have </p><ul><li>A training set <script type="math/tex">\mathbf{X} = \{ x_1, \ldots, x_n \}</script> consisting of $n$ independent observed data points. Each may be discrete or continuous. Associated with each data point <strong>may be</strong> a vector of observations <script type="math/tex">\mathbf{Y} = \{ y_1, \ldots, y_n \}</script>.</li><li>A set of <strong>unobserved</strong> latent data or missing values <script type="math/tex">\mathbf{Z} = \{ z_1, \ldots, z_n \}</script> associated with each data point. They are discrete, drawn from a fixed number of values, and with one latent variable per observed unit.</li><li>A vector of unknown parameters <script type="math/tex">\boldsymbol{\theta}</script> which are continuous, and are of two kinds: <ul><li>Parameters that are associated with all data points</li><li>Those associated with a specific value of a latent variable (i.e., associated with all data points which corresponding latent variable has that value).</li></ul></li></ul><p>We wish to fit the parameters of a model $p(x ; \boldsymbol{\theta})$ (may or may not include $y$) and the log <a href="https://en.wikipedia.org/wiki/Marginal_distribution" target="_blank" rel="noopener">marginal likelihood</a> function of all data points to maximize is given by</p><script type="math/tex; mode=display">\begin{aligned}    L(\boldsymbol{\theta};\mathbf{X}) &=\sum_{i=1}^{n} \log p(x_i ; \boldsymbol{\theta}) \\    &=\sum_{i=1}^{n} \log \sum_{z_i} p(x_i, z_i ; \boldsymbol{\theta})\end{aligned}</script><p>However, maximizing <script type="math/tex">L(\boldsymbol{\theta};\mathbf{X})</script> explicityly might be difficult because <script type="math/tex">\mathbf{Z}</script> are the latent random variables. The good thing of EM is not to maximizing the log likelihood directly, but instead, it leverages the following (we will see later):</p><script type="math/tex; mode=display">L(\boldsymbol{\theta}; \mathbf{X}, \mathbf{Z}) = \sum_{i=1}^{n} \log p(x_i, z_i ; \boldsymbol{\theta})</script><p>The EM algorithm seeks to find the MLE of the marginal likelihood by iteratively applying these two steps:</p><ul><li><p>Expectation step (E step): Define <script type="math/tex">Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)})</script> as the expected value of the log likelihood function of <script type="math/tex">\boldsymbol{\theta}</script>, with respect to the current conditional distribution of <script type="math/tex">\mathbf{Z}</script>,  given <script type="math/tex">\mathbf{X}</script> and the current estimates of the parameters <script type="math/tex">\boldsymbol{\theta}^{(t)}</script></p><script type="math/tex; mode=display">\begin{aligned}  Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}) &= \mathrm{E}_{z \mid x; \boldsymbol{\theta}^{(t)}} \left [ L(\boldsymbol{\theta}; \mathbf{X}, \mathbf{Z}) \right ] \\  &= \mathrm{E}_{z \mid x; \boldsymbol{\theta}^{(t)}} \left [ \sum_{i=1}^{n} \log p(x_i, z_i ; \boldsymbol{\theta}) \right ] \\  &= \sum_{i=1}^{n} \mathrm{E}_{z_i \mid x_i; \boldsymbol{\theta}^{(t)}} \left [ \log p(x_i, z_i ; \boldsymbol{\theta}) \right ] \\  &= \sum_{i=1}^{n} \sum_{z_i} p(z_i \mid x_i; \boldsymbol{\theta}^{(t)}) \log p(x_i, z_i ; \boldsymbol{\theta})\end{aligned}</script></li><li><p>Maximization step (M step): Find the parameters that maximize this quantity:</p><script type="math/tex; mode=display">\boldsymbol{\theta}^{(t+1)} = \underset{\boldsymbol{\theta}}{\arg \max} \, Q\left(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}\right)</script></li></ul><h1 id="Proof-of-correctness"><a href="#Proof-of-correctness" class="headerlink" title="Proof of correctness"></a>Proof of correctness</h1><p>The trick of EM algorithm is to improve <script type="math/tex">Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)})</script> rather than directly improving <script type="math/tex">L(\boldsymbol{\theta})</script>. Here is shown that improvements to the former imply improvements to the latter.</p><p>For any $z_i$ with non-zero probability $p(z_i \mid x_i; \boldsymbol{\theta})$, we can write the following (based on Bayes’ theorem)</p><script type="math/tex; mode=display">\log p(x_i ; \boldsymbol{\theta})=\log p(x_i, z_i ; \boldsymbol{\theta})-\log p(z_i \mid x_i; \boldsymbol{\theta})</script><p>We take the expectation over possible values of the unknown data $z_i$ under the current parameter estimate $\boldsymbol{\theta}^{(t)}$ by multiplying both sides by $p(z_i \mid x_i; \boldsymbol{\theta}^{(t)})$ and summing (or integrating) over $z$ . The left-hand side is the expectation of a constant, so we get:</p><script type="math/tex; mode=display">\begin{aligned}\log p(x_i ; \boldsymbol{\theta}) &= \sum_{z_i} p \left( z_i \mid x_i; \boldsymbol{\theta}^{(t)} \right) \log p(x_i, z_i ; \boldsymbol{\theta}) - \sum_{z_i} p \left( z_i \mid x_i; \boldsymbol{\theta}^{(t)} \right) \log p(z_i \mid x_i ; \boldsymbol{\theta}) \\ & = G_i \left( \boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)} \right) + H_i \left( \boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)} \right)\end{aligned}</script><p>where <script type="math/tex">H_i \left(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}\right)</script> is defined by the negated sum it is replacing and <script type="math/tex">Q(\cdot) = \sum_{i=1}^{n} G_i(\cdot)</script>.<br>This last equation holds for every value of $\boldsymbol{\theta}$ including <script type="math/tex">\boldsymbol{\theta} = \boldsymbol{\theta}^{(t)}</script>,</p><script type="math/tex; mode=display">\log p\left(x_i \mid \boldsymbol{\theta}^{(t)} \right) = G_i \left( \boldsymbol{\theta}^{(t)} \mid \boldsymbol{\theta}^{(t)} \right) + H_i \left( \boldsymbol{\theta}^{(t)} \mid \boldsymbol{\theta}^{(t)} \right)</script><p>and subtracting this last equation from the previous equation gives</p><script type="math/tex; mode=display">\log p(x_i ; \boldsymbol{\theta}) - \log p\left( x_i ; \boldsymbol{\theta}^{(t)} \right) = G_i \left( \boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)} \right) - G_i \left( \boldsymbol{\theta}^{(t)} \mid \boldsymbol{\theta}^{(t)} \right) + H_i \left( \boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)} \right) - H_i \left( \boldsymbol{\theta}^{(t)} \mid \boldsymbol{\theta}^{(t)} \right)</script><p>Note that</p><ul><li><script type="math/tex">H_i\left(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)} \right)</script> is exactly the cross entropy of the distribution $z_i \mid x_i; \boldsymbol{\theta}^{(t)}$ with the other distribution $z_i \mid x_i; \boldsymbol{\theta}$</li><li><script type="math/tex">H_i\left(\boldsymbol{\theta}^{(t)} \mid \boldsymbol{\theta}^{(t)} \right)</script> is exactly the entropy of the distribution of $z_i \mid x_i; \boldsymbol{\theta}^{(t)}$</li></ul><p>Based on <a href="https://en.wikipedia.org/wiki/Gibbs%27_inequality" target="_blank" rel="noopener">Gibbs’ inequality</a>: the information entropy of a distribution P is less than or equal to its cross entropy with any other distribution Q, which means</p><script type="math/tex; mode=display">H_i\left(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)} \right) \geq H_i\left(\boldsymbol{\theta}^{(t)} \mid \boldsymbol{\theta}^{(t)} \right)</script><p>Hence, we can conclude that</p><script type="math/tex; mode=display">\log p(x_i ; \boldsymbol{\theta}) - \log p\left( x_i ; \boldsymbol{\theta}^{(t)} \right) \geq G_i \left( \boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)} \right) - G_i \left( \boldsymbol{\theta}^{(t)} \mid \boldsymbol{\theta}^{(t)} \right)</script><p>By summation of all the data point</p><script type="math/tex; mode=display">\sum_{i=1}^{n} \log p(x_i ; \boldsymbol{\theta}) - \sum_{i=1}^{n} \log p\left( x_i ; \boldsymbol{\theta}^{(t)} \right)\geq \sum_{i=1}^{n} G_i \left( \boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)} \right) - \sum_{i=1}^{n} G_i \left( \boldsymbol{\theta}^{(t)} \mid \boldsymbol{\theta}^{(t)} \right)</script><p>which is equivalent to</p><script type="math/tex; mode=display">L(\boldsymbol{\theta}) - L\left(\boldsymbol{\theta}^{(t)}\right) \geq Q \left( \boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)} \right) - Q \left( \boldsymbol{\theta}^{(t)} \mid \boldsymbol{\theta}^{(t)} \right)</script><p>In words, choosing $\boldsymbol{\theta}$ to improve $Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)})$ causes $L(\boldsymbol{\theta})$ to improve at least as much.</p><p>According to this <a href="./em.pdf">note</a>, you can also prove this by using <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" target="_blank" rel="noopener">Jensen’s equality</a></p><h1 id="Gaussian-Mixture-Model-GMM"><a href="#Gaussian-Mixture-Model-GMM" class="headerlink" title="Gaussian Mixture Model (GMM)"></a>Gaussian Mixture Model (GMM)</h1><h2 id="Settings"><a href="#Settings" class="headerlink" title="Settings"></a>Settings</h2><p>A typical application of EM algorithm is the gaussian mixture model (GMM) which is a clustering problem as the following:</p><ul><li>A training set <script type="math/tex">\mathbf{X} = \{ x_1, \ldots, x_n \}</script> which is a sample of $n$ independent observations from a mixture of $k$ multivariate normal distributions of dimension $d$. (since we are in the unsupervised learning setting, these points do not come with any labels.)</li><li>Associated with the latent variables <script type="math/tex">\mathbf{Z} = \{ z_1,z_2,\ldots,z_n \}</script> that determine the component from which the observation originates. <script type="math/tex">z_i \sim \text { Multinomial }(\phi)</script> where <script type="math/tex">\phi_j \geq 0, \sum_{j=1}^{k} \phi_j=1</script> and the parameter <script type="math/tex">\phi_j</script> gives <script type="math/tex">p(z_i=j)</script>.</li><li>The model posits that each <script type="math/tex">x_i</script> was generated by randomly choosing <script type="math/tex">z_i</script> from <script type="math/tex">{1, \ldots, k}</script>, and then <script type="math/tex">x_i</script> was drawn from one of <script type="math/tex">k</script> Gaussians depending on <script type="math/tex">z_i</script>. Then, <script type="math/tex">x_i | z_i=j \sim \mathcal{N}\left(\mu_{j}, \Sigma_{j}\right)</script></li><li>The parameters to estimate <script type="math/tex">\boldsymbol{\theta} = \{\phi_j, \mu_j, \Sigma_j | j \in 1,\ldots,k\}</script>. For simplicity, <script type="math/tex">\theta_j = {\phi_j, \mu_j, \Sigma_j}</script></li></ul><p>And we wish to model the data by specifying the joint distribution <script type="math/tex">p(x_i, z_i)</script>. Hence, the log likelihood is</p><script type="math/tex; mode=display">\begin{aligned}L(\boldsymbol{\theta} ; \mathbf{X})&= \sum_{i=1}^{n} \log \sum_{j=1}^{k} p(x_i, z_i = j; \theta_j) \\&= \sum_{i=1}^{n} \log \sum_{j=1}^{k} p(x_i \mid z_i = j; \mu_j, \Sigma_j) \cdot p(z_i = j; \phi_j) \\&= \sum_{i=1}^{n} \log \sum_{j=1}^{k} f\left(x_i ; \mu_j, \Sigma_j\right) \cdot \phi_j\end{aligned}</script><p>However, if we set to zero the derivatives of this formula with respect to the parameters and try to solve, we’ll find that it is not possible to find the maximum likelihood estimates of the parameters in closed form. (Try this yourself at home.) This is where the EM algorithm comes in. </p><h2 id="E-step"><a href="#E-step" class="headerlink" title="E-step"></a>E-step</h2><script type="math/tex; mode=display">L(\theta ; \mathbf{X}, \mathbf{Z}) = \sum_{i=1}^{n} \log f\left(x_i ; \mu_j, \Sigma_j\right) \cdot \phi_{j}</script><p>where</p><script type="math/tex; mode=display">f\left(x_i ; \mu_j, \Sigma_j\right) = \frac{1}{(2 \pi)^{d / 2}\left|\Sigma_{j}\right|^{1 / 2}} \exp \left(-\frac{1}{2}\left(x_i-\mu_{j}\right)^{T} \Sigma_{j}^{-1}\left(x_i-\mu_{j}\right)\right)</script><script type="math/tex; mode=display">Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}) = \sum_{i=1}^{n} \sum_{j=1}^{k} p(z_i = j \mid x_i; \boldsymbol{\theta}^{(t)}) \cdot \log f\left(x_i ; \mu_j, \Sigma_j\right) \cdot \phi_j</script><p>Given our current estimate of the parameters <script type="math/tex">\theta^{(t)}</script>, the conditional distribution of the <script type="math/tex">z_i</script> is determined by Bayes theorem to be normalized Gaussian density weighted by <script type="math/tex">\phi_j</script>:</p><script type="math/tex; mode=display">\begin{aligned}p(z_i = j \mid x_i; \boldsymbol{\theta}^{(t)}) &= \frac{p(x_i, z_i = j ; \boldsymbol{\theta}^{(t)})}{p(x_i; \boldsymbol{\theta}^{(t)})} \\&= \frac{p(x_i, z_i = j ; \boldsymbol{\theta}^{(t)})}{\sum_{l = 1}^{k} p(x_i, z_i = l; \boldsymbol{\theta}^{(t)})} \\&= \frac{f\left(x_i ; \mu_j^{(t)}, \Sigma_j^{(t)}\right) \cdot \phi_{j}^{(t)}}{\sum_{l = 1}^{k} f\left(x_i ; \mu_l^{(t)}, \Sigma_l^{(t)}\right) \cdot \phi_{l}^{(t)}}\end{aligned}</script><p>For simplicity, let’s denote it by <script type="math/tex">p_{ji}^{(t)}</script></p><h2 id="M-step"><a href="#M-step" class="headerlink" title="M-step"></a>M-step</h2><p>We need to maximize, with respect to our parameters <script type="math/tex">\boldsymbol{\theta}^{(t)}</script>, the quantity</p><script type="math/tex; mode=display">\begin{aligned}Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}) &= \sum_{i=1}^{n}\sum_{j=1}^{k} p_{ji}^{(t)} \log \frac{1}{(2 \pi)^{d / 2}\left|\Sigma_{j}\right|^{1 / 2}} \exp \left(-\frac{1}{2}\left(x_i-\mu_{j}\right)^{T} \Sigma_{j}^{-1}\left(x_i-\mu_{j}\right)\right) \cdot \phi_{j} \\&= \sum_{i=1}^{n}\sum_{j=1}^{k} p_{ji}^{(t)} \left[\log \phi_j - \frac{1}{2} \log \left|\Sigma_{j}\right| - \frac{1}{2}(x_i-\mu_j)^T \Sigma_{j}^{-1}(x_i - \mu_j) - \frac{d}{2} \log(2\pi) \right]\end{aligned}</script><ul><li><p>Update <script type="math/tex">\phi_j</script>:<br>Grouping together only the terms that depend on <script type="math/tex">\phi_j</script>, we find that we need to maximize</p><script type="math/tex; mode=display">\sum_{i=1}^{n}\sum_{j=1}^{k} p_{ji}^{(t)} \log \phi_j</script><p>with subject to</p><script type="math/tex; mode=display">\sum_{j=1}^{k} \phi_j = 1</script><p>So we construct the Lagrangian</p><script type="math/tex; mode=display">\mathcal{L}(\phi) = \sum_{i=1}^{n}\sum_{j=1}^{k} p_{ji}^{(t)} \log \phi_j + \lambda \left( \sum_{j=1}^{k} \phi_j - 1 \right),</script><p>where <script type="math/tex">\lambda</script> is the Lagrange multiplier. Taking derivative, we find</p><script type="math/tex; mode=display">\frac{\partial}{\partial \phi_{j}} \mathcal{L}(\phi)=\sum_{i=1}^{n} \frac{p_{ji}^{(t)}}{\phi_{j}}+\lambda</script><p>Setting this to zero and solving, we get</p><script type="math/tex; mode=display">\phi_{j}=\frac{\sum_{i=1}^{n} p_{ji}^{(t)}}{-\lambda}</script><p>Using the constraint that <script type="math/tex">\sum_{j=1}^{k} \phi_j = 1</script> and knowing the fact that <script type="math/tex">\sum_{j=1}^{k} p_{ji}^{(t)} = 1</script> (probabilities sum to 1), we easily find:</p><script type="math/tex; mode=display">-\lambda=\sum_{i=1}^{n} \sum_{j=1}^{k} p_{ji}^{(t)} = \sum_{i=1}^{n} 1= n</script><p>We therefore have updates for the parameters <script type="math/tex">\phi_j</script>:</p><script type="math/tex; mode=display">\phi_{j} :=\frac{1}{n} \sum_{i=1}^{n} p_{ji}^{(t)}</script></li><li><p>Update <script type="math/tex">\mu_j</script></p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial}{\partial \mu_{j}} Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}) &= \frac{\partial}{\partial \mu_{j}} \sum_{i=1}^{n} -\frac{1}{2} p_{ji}^{(t)} (x_i-\mu_j)^T \Sigma_{j}^{-1}(x_i - \mu_j) \\&= \frac{1}{2} \sum_{i=1}^{n} p_{ji}^{(t)} \dfrac{\partial}{\partial \mu_{j}} \left( 2 \mu_j^T\Sigma_j^{-1}x_i - \mu_j^T\Sigma_j^{-1}\mu_j \right) \\&= \sum_{i=1}^{n} p_{ji}^{(t)} \left( \Sigma_j^{-1} x_i - \Sigma_j^{-1}\mu_j \right)\end{aligned}</script><p>Setting this to zero and solving for <script type="math/tex">\mu_j</script> therefore yields the update rule</p><script type="math/tex; mode=display">\mu_{j} :=\frac{\sum_{i=1}^{n} p_{ji}^{(t)} x_i}{\sum_{i=1}^{n} p_{ji}^{(t)}}</script></li><li><p>Update <script type="math/tex">\Sigma_j</script></p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial}{\partial \Sigma_{j}} Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}) &= \frac{\partial}{\partial \Sigma_{j}} \sum_{i=1}^{n} -\frac{1}{2} p_{ji}^{(t)} \left[ \log\left|\Sigma_{j}\right| + (x_i-\mu_j)^T \Sigma_{j}^{-1}(x_i - \mu_j)\right] \\&= -\frac{1}{2} \sum_{i=1}^{n} p_{ji}^{(t)} \dfrac{\partial}{\partial \mu_{j}} \left[ \log\left|\Sigma_{j}\right| + (x_i-\mu_j)^T \Sigma_{j}^{-1}(x_i - \mu_j)\right] \\&= -\frac{1}{2} \sum_{i=1}^{n} p_{ji}^{(t)} \left( \Sigma_{j}^{-1} - (x_i-\mu_j)(x_i - \mu_j)^T \Sigma_{j}^{-2} \right) \\\end{aligned}</script><p>Setting this to zero and solving for <script type="math/tex">\Sigma_j</script> therefore yields the update rule</p><script type="math/tex; mode=display">\Sigma_{j} :=\frac{\sum_{i=1}^{n} p_{ji}^{(t)} (x_i-\mu_j)(x_i - \mu_j)^T}{\sum_{i=1}^{n} p_{ji}^{(t)}}</script></li></ul><blockquote><p>If you are not familiar with the matrix / vector derivative calculus, you may refer to this <a href="./matrix.pdf">cheat sheet</a>.</p></blockquote><h2 id="More-about-GMM"><a href="#More-about-GMM" class="headerlink" title="More about GMM"></a>More about GMM</h2><p>Compared to K-Means which uses hard assignment, GMM uses soft assignment which a good way to represent uncertainty. But GMM is limited in its number of features since it requires storing a covariance matrix which has size quadratic in the number of features. Even when the number of features does not exceed this limit, this algorithm may perform poorly on high-dimensional data.<br>This is due to high-dimensional data:</p><ul><li>making it difficult to cluster at all (based on statistical/theoretical arguments) </li><li>numerical issues with Gaussian distributions</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;The EM algorithm is used to find (local) maximum likelihood parameters of a statistical model in cases where the equations cannot be solved directly. Typically these models involve latent variables in addition to unknown parameters and known data observations. In this post, we will start with how the algorithm works and then prove its correctness, finally we will show a concrete yet the most common use case where the algorithm is applied.&lt;/p&gt;
    
    </summary>
    
      <category term="Machine Learning" scheme="http://invkrh.me/categories/Machine-Learning/"/>
    
    
      <category term="Maths" scheme="http://invkrh.me/tags/Maths/"/>
    
  </entry>
  
  <entry>
    <title>无水葱油鸡</title>
    <link href="http://invkrh.me/2019/05/14/wu-shui-cong-you-ji/"/>
    <id>http://invkrh.me/2019/05/14/wu-shui-cong-you-ji/</id>
    <published>2019-05-14T21:09:25.000Z</published>
    <updated>2019-05-14T21:42:39.031Z</updated>
    
    <content type="html"><![CDATA[<h3 id="用时"><a href="#用时" class="headerlink" title="用时"></a>用时</h3><p>1小时</p><h3 id="必备厨具"><a href="#必备厨具" class="headerlink" title="必备厨具"></a>必备厨具</h3><p>Le Creuset 铸铁炖锅</p><a id="more"></a><h3 id="原料"><a href="#原料" class="headerlink" title="原料"></a>原料</h3><p>一只鸡（1.5kg）<br>盐<br>胡椒<br>葱<br>姜<br>香菜<br>花椒<br>小米椒<br>料酒<br>香油<br>生抽 或 蒸鱼豉油</p><h3 id="做法"><a href="#做法" class="headerlink" title="做法"></a>做法</h3><ol><li>鸡去脖子和屁股，洗干净不必控水，里外抹上盐、黑胡椒， 腌制半小时。</li><li>姜切片，一部分葱切段，锅底抹一层油。放入姜片和葱段铺满锅底。另一部分葱和小米椒切碎备用。</li><li>将腌制后的童子鸡放入锅中，淋上三大勺料酒。</li><li>盖上锅盖，中火开始煮，煮至有蒸汽出来后，调整火力至小火。</li><li>小火20分钟后，如筷子可以轻松戳透鸡肉，说明鸡煮好了。</li><li>别关火，倒入少许生抽或者蒸鱼豉油。</li><li>倒入切碎的葱和小米椒，一把香菜，均匀撒在鸡身上。</li><li>炒锅加热，倒入适量油，加入一小撮花椒粒，加一勺香油，烧到滚烫冒烟，关火，花椒粒弃用。</li><li>把滚烫的油浇到鸡上，烫熟葱和小米椒。</li></ol><h3 id="成品"><a href="#成品" class="headerlink" title="成品"></a>成品</h3><img src="/2019/05/14/wu-shui-cong-you-ji/result.jpeg">]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;用时&quot;&gt;&lt;a href=&quot;#用时&quot; class=&quot;headerlink&quot; title=&quot;用时&quot;&gt;&lt;/a&gt;用时&lt;/h3&gt;&lt;p&gt;1小时&lt;/p&gt;
&lt;h3 id=&quot;必备厨具&quot;&gt;&lt;a href=&quot;#必备厨具&quot; class=&quot;headerlink&quot; title=&quot;必备厨具&quot;&gt;&lt;/a&gt;必备厨具&lt;/h3&gt;&lt;p&gt;Le Creuset 铸铁炖锅&lt;/p&gt;
    
    </summary>
    
      <category term="Cuisine" scheme="http://invkrh.me/categories/Cuisine/"/>
    
    
      <category term="Plat" scheme="http://invkrh.me/tags/Plat/"/>
    
      <category term="Poulet" scheme="http://invkrh.me/tags/Poulet/"/>
    
  </entry>
  
  <entry>
    <title>Google Hash Code 2019</title>
    <link href="http://invkrh.me/2019/04/13/google-hash-code-2019/"/>
    <id>http://invkrh.me/2019/04/13/google-hash-code-2019/</id>
    <published>2019-04-12T22:00:00.000Z</published>
    <updated>2019-05-16T21:36:37.418Z</updated>
    
    <content type="html"><![CDATA[<p>My thoughts on the qualification round of <a href="/2019/04/13/google-hash-code-2019/qualification.pdf" title="Google Hash Code 2019">Google Hash Code 2019</a>. If you are interested in the past Online Qualification Round and Finals problem statements, you should check this <a href="https://codingcompetitions.withgoogle.com/hashcode/archive" target="_blank" rel="noopener">page</a>.</p><a id="more"></a><h2 id="General-information"><a href="#General-information" class="headerlink" title="General information"></a>General information</h2><p>Based on the problem statement, your team of 4 need to solve 5 different cases of the problem. Each case is represented by an input text file. An output file should be generated for each input file and submitted to the Judge System. The Judge System will give a score for each output file. You can submit as many as you can, but only the highest score will be counted for each case. The summation of all the scores is the final score of the team. The top teams will be invited to the final round which will be held at Google Dublin on April 27, 2019.</p><blockquote><p>The problem is always NP-hard, which means that you can never find the best solution in polynomial time. Fortunately, you can make the result better and better. The <strong><a href="https://en.wikipedia.org/wiki/Greedy_algorithm" target="_blank" rel="noopener">greedy algorithm</a></strong> and <strong><a href="https://en.wikipedia.org/wiki/Dynamic_programming" target="_blank" rel="noopener">dynamic programming</a></strong> are very helpful.</p></blockquote><h2 id="Problem-statement"><a href="#Problem-statement" class="headerlink" title="Problem statement"></a>Problem statement</h2><ul><li>A slideshow contains a list of slides</li><li>A slide contains either <strong>one</strong> horizontal photo or <strong>two</strong> vertical photos</li><li>A photo contains a list of tags (string type)</li><li>The tag set of a <strong>slide</strong> is the set of all tags of the photos in the slide <ul><li>If the slide containing 1 horizontal photos: <script type="math/tex">S = tags(h)</script></li><li>If the slide containing 2 vertical photos: <script type="math/tex">S = tags(v_1) \cup tags(v_2)</script></li></ul></li><li>The slideshow is scored based on how interesting the transitions between each pair of subsequent slides are</li><li>For two subsequent slides <script type="math/tex">S_i</script> and <script type="math/tex">S_{i+1}</script>, the score is the minimum (the smallest number of the three) of:<ol><li>the number of common tags between <script type="math/tex">S_i</script> and <script type="math/tex">S_{i+1}</script></li><li>the number of tags in <script type="math/tex">S_i</script> but not in <script type="math/tex">S_{i+1}</script></li><li>the number of tags in <script type="math/tex">S_{i+1}</script> but not in <script type="math/tex">S_i</script></li></ol></li><li>Score formula: <script type="math/tex">min(\vert S_i \cap S_{i+1} \vert, \vert S_i \setminus S_{i+1} \vert, \vert S_{i+1} \setminus S_i \vert)</script></li></ul><blockquote><p>For simplicity, <strong>horizontal slide</strong> refers to a slide containing 1 horizontal slide and <strong>vertical slide</strong> refers to a slide containing 2 vertical slides</p></blockquote><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>First of all, let’s look at how the tags are distributed in the input files. This may help us understand the problem better.</p><div class="table-container"><table><thead><tr><th>Orientation</th><th>Metric</th><th>A</th><th>B</th><th>C</th><th>D</th><th>E</th></tr></thead><tbody><tr><td>H</td><td># photos</td><td>2</td><td>80000</td><td>500</td><td>30000</td><td>NA</td></tr><tr><td>H</td><td># distict tags</td><td>4</td><td>840000</td><td>1558</td><td>220</td><td>NA</td></tr><tr><td>H</td><td># tags per photo</td><td>2.5</td><td>18.0</td><td>9.43</td><td>10.04</td><td>NA</td></tr><tr><td>H</td><td># photos per tag</td><td>1.25</td><td>1.71</td><td>3.02</td><td>1369.09</td><td>NA</td></tr><tr><td>V</td><td># photos</td><td>2</td><td>NA</td><td>500</td><td>60000</td><td>80000</td></tr><tr><td>V</td><td># distict tags</td><td>3</td><td>NA</td><td>1548</td><td>220</td><td>500</td></tr><tr><td>V</td><td># tags per photo</td><td>2</td><td>NA</td><td>9.52</td><td>10.01</td><td>19.09</td></tr><tr><td>V</td><td># photos per tag</td><td>1.3</td><td>NA</td><td>3.07</td><td>2732.06</td><td>3055.96</td></tr></tbody></table></div><p>Some observations:</p><ul><li>B, D, E will come up with a large amount of score because there are many photos in these cases</li><li>B contains horizontal photos only, which would be a good start point to try ideas</li><li>B has many distinct tags which are far more than D, E</li><li>In B and E, photos have twice more tags than D</li><li>In term of photos per tag, D and E are thousands of times larger than B</li><li>E contains vertical photos only which would be the last challenge</li></ul><h3 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h3><p>Actually, we can consider the problem as building a sequence of slides and maximizing its overall transition score. Given the tag set of the previous slide, we can find a slide which maximizes the current transition. If every subsequent transition is optimized, we could eventually optimize the final score.</p><h3 id="Greedy-Algorithm"><a href="#Greedy-Algorithm" class="headerlink" title="Greedy Algorithm"></a>Greedy Algorithm</h3><pre><code>prevSlide &lt;- create a slide of 1 horizontal unused photo or 2 vertical unused photosresult += prevSlidewhile some photos are unused:    (hSlide, hScore) &lt;- Best horizontal slide w.r.t prevSlide    (vSlide, vScore) &lt;- Best vertical slide w.r.t prevSlide    if both hSlide and vSlide are found, then        if hScove &gt;= vScore, then            prevSlide &lt;- hScove        else            prevSlide &lt;- vScove    else if hSlide not found, then        prevSlide &lt;- vSlide    else if vSlide not found, then        prevSlide &lt;- hSlide    else        preSlide &lt;- create a slide of 1 horizontal unused photo or 2 vertical unused photos    res += prevSlidedonereturn result</code></pre><h4 id="How-to-find-the-best-horizontal-slide-w-r-t-the-previous-slide"><a href="#How-to-find-the-best-horizontal-slide-w-r-t-the-previous-slide" class="headerlink" title="How to find the best horizontal slide w.r.t the previous slide?"></a>How to find the best horizontal slide w.r.t the previous slide?</h4><p>Obviously, we just iterate over the horizontal photos <script type="math/tex">h_i</script> and take the photo with the highest score after the previous slide <script type="math/tex">p</script>.</p><script type="math/tex; mode=display">h_{i}^{*} = \underset{h_i \in H}{\operatorname{argmax}} score(p, h_i)</script><p>Actually, we don’t need to search all the photos in the pool of horizontal photos <code>hPool</code>, instead, we only need to look for those having at least one common tag with the previous slide, otherwise, the score must be zero. The search space is restricted in those <code>candidates</code></p><p>Essentially, we can keep track of the photos sharing the same tag. We store this information in a <code>HashMap[String, HashSet[Photo]]</code> call <code>hDict</code>, which is a mapping from tag to a list of photos containing the tag.</p><h4 id="How-to-find-the-best-vertical-slide-w-r-t-the-previous-slide"><a href="#How-to-find-the-best-vertical-slide-w-r-t-the-previous-slide" class="headerlink" title="How to find the best vertical slide w.r.t the previous slide?"></a>How to find the best vertical slide w.r.t the previous slide?</h4><p>This is the most tricky part of the problem.</p><p>One solution: we can iterate over all the combination of 2 vertical photos and choose the best vertical slide as we did for horizontal slide. However, the time complexity for each step is very high, <script type="math/tex">O(N_v^2)</script>. If you compute the <code>candidates</code> size for each previous slides, we will find that it is, in average, about <strong>7200</strong> vertical photos in case D and <strong>60000</strong> in case E. The brute force solution may work for D, but never works for E.</p><p>A more reasonable solution is heuristic. Given a previous slide, we can first take the best vertical <strong>photo</strong>, denoted as <script type="math/tex">v_i</script>, and then find the second vertical photo <script type="math/tex">v_j</script> maximizing the score between the previous slide <script type="math/tex">p</script> and the vertical slide of <script type="math/tex">v_i</script> and <script type="math/tex">v_j</script>.</p><script type="math/tex; mode=display">v_{i}^{*} = \underset{v_i \in V}{\operatorname{argmax}} score(p, v_i)</script><script type="math/tex; mode=display">v_{j}^{*} = \underset{v_j \in (V - v_i^*)}{\operatorname{argmax}} score(p, v_{i}^{*} + v_j)</script><h4 id="Can-we-do-better"><a href="#Can-we-do-better" class="headerlink" title="Can we do better?"></a>Can we do better?</h4><p>We notice that the sequence can be built in parallel. More precisely, we can hash partition the input photos into <script type="math/tex">M</script> blocks. Each block of photos can be used to build a sub-sequence independently, and then all the sub-sequences could be joined into a sequence. This is a classical map-reduce pattern which cam largely improve the performance. </p><h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><p>You may find the code here (comments included)<br><a href="https://github.com/invkrh/google-hash-code/blob/master/src/main/scala/q2019/SlideShowSolver.scala" target="_blank" rel="noopener">https://github.com/invkrh/google-hash-code/blob/master/src/main/scala/q2019/SlideShowSolver.scala</a></p><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><p>With the heuristic algorithm and parallelization trick, we can obtain the following result. The final score is <strong>1112255</strong> which around top 20 in the qualification round, see <a href="https://codingcompetitions.withgoogle.com/hashcode/archive/2019" target="_blank" rel="noopener">here</a>. Moreover, the time for D and E are no more than 5 mins which are also acceptable during the competition.</p><pre><code class="lang-python">Case: a_exampleTime: 119 msScore: 2Case: b_lovely_landscapesTime: 17569 msScore: 201792Case: c_memorable_momentsTime: 165 msScore: 1736Case: d_pet_picturesTime: 228491 msScore: 428041Case: e_shiny_selfiesTime: 294582 msScore: 480684Final score = 1112255</code></pre><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>The competition has been well organized as usual. However, I am a little disappointed at the problem itself. The provided data set is very easy to be cracked brute forced. One can just randomize the slide sequence several times and return the best one. Some teams did that and had a good result. Fortunately, these randomized brute force solutions are not good enough for the finalist. Anyway, this reminds me again the maxim <strong>Done is better than perfect</strong>.</p><h2 id="Auxiliary"><a href="#Auxiliary" class="headerlink" title="Auxiliary"></a>Auxiliary</h2><p>The CPU information of the computer on which the programme has been run.</p><pre><code class="lang-bash">$ lscpu   Architecture:          x86_64CPU op-mode(s):        32-bit, 64-bitByte Order:            Little EndianCPU(s):                8On-line CPU(s) list:   0-7Thread(s) per core:    2Core(s) per socket:    4Socket(s):             1NUMA node(s):          1Vendor ID:             GenuineIntelCPU family:            6Model:                 94Model name:            Intel(R) Xeon(R) CPU E3-1270 v5 @ 3.60GHzStepping:              3CPU MHz:               800.015CPU max MHz:           4000.0000CPU min MHz:           800.0000...</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;My thoughts on the qualification round of &lt;a href=&quot;/2019/04/13/google-hash-code-2019/qualification.pdf&quot; title=&quot;Google Hash Code 2019&quot;&gt;Google Hash Code 2019&lt;/a&gt;. If you are interested in the past Online Qualification Round and Finals problem statements, you should check this &lt;a href=&quot;https://codingcompetitions.withgoogle.com/hashcode/archive&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;page&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
      <category term="Competition" scheme="http://invkrh.me/categories/Competition/"/>
    
    
      <category term="Algorithm" scheme="http://invkrh.me/tags/Algorithm/"/>
    
      <category term="Coding" scheme="http://invkrh.me/tags/Coding/"/>
    
  </entry>
  
  <entry>
    <title>Tensorflow Specialization Learning Note</title>
    <link href="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/"/>
    <id>http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/</id>
    <published>2019-03-11T22:03:33.000Z</published>
    <updated>2019-05-07T19:56:06.056Z</updated>
    
    <content type="html"><![CDATA[<p>A learning note of the coursera specialization <strong><a href="https://www.deeplearning.ai/tensorflow-specialization/" target="_blank" rel="noopener">TensorFlow: From Basics to Mastery</a></strong> given by <a href="http://deeplearning.ai" target="_blank" rel="noopener">deeplearning.ai</a>.</p><ul><li>Course 1: Introduction to TensorFlow for AI, ML and DL</li><li>Course 2: Convolutional Neural Networks in TensorFlow</li><li>Coming soon …</li></ul><a id="more"></a><h1 id="C1W1-A-New-Programming-Paradigm"><a href="#C1W1-A-New-Programming-Paradigm" class="headerlink" title="C1W1: A New Programming Paradigm"></a>C1W1: A New Programming Paradigm</h1><h2 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h2><h3 id="New-programming-paradigm"><a href="#New-programming-paradigm" class="headerlink" title="New programming paradigm"></a>New programming paradigm</h3><div class="table-container"><table><thead><tr><th></th><th>input</th><th>output</th></tr></thead><tbody><tr><td>Triditional Programming</td><td>Rules, Data</td><td>Answers</td></tr><tr><td>Machine Learning</td><td>Answers, Data</td><td>Rules</td></tr></tbody></table></div><h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><h3 id="How-to-fit-a-line"><a href="#How-to-fit-a-line" class="headerlink" title="How to fit a line"></a>How to fit a line</h3><pre><code class="lang-python">import tensorflow as tfimport numpy as npfrom tensorflow import kerasmodel = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])model.compile(optimizer=&#39;sgd&#39;, loss=&#39;mean_squared_error&#39;)xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)model.fit(xs, ys, epochs=500)print(model.predict([10.0]))</code></pre><p>The predicted value is not 19.0 but a little under. It is because neural networks deal with probabilities, so given the data that we fed the NN with, it calculated that there is a very high probability that the relationship between $X$ and $Y$ is $Y=2X-1$, but with only 6 data points we can’t know for sure. As a result, the result for 10 is very close to 19, but not necessarily 19.</p><hr><h1 id="C1W2-Introduction-to-Computer-Vision"><a href="#C1W2-Introduction-to-Computer-Vision" class="headerlink" title="C1W2: Introduction to Computer Vision"></a>C1W2: Introduction to Computer Vision</h1><h2 id="Note-1"><a href="#Note-1" class="headerlink" title="Note"></a>Note</h2><h3 id="Why-are-the-labels-numbers-instead-of-words"><a href="#Why-are-the-labels-numbers-instead-of-words" class="headerlink" title="Why are the labels numbers instead of words"></a>Why are the labels numbers instead of words</h3><p>Using a number is a first step in avoiding bias — instead of labelling it with words in a specific language and excluding people who don’t speak that language! You can learn more about bias and techniques to avoid it <a href="https://developers.google.com/machine-learning/fairness-overview/" target="_blank" rel="noopener">here</a>.</p><h3 id="What-is-cross-entropy-CE"><a href="#What-is-cross-entropy-CE" class="headerlink" title="What is cross entropy (CE)"></a>What is cross entropy (CE)</h3><script type="math/tex; mode=display">CE = - \sum_{i=0}^{C - 1} y_i \cdot log( f(\vec{x_i}) )</script><p>where </p><ul><li>$C$: the number of classes</li><li>$\vec{x_i}$: the feature vector of the example $i$</li><li>$y_i$: the label of the example $i$</li><li>$f$: the learned prediction function which takes the feacture vector $\vec{x_i}$ and returns the probability of being class $y_i$</li></ul><p>When $c = 2$</p><script type="math/tex; mode=display">CE = - \big[ y_i \cdot log( p_i ) + (1 - y_i) \cdot log( 1 - p_i ) \big]</script><h3 id="Difference-between-categorical-crossentropy-and-sparse-categorical-crossentropy"><a href="#Difference-between-categorical-crossentropy-and-sparse-categorical-crossentropy" class="headerlink" title="Difference between categorical_crossentropy and sparse_categorical_crossentropy"></a>Difference between <code>categorical_crossentropy</code> and <code>sparse_categorical_crossentropy</code></h3><ul><li>If your targets are one-hot encoded, use categorical_crossentropy.<br>Examples of one-hot encodings:<pre><code>[1,0,0][0,1,0][0,0,1]</code></pre></li><li>But if your targets are integers, use sparse_categorical_crossentropy.<br>Examples of integer encodings (for the sake of completion):<pre><code>123</code></pre></li></ul><h2 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h2><pre><code class="lang-python"># Early stoppingclass myCallback(tf.keras.callbacks.Callback):  def on_epoch_end(self, epoch, logs={}):    if(logs.get(&#39;loss&#39;)&lt;0.4):      print(&quot;\nReached 60% accuracy so cancelling training!&quot;)      self.model.stop_training = Truecallbacks = myCallback()mnist = tf.keras.datasets.fashion_mnist(training_images, training_labels), (test_images, test_labels) = mnist.load_data()# Data normalizationtraining_images  = training_images / 255.0test_images = test_images / 255.0model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),                                     tf.keras.layers.Dense(128, activation=tf.nn.relu),                                     tf.keras.layers.Dense(10, activation=tf.nn.softmax)])model.compile(optimizer = &#39;adam&#39;,              loss = &#39;sparse_categorical_crossentropy&#39;,              metrics=[&#39;accuracy&#39;])model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])model.evaluate(test_images, test_labels)</code></pre><hr><h1 id="C1W3-Enhancing-Vision-with-Convolutional-Neural-Networks"><a href="#C1W3-Enhancing-Vision-with-Convolutional-Neural-Networks" class="headerlink" title="C1W3: Enhancing Vision with Convolutional Neural Networks"></a>C1W3: Enhancing Vision with Convolutional Neural Networks</h1><h2 id="Note-2"><a href="#Note-2" class="headerlink" title="Note"></a>Note</h2><h3 id="Convolution-Layer"><a href="#Convolution-Layer" class="headerlink" title="Convolution Layer"></a>Convolution Layer</h3><p>Each kernal is an edge detector which is perfect for computer vision, because often it’s features that can get highlighted like this that distinguish one item for another, and the amount of information needed is then much less…because you’ll just train on the highlighted features.</p><h3 id="MaxPooling-Layer"><a href="#MaxPooling-Layer" class="headerlink" title="MaxPooling Layer"></a>MaxPooling Layer</h3><p>The convolution layer is followed by a MaxPooling layer which is then designed to compress the image, while maintaining the content of the features that were highlighted by the convolution</p><h3 id="Why-CNN-works"><a href="#Why-CNN-works" class="headerlink" title="Why CNN works"></a>Why CNN works</h3><p>CNN tries different filters on the image and learning which ones work when looking at the training data. As a result, when it works, you’ll have greatly reduced information passing through the network, but because it isolates and identifies features, you can also get increased accuracy</p><h2 id="Code-2"><a href="#Code-2" class="headerlink" title="Code"></a>Code</h2><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><pre><code class="lang-Python"># Reshape to a 4D tensor, otherwise the Convolutions do not recognize the shapetraining_images=training_images.reshape(60000, 28, 28, 1)training_images=training_images / 255.0test_images = test_images.reshape(10000, 28, 28, 1)test_images=test_images/255.0# 2-convolution-layer NNmodel = tf.keras.models.Sequential([  # default: strides = 1, padding = &#39;valid&#39;  tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;, input_shape=(28, 28, 1)),   # default: strides = None (same as pool_size), padding = &#39;valid&#39;  tf.keras.layers.MaxPooling2D(2, 2),  tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;),   tf.keras.layers.MaxPooling2D(2,2),  tf.keras.layers.Flatten(),  tf.keras.layers.Dense(128, activation=&#39;relu&#39;),  tf.keras.layers.Dense(10, activation=&#39;softmax&#39;)])</code></pre><pre><code>_________________________________________________________________ || Layer (type)                 Output Shape              Param #    || Comments================================================================= || conv2d (Conv2D)              (None, 26, 26, 64)        640        || = 64 x (3 x 3 x 1 + 1)_________________________________________________________________ || max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0          || _________________________________________________________________ || conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928      || = 64 x (3 x 3 x 64 + 1)_________________________________________________________________ || max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0          || _________________________________________________________________ || flatten_1 (Flatten)          (None, 1600)              0          || _________________________________________________________________ || dense_2 (Dense)              (None, 128)               204928     || = 128 x (1600 + 1)_________________________________________________________________ || dense_3 (Dense)              (None, 10)                1290       || = 10 * (128 + 1)================================================================= || Total params: 243,786Trainable params: 243,786Non-trainable params: 0</code></pre><h3 id="How-to-compute-output-size"><a href="#How-to-compute-output-size" class="headerlink" title="How to compute output size"></a>How to compute output size</h3><p>Convolution layer</p><script type="math/tex; mode=display">(n + 2p - f + 1) \times (n + 2p - f + 1)</script><p>MaxPooling layer</p><script type="math/tex; mode=display">Floor(\frac{height - f}{s} + 1) \times Floor(\frac{weight - f}{s} + 1)</script><ul><li>$n$: input size</li><li>$p$: padding size</li><li>$f$: filter size</li></ul><h3 id="Two-kinds-of-padding"><a href="#Two-kinds-of-padding" class="headerlink" title="Two kinds of padding:"></a>Two kinds of padding:</h3><ul><li>Valid: no padding<script type="math/tex; mode=display">p = 0</script></li><li>Same: results in padding the input such that the output has the same length as the original input<script type="math/tex; mode=display">n + 2p - f + 1 = n \implies p = (f - 1) / 2</script>where $f$ is almost always odd number</li></ul><h3 id="How-to-compute-number-of-parameters"><a href="#How-to-compute-number-of-parameters" class="headerlink" title="How to compute number of parameters"></a>How to compute number of parameters</h3><script type="math/tex; mode=display">NF \times (f \times f \times NC_{input} + 1 )</script><ul><li>$NF$: number of filters</li><li>$NC_{input}$: number of input channels</li><li>Each filter has a bias term</li><li><a href="https://www.youtube.com/watch?v=KTB_OFoAQcc" target="_blank" rel="noopener">Convolutions Over Volume</a></li></ul><h3 id="Visualizing-the-Convolutions-and-Pooling"><a href="#Visualizing-the-Convolutions-and-Pooling" class="headerlink" title="Visualizing the Convolutions and Pooling"></a>Visualizing the Convolutions and Pooling</h3><img src="/2019/03/11/tensorflow-specialization-learning-note/shoes.png" title="Layer outputs"><p>Each row represents an itea. There are 3 shoes images here.<br>The 4 columns represent the output of the first 4 layers (conv2d, max_pooling2d, conv2d_1, max_pooling2d_1).<br>We can find the commonality for the same kind of items.</p><hr><h1 id="C1W4-Using-Real-world-Images"><a href="#C1W4-Using-Real-world-Images" class="headerlink" title="C1W4: Using Real-world Images"></a>C1W4: Using Real-world Images</h1><h2 id="Note-3"><a href="#Note-3" class="headerlink" title="Note"></a>Note</h2><h3 id="ImageGenerator"><a href="#ImageGenerator" class="headerlink" title="ImageGenerator"></a>ImageGenerator</h3><ul><li>ImageGenerator can flow images from a directory and perform operations such as resizing them on the fly. </li><li>You can point it at a directory and then the <strong>sub-directories</strong> of that will automatically generate labels for you</li></ul><pre><code>images|-- training|   |-- horse|   |   |-- 1.jpg|   |   |-- 2.jpg|   |   `-- 3.jpg|   `-- human|       |-- 1.jpg|       |-- 2.jpg|       `-- 3.jpg`-- validation    |-- horse    |   |-- 1.jpg    |   |-- 2.jpg    |   `-- 3.jpg    `-- human        |-- 1.jpg        |-- 2.jpg        `-- 3.jpg</code></pre><p>If you point <code>ImageGenerator</code> to <strong>training</strong> directory, it will generate a stream of images labelled with horse or human</p><h3 id="Mini-batch"><a href="#Mini-batch" class="headerlink" title="Mini-batch"></a>Mini-batch</h3><h4 id="Why-mini-batch"><a href="#Why-mini-batch" class="headerlink" title="Why mini-batch"></a>Why mini-batch</h4><p>For large neural networks with very large and highly redundant training sets, it is nearly always best to use mini-batch learning.</p><ul><li>The mini-batches may need to be quite big when adapting fancy methods.</li><li>Big mini-batches are more computationally efficient.</li></ul><h4 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h4><ul><li>Momentum</li><li>RMSProp</li><li>Adam</li></ul><h2 id="Code-3"><a href="#Code-3" class="headerlink" title="Code"></a>Code</h2><h3 id="Model-1"><a href="#Model-1" class="headerlink" title="Model"></a>Model</h3><pre><code class="lang-python">import tensorflow as tffrom tensorflow.keras.optimizers import RMSpropmodel = tf.keras.models.Sequential([    # Note the input shape is the desired size of the image 300x300 with 3 bytes color    # This is the first convolution    tf.keras.layers.Conv2D(16, (3,3), activation=&#39;relu&#39;, input_shape=(300, 300, 3)),    tf.keras.layers.MaxPooling2D(2, 2),    # The second convolution    tf.keras.layers.Conv2D(32, (3,3), activation=&#39;relu&#39;),    tf.keras.layers.MaxPooling2D(2,2),    # The third convolution    tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;),    tf.keras.layers.MaxPooling2D(2,2),    # The fourth convolution    tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;),    tf.keras.layers.MaxPooling2D(2,2),    # The fifth convolution    tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;),    tf.keras.layers.MaxPooling2D(2,2),    # Flatten the results to feed into a DNN    tf.keras.layers.Flatten(),    # 512 neuron hidden layer    tf.keras.layers.Dense(512, activation=&#39;relu&#39;),    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class (&#39;horses&#39;) and 1 for the other (&#39;humans&#39;)    tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)])# Train our model with the binary_crossentropy loss, # because it&#39;s a binary classification problem and our final activation is a sigmoid.# [More details](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)model.compile(loss=&#39;binary_crossentropy&#39;,              optimizer=RMSprop(lr=0.001),              metrics=[&#39;acc&#39;])model.summary()</code></pre><pre><code>Layer (type)                 Output Shape              Param #   =================================================================conv2d (Conv2D)              (None, 298, 298, 16)      448       _________________________________________________________________max_pooling2d (MaxPooling2D) (None, 149, 149, 16)      0         _________________________________________________________________conv2d_1 (Conv2D)            (None, 147, 147, 32)      4640      _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 73, 73, 32)        0         _________________________________________________________________conv2d_2 (Conv2D)            (None, 71, 71, 64)        18496     _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 35, 35, 64)        0         _________________________________________________________________conv2d_3 (Conv2D)            (None, 33, 33, 64)        36928     _________________________________________________________________max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         _________________________________________________________________conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     _________________________________________________________________max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         _________________________________________________________________flatten (Flatten)            (None, 3136)              0         _________________________________________________________________dense (Dense)                (None, 512)               1606144   _________________________________________________________________dense_1 (Dense)              (None, 1)                 513       =================================================================Total params: 1,704,097Trainable params: 1,704,097Non-trainable params: 0</code></pre><blockquote><p>The convolutions reduce the shape from 90000 (300 x 300) down to 3136</p></blockquote><h3 id="ImageDataGenerator"><a href="#ImageDataGenerator" class="headerlink" title="ImageDataGenerator"></a>ImageDataGenerator</h3><pre><code class="lang-python"># All images will be rescaled by 1./255train_datagen = ImageDataGenerator(rescale=1/255)validation_datagen = ImageDataGenerator(rescale=1/255)# Flow training images in batches of 128 using train_datagen generatortrain_generator = train_datagen.flow_from_directory(        &#39;/tmp/horse-or-human/&#39;,  # This is the source directory for training images        target_size=(300, 300),  # All images will be resized to 150x150        batch_size=128, # number of images for each batch        # Since we use binary_crossentropy loss, we need binary labels        class_mode=&#39;binary&#39;)# Flow training images in batches of 128 using train_datagen generatorvalidation_generator = validation_datagen.flow_from_directory(        &#39;/tmp/validation-horse-or-human/&#39;,  # This is the source directory for validation images        target_size=(300, 300),  # All images will be resized to 150x150        batch_size=32, # number of images for each batch        # Since we use binary_crossentropy loss, we need binary labels        class_mode=&#39;binary&#39;)history = model.fit_generator(      train_generator,      steps_per_epoch=8, # number of batches for each epoch durning training        epochs=15,      verbose=1,      validation_data = validation_generator,      validation_steps=8) # number of batches for each epoch durning validation</code></pre><h3 id="Visualizing-Intermediate-Representations"><a href="#Visualizing-Intermediate-Representations" class="headerlink" title="Visualizing Intermediate Representations"></a>Visualizing Intermediate Representations</h3><img src="/2019/03/11/tensorflow-specialization-learning-note/horse-vs-human.jpeg" title="Intermediate outputs"><p>As you can see we go from the raw pixels of the images to increasingly abstract and compact representations. The representations downstream start highlighting what the network pays attention to, and they show fewer and fewer features being “activated”; most are set to zero. This is called “sparsity.” Representation sparsity is a key feature of deep learning.</p><p>These representations carry increasingly less information about the original pixels of the image, but increasingly refined information about the class of the image. You can think of a convnet (or a deep network in general) as an information distillation pipeline.</p><h1 id="C2W1-Exploring-a-Larger-Dataset"><a href="#C2W1-Exploring-a-Larger-Dataset" class="headerlink" title="C2W1: Exploring a Larger Dataset"></a>C2W1: Exploring a Larger Dataset</h1><h2 id="Note-4"><a href="#Note-4" class="headerlink" title="Note"></a>Note</h2><ul><li>Data: <a href="https://www.kaggle.com/c/dogs-vs-cats" target="_blank" rel="noopener">https://www.kaggle.com/c/dogs-vs-cats</a></li><li><code>model.layers</code> API allows you to inspect the impact of convolutions on the images.</li></ul><h2 id="Code-4"><a href="#Code-4" class="headerlink" title="Code"></a>Code</h2><pre><code class="lang-python">import numpy as npimport randomfrom   tensorflow.keras.preprocessing.image import img_to_array, load_img# Let&#39;s define a new Model that will take an image as input, and will output# intermediate representations for all layers in the previous model after# the first.successive_outputs = [layer.output for layer in model.layers[1:]]#visualization_model = Model(img_input, successive_outputs)visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)# Let&#39;s prepare a random input image of a cat or dog from the training set.cat_img_files = [os.path.join(train_cats_dir, f) for f in train_cat_fnames]dog_img_files = [os.path.join(train_dogs_dir, f) for f in train_dog_fnames]img_path = random.choice(cat_img_files + dog_img_files)img = load_img(img_path, target_size=(150, 150))  # this is a PIL imagex   = img_to_array(img)                           # Numpy array with shape (150, 150, 3)x   = x.reshape((1,) + x.shape)                   # Numpy array with shape (1, 150, 150, 3)# Rescale by 1/255x /= 255.0# Let&#39;s run our image through our network, thus obtaining all# intermediate representations for this image.successive_feature_maps = visualization_model.predict(x)# These are the names of the layers, so can have them as part of our plotlayer_names = [layer.name for layer in model.layers]# -----------------------------------------------------------------------# Now let&#39;s display our representations# -----------------------------------------------------------------------for layer_name, feature_map in zip(layer_names, successive_feature_maps):  if len(feature_map.shape) == 4:    #-------------------------------------------    # Just do this for the conv / maxpool layers, not the fully-connected layers    #-------------------------------------------    n_features = feature_map.shape[-1]  # number of features in the feature map    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)    # We will tile our images in this matrix    display_grid = np.zeros((size, size * n_features))    #-------------------------------------------------    # Postprocess the feature to be visually palatable    #-------------------------------------------------    for i in range(n_features):      x  = feature_map[0, :, :, i]      x -= x.mean()      x /= x.std ()      x *=  64      x += 128      x  = np.clip(x, 0, 255).astype(&#39;uint8&#39;)      display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid    #-----------------    # Display the grid    #-----------------    scale = 20. / n_features    plt.figure( figsize=(scale * n_features, scale) )    plt.title ( layer_name )    plt.grid  ( False )    plt.imshow( display_grid, aspect=&#39;auto&#39;, cmap=&#39;viridis&#39; )</code></pre><h1 id="C2W2-Augmentation-A-technique-to-avoid-overfitting"><a href="#C2W2-Augmentation-A-technique-to-avoid-overfitting" class="headerlink" title="C2W2: Augmentation: A technique to avoid overfitting"></a>C2W2: Augmentation: A technique to avoid overfitting</h1><h2 id="Note-5"><a href="#Note-5" class="headerlink" title="Note"></a>Note</h2><h3 id="Image-augmentation"><a href="#Image-augmentation" class="headerlink" title="Image augmentation"></a>Image augmentation</h3><ul><li><p>Image augmentation implementation in Keras: <a href="https://keras.io/preprocessing/image/" target="_blank" rel="noopener">https://keras.io/preprocessing/image/</a></p></li><li><p>Image generator library lets you load the images into memory, process the images and then steam that to the training set to the neural network we will ultimatedly learn on.The preprocessing doesn’t require you to edit your raw images, nor does it amend them for you on-disk. It does it in-memory as it’s performing the training, allowing you to experiment without impacting your dataset.</p></li><li><p>As we start training, we’ll initially see that the accuracy is lower than with the non-augmented version. This is because of the random effects of the different image processing that’s being done. As it runs for a few more epochs, you’ll see the accuracy slowly climbing.</p></li><li><p>The image augmentation introduces a random element to the training images but if the validation set doesn’t have the same randomness, then its results can fluctuate. You don’t just need a broad set of images for training, you also need them for testing or the image augmentation won’t help you very much.(which does NOT mean that you should augment your validation set, see below)</p></li><li><p>Validation dataset should not be augmented: the validation set is used to estimate how your method works on real world data, thus it should only contain real world data. Adding augmented data will not improve the accuracy of the validation. It will at best say something about how well your method responds to the data augmentation, and at worst ruin the validation results and interpretability. As the validation accuracy is no longer a good proxy for the accuracy on new unseen data if you augment the validation data</p></li></ul><h2 id="Code-5"><a href="#Code-5" class="headerlink" title="Code"></a>Code</h2><pre><code class="lang-python">train_datagen = ImageDataGenerator(      rescale=1./255,      rotation_range=40,      width_shift_range=0.2,      height_shift_range=0.2,      shear_range=0.2,      zoom_range=0.2,      horizontal_flip=True,      fill_mode=&#39;nearest&#39;)</code></pre><h1 id="C2W3-Transfer-Learning"><a href="#C2W3-Transfer-Learning" class="headerlink" title="C2W3: Transfer Learning"></a>C2W3: Transfer Learning</h1><h2 id="Note-6"><a href="#Note-6" class="headerlink" title="Note"></a>Note</h2><h3 id="What-is-transfer-learning"><a href="#What-is-transfer-learning" class="headerlink" title="What is transfer learning"></a>What is transfer learning</h3><p>You can take an existing model, freeze many of its layers to prevent them being retrained, and effectively ‘remember’ the convolutions it was trained on to fit images, then added your own DNN underneath this so that you could retrain on your images using the convolutions from the other model.</p><h3 id="Why-dropout-can-do-the-regularization"><a href="#Why-dropout-can-do-the-regularization" class="headerlink" title="Why dropout can do the regularization"></a>Why dropout can do the regularization</h3><p>The idea behind Dropouts is that they remove a random number of neurons in your neural network. This works very well for two reasons: </p><ul><li><p>The first is that neighboring neurons often end up with similar weights, which can lead to overfitting, so dropping some out at random can remove this. </p></li><li><p>The second is that often a neuron can over-weigh the input from a neuron in the previous layer, and can over specialize as a result. It can not rely on any of the input which will be randomly dropped, instead, it will spread the weights, by which the weights will be shrinked.</p></li></ul><h2 id="Code-6"><a href="#Code-6" class="headerlink" title="Code"></a>Code</h2><pre><code class="lang-python">from tensorflow.keras import layersfrom tensorflow.keras import Modelfrom tensorflow.keras.optimizers import RMSpropfrom tensorflow.keras.applications.inception_v3 import InceptionV3local_weights_file = &#39;/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5&#39;pre_trained_model = InceptionV3(input_shape = (150, 150, 3),                                 include_top = False,  # whether to include the fully-connected layer at the top of the network.                                weights = None) # one of None (random initialization) or &#39;imagenet&#39; (pre-training on ImageNet).for layer in pre_trained_model.layers:  layer.trainable = Falselast_layer = pre_trained_model.get_layer(&#39;mixed7&#39;)last_output = last_layer.output# Flatten the output layer to 1 dimensionx = layers.Flatten()(last_output)# Add a fully connected layer with 1,024 hidden units and ReLU activationx = layers.Dense(1024, activation=&#39;relu&#39;)(x)# Add a dropout rate of 0.2x = layers.Dropout(0.2)(x)                  # Add a final sigmoid layer for classificationx = layers.Dense  (1, activation=&#39;sigmoid&#39;)(x)           model = Model( pre_trained_model.input, x) model.compile(optimizer = RMSprop(lr=0.0001),               loss = &#39;binary_crossentropy&#39;,               metrics = [&#39;acc&#39;])</code></pre><h1 id="C2W4-Multiclass-Classification"><a href="#C2W4-Multiclass-Classification" class="headerlink" title="C2W4: Multiclass Classification"></a>C2W4: Multiclass Classification</h1><h2 id="Note-7"><a href="#Note-7" class="headerlink" title="Note"></a>Note</h2><ul><li>Use CGI to generate images for Rock, Paper, Scissors</li></ul><h2 id="Code-7"><a href="#Code-7" class="headerlink" title="Code"></a>Code</h2><pre><code class="lang-python">train_generator = training_datagen.flow_from_directory(    TRAINING_DIR,    target_size=(150,150),    class_mode=&#39;categorical&#39;)# Same for validationmodel = tf.keras.models.Sequential([    # Convolution layers    # ...    # Flatten the results to feed into a DNN    tf.keras.layers.Flatten(),    tf.keras.layers.Dropout(0.5),    # 512 neuron hidden layer    tf.keras.layers.Dense(512, activation=&#39;relu&#39;),    # 3 nodes with softmax    tf.keras.layers.Dense(3, activation=&#39;softmax&#39;) ])</code></pre><p>Another way of using <code>fit_generator</code> API via <code>(images, labels)</code>, instead of via directory</p><pre><code class="lang-python">history = model.fit_generator(train_datagen.flow(training_images, training_labels, batch_size=32),                              steps_per_epoch=len(training_images) / 32,                              epochs=15,                              validation_data=validation_datagen.flow(testing_images, testing_labels, batch_size=32),                              validation_steps=len(testing_images) / 32)</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A learning note of the coursera specialization &lt;strong&gt;&lt;a href=&quot;https://www.deeplearning.ai/tensorflow-specialization/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TensorFlow: From Basics to Mastery&lt;/a&gt;&lt;/strong&gt; given by &lt;a href=&quot;http://deeplearning.ai&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;deeplearning.ai&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Course 1: Introduction to TensorFlow for AI, ML and DL&lt;/li&gt;
&lt;li&gt;Course 2: Convolutional Neural Networks in TensorFlow&lt;/li&gt;
&lt;li&gt;Coming soon …&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="Deep Learning" scheme="http://invkrh.me/categories/Deep-Learning/"/>
    
    
      <category term="Coding" scheme="http://invkrh.me/tags/Coding/"/>
    
      <category term="TensorFlow" scheme="http://invkrh.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>蒜香蜂蜜鸡</title>
    <link href="http://invkrh.me/2019/02/23/suan-xiang-feng-mi-ji/"/>
    <id>http://invkrh.me/2019/02/23/suan-xiang-feng-mi-ji/</id>
    <published>2019-02-22T23:56:42.000Z</published>
    <updated>2019-05-14T21:39:21.426Z</updated>
    
    <content type="html"><![CDATA[<h3 id="用时"><a href="#用时" class="headerlink" title="用时"></a>用时</h3><p>30分钟</p><a id="more"></a><h3 id="原料"><a href="#原料" class="headerlink" title="原料"></a>原料</h3><p>四块鸡上腿肉<br>盐<br>胡椒<br>四瓣蒜<br>蜂蜜（2）<br>米醋（1）<br>酱油（1）<br>大蒜粉（opt）<br>白芝麻（opt）<br>葱（opt）</p><h3 id="做法"><a href="#做法" class="headerlink" title="做法"></a>做法</h3><ol><li>鸡肉洗干净，抹上盐，黑胡椒，大蒜粉，腌制10分钟。</li><li>四瓣蒜做成蒜末，最好蒜泥。</li><li>用中大火预热不粘锅，不用放一滴油（鸡皮出油）。鸡皮朝下放入锅中，每2分钟翻面，共8分钟。</li><li>锅中央放入蒜泥、蜂蜜、米醋、酱油、80毫升水。火开大点。</li><li>收汁，翻面，等汁浓稠，出锅（可以撒上白芝麻和葱）。</li></ol><h3 id="成品"><a href="#成品" class="headerlink" title="成品"></a>成品</h3><img src="/2019/02/23/suan-xiang-feng-mi-ji/result.jpeg">]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;用时&quot;&gt;&lt;a href=&quot;#用时&quot; class=&quot;headerlink&quot; title=&quot;用时&quot;&gt;&lt;/a&gt;用时&lt;/h3&gt;&lt;p&gt;30分钟&lt;/p&gt;
    
    </summary>
    
      <category term="Cuisine" scheme="http://invkrh.me/categories/Cuisine/"/>
    
    
      <category term="Plat" scheme="http://invkrh.me/tags/Plat/"/>
    
      <category term="Poulet" scheme="http://invkrh.me/tags/Poulet/"/>
    
  </entry>
  
  <entry>
    <title>Interview Questions in 2018</title>
    <link href="http://invkrh.me/2018/12/19/intervew-questions-in-2018/"/>
    <id>http://invkrh.me/2018/12/19/intervew-questions-in-2018/</id>
    <published>2018-12-18T23:00:00.000Z</published>
    <updated>2019-04-13T13:04:26.855Z</updated>
    
    <content type="html"><![CDATA[<p>A list of coding interview questions that I was asked in 2018.</p><a id="more"></a><h2 id="Question-1-Add-Binary"><a href="#Question-1-Add-Binary" class="headerlink" title="Question 1: Add Binary"></a>Question 1: Add Binary</h2><h3 id="Statement-leetcode-791"><a href="#Statement-leetcode-791" class="headerlink" title="Statement (leetcode: 791)"></a>Statement (leetcode: 791)</h3><p>Given two binary strings, return their sum (also a binary string).<br>The input strings are both non-empty and contains only characters 1 or 0.</p><pre><code>Example 1:Input: a = &quot;11&quot;, b = &quot;1&quot;Output: &quot;100&quot;Example 2:Input: a = &quot;1010&quot;, b = &quot;1011&quot;Output: &quot;10101&quot;</code></pre><h3 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h3><pre><code class="lang-java">String addBinary(String a, String b) {    int i = a.length() - 1, j = b.length() - 1;    int carry = 0;    StringBuilder sb = new StringBuilder();    while (i &gt;= 0 || j &gt;= 0) {        int sum = carry;        if (i &gt;= 0) {            sum += a.charAt(i) - &#39;0&#39;;            i--;        }        if (j &gt;= 0) {            sum += b.charAt(j) - &#39;0&#39;;            j--;        }        carry = sum / 2;        sb.insert(0, sum % 2);    }    if (carry != 0) sb.insert(0, carry);    return sb.toString();}</code></pre><div class="table-container"><table><thead><tr><th></th><th>Complexity</th><th>Comments</th></tr></thead><tbody><tr><td>Time</td><td>O(n)</td><td>/</td></tr><tr><td>Space</td><td>O(n)</td><td>StringBuilder</td></tr></tbody></table></div><h3 id="Extention"><a href="#Extention" class="headerlink" title="Extention"></a>Extention</h3><p>What if the given strings can be numbers of any base ?</p><pre><code class="lang-java">String addBinary(String a, String b) {    int i = a.length() - 1, j = b.length() - 1;    int carry = 0;    StringBuilder sb = new StringBuilder();    while (i &gt;= 0 || j &gt;= 0) {        int sum = carry;        if (i &gt;= 0) {            sum += a.charAt(i) - &#39;0&#39;;            i--;        }        if (j &gt;= 0) {            sum += b.charAt(j) - &#39;0&#39;;            j--;        }        carry = sum / 2;        sb.insert(0, sum % 2);    }    if (carry != 0) sb.insert(0, carry);    return sb.toString();}</code></pre><h2 id="Question-2-cd-command"><a href="#Question-2-cd-command" class="headerlink" title="Question 2: cd command"></a>Question 2: <code>cd</code> command</h2><h3 id="Statement"><a href="#Statement" class="headerlink" title="Statement"></a>Statement</h3><p>Write a function to simluate linux command <code>cd</code></p><pre><code>Example 1:Input: cur = &quot;/etc&quot;, path = &quot;/bin/&quot;Output: &quot;/bin&quot;Example 2:Input: a = &quot;/etc&quot;, b = &quot;hadoop&quot;Output: &quot;/etc/hadoop&quot;Example 3:Input: a = &quot;/etc/hadoop/conf&quot;, b = &quot;../../hive&quot;Output: &quot;/etc/hive&quot;Example 4:Input: a = &quot;/etc/hadoop/conf&quot;, b = &quot;.././conf&quot;Output: &quot;/etc/hadoop/conf&quot;</code></pre><h3 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution"></a>Solution</h3><pre><code class="lang-java">String cd(String cur, String path) {    if (path.startsWith(&quot;/&quot;)) return path;    Stack&lt;String&gt; stack = new Stack&lt;&gt;();    for (String dir : cur.split(&quot;/&quot;))        if (!dir.isEmpty()) stack.push(dir);    for (String dir : path.split(&quot;/&quot;))        if (dir.equals(&quot;..&quot;)) {            if (!stack.isEmpty()) stack.pop();        } else if (!dir.equals(&quot;.&quot;)) {            stack.push(dir);        }    String res = String.join(&quot;/&quot;, stack);    return res.startsWith(&quot;/&quot;) ? res : &quot;/&quot; + res;}</code></pre><div class="table-container"><table><thead><tr><th></th><th>Complexity</th><th>Comments</th></tr></thead><tbody><tr><td>Time</td><td>O(n)</td><td>/</td></tr><tr><td>Space</td><td>O(n)</td><td>Stack</td></tr></tbody></table></div><h2 id="Question-3-Custom-Sort-String"><a href="#Question-3-Custom-Sort-String" class="headerlink" title="Question 3: Custom Sort String"></a>Question 3: Custom Sort String</h2><h3 id="Statement-leetcode-791-1"><a href="#Statement-leetcode-791-1" class="headerlink" title="Statement (leetcode: 791)"></a>Statement (leetcode: 791)</h3><p><code>S</code> and <code>T</code> are strings composed of lowercase letters. In <code>S</code>, no letter occurs more than once.<br><code>S</code> was sorted in some custom order previously. We want to permute the characters of <code>T</code> so that they match the order that <code>S</code> was sorted. More specifically, if <code>x</code> occurs before <code>y</code> in <code>S</code>, then <code>x</code> should occur before <code>y</code> in the returned string.<br>Return any permutation of <code>T</code> (as a string) that satisfies this property.</p><pre><code>Example :Input: S = &quot;cba&quot;, T = &quot;abcd&quot;Output: &quot;cbad&quot;Explanation: &quot;a&quot;, &quot;b&quot;, &quot;c&quot; appear in S, so the order of &quot;a&quot;, &quot;b&quot;, &quot;c&quot; should be &quot;c&quot;, &quot;b&quot;, and &quot;a&quot;. Since &quot;d&quot; does not appear in S, it can be at any position in T. &quot;dcba&quot;, &quot;cdba&quot;, &quot;cbda&quot; are also valid outputs.</code></pre><p>Note:</p><ul><li>S has length at most 26, and no character is repeated in S.</li><li>T has length at most 200.</li><li>S and T consist of lowercase letters only.</li></ul><h3 id="Solution-2"><a href="#Solution-2" class="headerlink" title="Solution"></a>Solution</h3><pre><code class="lang-java">public String customSortString(String S, String T) {    int[] dict = new int[26];    for (char c : T.toCharArray()) {        dict[c - &#39;a&#39;] += 1;    }    StringBuilder sb = new StringBuilder();    for (char c : S.toCharArray()) {        for (int i = 0; i &lt; dict[c - &#39;a&#39;]; i++)            sb.append(c);        dict[c - &#39;a&#39;] = 0;    }    for (char c = &#39;a&#39;; c &lt;= &#39;z&#39;; c++)        for (int i = 0; i &lt; dict[c - &#39;a&#39;]; i++)            sb.append(c);    return sb.toString();}</code></pre><div class="table-container"><table><thead><tr><th></th><th>Complexity</th><th>Comments</th></tr></thead><tbody><tr><td>Time</td><td>O(n)</td><td>/</td></tr><tr><td>Space</td><td>O(n)</td><td>StringBuilder</td></tr></tbody></table></div><h2 id="Question-4-Position-of-the-leftmost-one"><a href="#Question-4-Position-of-the-leftmost-one" class="headerlink" title="Question 4: Position of the leftmost one"></a>Question 4: Position of the leftmost one</h2><h3 id="Statement-1"><a href="#Statement-1" class="headerlink" title="Statement"></a>Statement</h3><p>Given a binary matrix (containing only 0 and 1) of order <code>n * n</code>. All rows are sorted already. We need to find position of the left most 1.<br>Note: in case of tie, return the position of the smallest row number.</p><pre><code>Example:Input matrix0 1 1 10 0 1 11 1 1 1  // this row has maximum 1s0 0 0 0Output: [2, 0]</code></pre><h3 id="Solution-3"><a href="#Solution-3" class="headerlink" title="Solution"></a>Solution</h3><pre><code class="lang-java">int[] findPosition(int[][] matrix) {    int r = matrix.length;    if (r == 0) return null;    int c = matrix[0].length;    if (c == 0) return null;    int[] res = new int[] {};    int j = c - 1;    for (int i = 0; i &lt; r; i++) {        while (j &gt;= 0 &amp;&amp; matrix[i][j] == 1) {            j--;            res = new int[] {i, j + 1};        }    }    return res;}</code></pre><div class="table-container"><table><thead><tr><th></th><th>Complexity</th><th>Comments</th></tr></thead><tbody><tr><td>Time</td><td>O(r + c)</td><td>ends on the boundary</td></tr><tr><td>Space</td><td>O(1)</td><td>/</td></tr></tbody></table></div><h2 id="Question-5-Validate-Binary-Search-Tree"><a href="#Question-5-Validate-Binary-Search-Tree" class="headerlink" title="Question 5: Validate Binary Search Tree"></a>Question 5: Validate Binary Search Tree</h2><h3 id="Statement-leetcode-98"><a href="#Statement-leetcode-98" class="headerlink" title="Statement (leetcode: 98)"></a>Statement (leetcode: 98)</h3><p>Given a binary tree, determine if it is a valid binary search tree (BST).<br>Assume a BST is defined as follows:</p><ul><li>The left subtree of a node contains only nodes with keys less than the node’s key.</li><li>The right subtree of a node contains only nodes with keys greater than the node’s key.</li><li>Both the left and right subtrees must also be binary search trees.</li></ul><pre><code>Example 1:Input:    2   / \  1   3Output: trueExample 2:    5   / \  1   4     / \    3   6Output: falseExplanation: The input is: [5,1,4,null,null,3,6]. The root node&#39;s value             is 5 but its right child&#39;s value is 4.</code></pre><h3 id="Solution-4"><a href="#Solution-4" class="headerlink" title="Solution"></a>Solution</h3><pre><code class="lang-java">boolean validate(TreeNode node, long min, long max) {    if (node == null) {        return true;    } else {        if (node.val &gt; min &amp;&amp; node.val &lt; max) {            return validate(node.left, min, node.val) &amp;&amp; validate(node.right, node.val, max);        } else {            return false;        }    }}boolean isValidBST(TreeNode root) {    return validate(root, Long.MIN_VALUE, Long.MAX_VALUE);}</code></pre><div class="table-container"><table><thead><tr><th></th><th>Complexity</th><th>Comments</th></tr></thead><tbody><tr><td>Time</td><td>O(n)</td><td>visit all the nodes</td></tr><tr><td>Space</td><td>O(log n)</td><td>recursice call stack</td></tr></tbody></table></div><h2 id="Question-6-Search-word-in-the-dictionary"><a href="#Question-6-Search-word-in-the-dictionary" class="headerlink" title="Question 6: Search word in the dictionary"></a>Question 6: Search word in the dictionary</h2><h3 id="Statement-leetcode-211"><a href="#Statement-leetcode-211" class="headerlink" title="Statement (leetcode: 211)"></a>Statement (leetcode: 211)</h3><p>Design a data structure that supports the following two operations:</p><pre><code class="lang-java">class WordDictionary {    /** Initialize data structure */    public WordDictionary()    /** Adds a word into the data structure. */    public void addWord(String word)    /** Returns if the word is in the data structure. A word could contain the dot character &#39;.&#39; to represent any one letter. */    public boolean search(String word)}void addWord(word)bool search(word)</code></pre><p><code>search(word)</code> can search a literal word or a regular expression string containing only letters a-z or <code>.</code>. A <code>.</code> means it can represent any one letter.</p><pre><code class="lang-java">Example:addWord(&quot;bad&quot;)addWord(&quot;dad&quot;)addWord(&quot;mad&quot;)search(&quot;pad&quot;) -&gt; falsesearch(&quot;bad&quot;) -&gt; truesearch(&quot;.ad&quot;) -&gt; truesearch(&quot;b..&quot;) -&gt; true</code></pre><h3 id="Solution-5"><a href="#Solution-5" class="headerlink" title="Solution"></a>Solution</h3><pre><code class="lang-java">class WordDictionary {    class TrieNode {        TrieNode[] next = new TrieNode[26];        String word = null;    }    TrieNode root;    public WordDictionary() {        this.root = new TrieNode();    }    /** Adds a word into the data structure. */    public void addWord(String word) {        TrieNode node = root;        for (int i = 0; i &lt; word.length(); i++) {            char c = word.charAt(i);            if (node.next[c - &#39;a&#39;] == null) node.next[c - &#39;a&#39;] = new TrieNode();            node = node.next[c - &#39;a&#39;];        }        node.word = word;    }    /** Returns if the word is in the data structure. A word could contain the dot character &#39;.&#39; to represent any one letter. */    public boolean search(String word) {        return match(word, 0, root);    }    private boolean match(String word, int i, TrieNode node) {        if (i == word.length()) return node.word != null;        char c = word.charAt(i);        if (c == &#39;.&#39;) {            for (TrieNode nextNode : node.next) {                if (nextNode != null &amp;&amp; match(word, i + 1, nextNode)) {                    return true;                }            }            return false;        } else {            TrieNode nextNode = node.next[c - &#39;a&#39;];            return nextNode != null &amp;&amp; match(word, i + 1, nextNode);        }    }}</code></pre><div class="table-container"><table><thead><tr><th>add</th><th>Complexity</th><th>Comments</th></tr></thead><tbody><tr><td>Time</td><td>O(n)</td><td>/</td></tr><tr><td>Space</td><td>O(n)</td><td>node creation</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>search</th><th>Complexity</th><th>Comments</th></tr></thead><tbody><tr><td>Time</td><td>O(n)</td><td>/</td></tr><tr><td>Space</td><td>O(n)</td><td>recursive call stack</td></tr></tbody></table></div><h2 id="Question-7-Valid-Palindrome"><a href="#Question-7-Valid-Palindrome" class="headerlink" title="Question 7: Valid Palindrome"></a>Question 7: Valid Palindrome</h2><h3 id="Statement-leetcode-125"><a href="#Statement-leetcode-125" class="headerlink" title="Statement (leetcode: 125)"></a>Statement (leetcode: 125)</h3><p>Given a string, determine if it is a palindrome, considering only alphanumeric characters and ignoring cases.<br>Note: For the purpose of this problem, we define empty string as valid palindrome.</p><pre><code>Example 1:Input: &quot;A man, a plan, a canal: Panama&quot;Output: trueExample 2:Input: &quot;race a car&quot;Output: false</code></pre><h3 id="Solution-6"><a href="#Solution-6" class="headerlink" title="Solution"></a>Solution</h3><pre><code class="lang-java">boolean isValid(char c) {    return (c &gt;= &#39;a&#39; &amp;&amp; c &lt;= &#39;z&#39;) || (c &gt;= &#39;A&#39; &amp;&amp; c &lt;= &#39;Z&#39;) || (c &gt;= &#39;0&#39; &amp;&amp; c &lt;= &#39;9&#39;);}boolean isPalindrome(String s) {    int i = 0;    int j = s.length() - 1;    while(i &lt;= j) {        if (!isValid(s.charAt(i))) {            i++;            continue;        }        if (!isValid(s.charAt(j))) {            j--;            continue;        }        if (Character.toLowerCase(s.charAt(i)) == Character.toLowerCase(s.charAt(j))) {            i++;            j--;        } else {            return false;        }    }    return true;}</code></pre><div class="table-container"><table><thead><tr><th></th><th>Complexity</th><th>Comments</th></tr></thead><tbody><tr><td>Time</td><td>O(n)</td><td>/</td></tr><tr><td>Space</td><td>O(1)</td><td>/</td></tr></tbody></table></div><h2 id="Question-8-Shortest-Distance-To-All-Stations"><a href="#Question-8-Shortest-Distance-To-All-Stations" class="headerlink" title="Question 8: Shortest Distance To All Stations"></a>Question 8: Shortest Distance To All Stations</h2><h3 id="Statement-2"><a href="#Statement-2" class="headerlink" title="Statement"></a>Statement</h3><img src="/2018/12/19/intervew-questions-in-2018/tube-map.gif" title="London Tube Map"><p>Given a metro map of London, find the station which is closest to all the others stations.</p><h3 id="Solution-Floyd–Warshall-algorithm"><a href="#Solution-Floyd–Warshall-algorithm" class="headerlink" title="Solution (Floyd–Warshall algorithm)"></a>Solution (Floyd–Warshall algorithm)</h3><pre><code class="lang-java">/** graph is a weighted undirected adjacency matrix */int solve(double[][] graph) {    int n = graph.length;    double[][] dist = new double[n][n];    for (int i = 0; i &lt; n; i++) {        for (int j = 0; j &lt; n; j++) {                dist[i][j] = graph[i][j];            }        }    /** Floyd–Warshall algorithm */    for (int k = 0; k &lt; n; k++) {        for (int i = 0; i &lt; n; i++) {            for (int j = 0; j &lt; n; j++) {                if (dist[i][j] &gt; dist[i][k] + dist[k][j]) {                    dist[i][j] = dist[i][k] + dist[k][j];                }            }        }    }    double min = Integer.MAX_VALUE;    int res = -1;    for (int i = 0; i &lt; n; i++) {        double sum = 0        for (double d : dist[i])            sum += d;        if (sum &lt; min) {            res = i;            min = sum;        }    }    return res;}</code></pre><div class="table-container"><table><thead><tr><th></th><th>Complexity</th><th>Comments</th></tr></thead><tbody><tr><td>Time</td><td>O(n ^ 3)</td><td>/</td></tr><tr><td>Space</td><td>O(n ^ 2)</td><td>/</td></tr></tbody></table></div><h2 id="Question-9-Equilibrium-Point"><a href="#Question-9-Equilibrium-Point" class="headerlink" title="Question 9: Equilibrium Point"></a>Question 9: Equilibrium Point</h2><h3 id="Statement-leetcode-724"><a href="#Statement-leetcode-724" class="headerlink" title="Statement (leetcode: 724)"></a>Statement (leetcode: 724)</h3><p>Given an array of integers nums, write a method that returns the “pivot” index of this array.<br>We define the pivot index as the index where the sum of the numbers to the left of the index is equal to the sum of the numbers to the right of the index.<br>If no such index exists, we should return -1. If there are multiple pivot indexes, you should return the left-most pivot index.</p><pre><code>Example 1:Input: nums = [1, 7, 3, 6, 5, 6]Output: 3Explanation: The sum of the numbers to the left of index 3 (nums[3] = 6) is equal to the sum of numbers to the right of index 3.Also, 3 is the first index where this occurs.Example 2:Input: nums = [1, 2, 3]Output: -1Explanation: There is no index that satisfies the conditions in the problem statement.</code></pre><h3 id="Solution-7"><a href="#Solution-7" class="headerlink" title="Solution"></a>Solution</h3><pre><code class="lang-java">pivotIndex(int[] nums) {    int sum = 0, leftsum = 0;    for (int x: nums) sum += x;    for (int i = 0; i &lt; nums.length; ++i) {        if (leftsum == sum - leftsum - nums[i]) return i;        leftsum += nums[i];    }    return -1;}</code></pre><div class="table-container"><table><thead><tr><th></th><th>Complexity</th><th>Comments</th></tr></thead><tbody><tr><td>Time</td><td>O(n)</td><td>/</td></tr><tr><td>Space</td><td>O(1)</td><td>/</td></tr></tbody></table></div><h2 id="Question-10-Complete-Binary-Tree"><a href="#Question-10-Complete-Binary-Tree" class="headerlink" title="Question 10: Complete Binary Tree"></a>Question 10: Complete Binary Tree</h2><h3 id="Statement-3"><a href="#Statement-3" class="headerlink" title="Statement"></a>Statement</h3><p>Given a complete binary tree in which each node marked with a number in level order (root = 1) and several connections are removed.<br>Find if the given number is still reachable from the root of the tree.</p><pre><code>Example 1:Input: tree = root, num = 5            1 -&gt; root           / \          /   \         /     \        /       \       /         \      2           3     /           / \    /           /   \   4     5     6     7  / \   / \   / \   / \ 8   9 10 11 12 13 14 15Output: falseExample 2:Input: tree = root, num = 6            1 -&gt; root             \              \               \                \                 \      2           3     / \         / \    /   \       /   \   4     5     6     7  / \   / \   / \   / \ 8   9 10 11 12 13 14 15Output: true</code></pre><h3 id="Solution-8"><a href="#Solution-8" class="headerlink" title="Solution"></a>Solution</h3><pre><code class="lang-java">boolean findInCompleteTree(TreeNode root, int n) {    List&lt;Boolean&gt; path = new LinkedList&lt;&gt;();    while (n &gt; 1) {        if (n % 2 == 0) {            path.add(0, true);        } else {            path.add(0, false);        }        n /= 2;    }    for (boolean p : path) {        if (p) root = root.left;        else root = root.right;        if (root == null) return false;    }    return true;}</code></pre><div class="table-container"><table><thead><tr><th></th><th>Complexity</th><th>Comments</th></tr></thead><tbody><tr><td>Time</td><td>O(log n)</td><td>/</td></tr><tr><td>Space</td><td>O(log n)</td><td>/</td></tr></tbody></table></div><h3 id="Extension-leetcode-222"><a href="#Extension-leetcode-222" class="headerlink" title="Extension (leetcode: 222)"></a>Extension (leetcode: 222)</h3><p>Count the number of node in a complete binary tree.</p><pre><code>Example 1:Input: tree = root            1 -&gt; root           / \          /   \         /     \        /       \       /         \      2           3     / \         / \    /   \       /   \   4     5     6     7  / \   / \   / 8   9 10 11 12Output: 12</code></pre><pre><code class="lang-java">int countInCompleteTree(TreeNode root) {    TreeNode node = root;    int depthLeft = 0;    while (node != null) {        depthLeft++;        node = node.left;    }    node = root;    int depthRight = 0;    while (node != null) {        depthRight++;        node = node.right;    }    return depthLeft == depthRight ?        (1 &lt;&lt; depthLeft) - 1 :        1 + countInCompleteTree(root.left) + countInCompleteTree(root.right);}</code></pre><div class="table-container"><table><thead><tr><th></th><th>Complexity</th><th>Comments</th></tr></thead><tbody><tr><td>Time</td><td>O(log n * log n)</td><td>log n calls and each call takes log n to compute depth</td></tr><tr><td>Space</td><td>O(log n)</td><td>recursive call stack</td></tr></tbody></table></div><h2 id="Question-11-UTF-8-Encoding"><a href="#Question-11-UTF-8-Encoding" class="headerlink" title="Question 11: UTF-8 Encoding"></a>Question 11: UTF-8 Encoding</h2><h3 id="Statement-4"><a href="#Statement-4" class="headerlink" title="Statement"></a>Statement</h3><p>A character in UTF8 can be from 1 to 4 bytes long, subjected to the following rules:</p><ul><li>For 1-byte character, the first bit is a 0, followed by its unicode code.</li><li>For n-bytes character, the first n-bits are all one’s, the n+1 bit is 0, followed by n-1 bytes with most significant 2 bits being 10.</li></ul><p>This is how the UTF-8 encoding would work:    </p><div class="table-container"><table><thead><tr><th>Number of bytes</th><th>Bits for code point</th><th>First code point</th><th>Last code point</th><th>Byte 1</th><th>Byte 2</th><th>Byte 3</th><th>Byte 4</th></tr></thead><tbody><tr><td>1</td><td>7</td><td>U+0000</td><td>U+007F</td><td>0xxxxxxx</td><td></td><td></td><td></td></tr><tr><td>2</td><td>11</td><td>U+0080</td><td>U+07FF</td><td>110xxxxx</td><td>10xxxxxx</td><td></td><td></td></tr><tr><td>3</td><td>16</td><td>U+0800</td><td>U+FFFF</td><td>1110xxxx</td><td>10xxxxxx</td><td>10xxxxxx</td><td></td></tr><tr><td>4</td><td>21</td><td>U+10000</td><td>U+10FFFF</td><td>11110xxx</td><td>10xxxxxx</td><td>10xxxxxx</td><td>10xxxxxx</td></tr></tbody></table></div><p>Given a byte array which contains only UTF-8 encoded characters and an integer <code>limit</code>,<br>return the max number of bytes contains only valid UTF-8 encordings in the first <code>limit</code> bytes.</p><pre><code>Example 1:Input:stream = | 0xxxxxxx | 110xxxxx | 10xxxxxx | 1110xxxx | 10xxxxxx | 10xxxxxx | 11110xxx | 10xxxxxx ||| 10xxxxxx | 10xxxxxx |limit = 8Output: 5Example 2:Input:stream = | 0xxxxxxx | 110xxxxx | 10xxxxxx |limit = 5Output: 2</code></pre><h3 id="Solution-9"><a href="#Solution-9" class="headerlink" title="Solution"></a>Solution</h3><pre><code class="lang-java">int countUTF8Byte(byte[] stream, int limit) {    if (stream.length &lt;= limit) {        return stream.length;    } else {        while (limit &gt; 0 &amp;&amp; (stream[limit] &amp; 0xFF) &gt;&gt; 6 == 2) {            limit--;        }        return limit;    }}</code></pre><div class="table-container"><table><thead><tr><th></th><th>Complexity</th><th>Comments</th></tr></thead><tbody><tr><td>Time</td><td>O(1)</td><td>No more than 6 bytes</td></tr><tr><td>Space</td><td>O(1)</td><td>/</td></tr></tbody></table></div><h2 id="Question-12-Design-Rate-limiter"><a href="#Question-12-Design-Rate-limiter" class="headerlink" title="Question 12: Design Rate limiter"></a>Question 12: Design Rate limiter</h2><h3 id="Statement-inspired-by-leetcode-362"><a href="#Statement-inspired-by-leetcode-362" class="headerlink" title="Statement (inspired by leetcode: 362)"></a>Statement (inspired by leetcode: 362)</h3><p>Design rate limiter API based on the count limit per minute and per hour.<br>The granularity of timestamp is in second if needed.</p><pre><code class="lang-java">class RateLimiter {    /** Initialize data structure */    public RateLimiter(long minuteCount, long hourCount)    /** Return true if the function calls exceeded either minuteCount or hourCount, otherwise return false */    public boolean isLimited() }RateLimiter rl = new RateLimit(100, 6000);rl.isLimited() // return false;</code></pre><h3 id="Solution-10"><a href="#Solution-10" class="headerlink" title="Solution"></a>Solution</h3><pre><code class="lang-java">public class RateLimiter {    class HitCounter {        private int   numBucket;        private int[] time;        private int[] hit;        public HitCounter(int numBucket) {            this.numBucket = numBucket;            this.time = new int[numBucket];            this.hit = new int[numBucket];        }        public void hit(int ts) {            int bucket = ts % this.numBucket;            if (time[bucket] == ts) {                hit[bucket]++;            } else {                time[bucket] = ts;                hit[bucket] = 1;            }        }        public int count(int ts) {            int cnt = 0;            for (int i = 0; i &lt; this.numBucket; i++) {                if (ts - time[i] &lt; this.numBucket) {                    cnt += hit[i];                }            }            return cnt;        }    }    private long       minuteLimit;    private long       hourLimit;    private HitCounter minuteCounter;    private HitCounter hourCounter;    public RateLimiter(long minuteLimit, long hourLimit) {        this.minuteLimit = minuteLimit;        this.hourLimit = hourLimit;        this.minuteCounter = new HitCounter(60);        this.hourCounter = new HitCounter(3600);    }    public boolean isLimited() {        int tsInSec = (int) (System.currentTimeMillis() / 1000);        if (this.minuteCounter.count(tsInSec) &lt; this.minuteLimit &amp;&amp;            this.hourCounter.count(tsInSec) &lt; this.hourLimit) {            minuteCounter.hit(tsInSec);            hourCounter.hit(tsInSec);            return false;        } else {            return true;        }    }    public static void main(String[] args) throws InterruptedException {        RateLimiter rl = new RateLimiter(10, 600);        int count = 0;        while (true) {            Thread.sleep(1000);            if (rl.isLimited()) {                break;            } else {                count++;                System.out.println(&quot;Limit not reached: &quot; + count);            }        }        System.out.println(&quot;Limit exceeded: &quot; + count);    }}</code></pre><div class="table-container"><table><thead><tr><th>hit</th><th>Complexity</th><th>Comments</th></tr></thead><tbody><tr><td>Time</td><td>O(1)</td><td>/</td></tr><tr><td>Space</td><td>O(n)</td><td>number of the buckets</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>count</th><th>Complexity</th><th>Comments</th></tr></thead><tbody><tr><td>Time</td><td>O(n)</td><td>number of the buckets</td></tr><tr><td>Space</td><td>O(n)</td><td>number of the buckets</td></tr></tbody></table></div><h2 id="Question-13-Design-Task-Scheduler-cron"><a href="#Question-13-Design-Task-Scheduler-cron" class="headerlink" title="Question 13: Design Task Scheduler (cron)"></a>Question 13: Design Task Scheduler (<code>cron</code>)</h2><h3 id="Statement-5"><a href="#Statement-5" class="headerlink" title="Statement"></a>Statement</h3><p>Implement the following 3 methods. Start with scheduling part and then execution part.</p><pre><code class="lang-java">public class CronScheduler {    void schedule(TimerTask task, long delay) {}    void repeat(TimerTask t, long delay, long period) {}    void daily(TimerTask t, long delay) {}}</code></pre><h3 id="Solution-11"><a href="#Solution-11" class="headerlink" title="Solution"></a>Solution</h3><p>Reference: <code>java.util.Timer</code> and <code>java.util.TimerTask</code></p><pre><code class="lang-java">// TODO</code></pre>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A list of coding interview questions that I was asked in 2018.&lt;/p&gt;
    
    </summary>
    
      <category term="Interview" scheme="http://invkrh.me/categories/Interview/"/>
    
    
      <category term="Algorithm" scheme="http://invkrh.me/tags/Algorithm/"/>
    
      <category term="Coding" scheme="http://invkrh.me/tags/Coding/"/>
    
  </entry>
  
</feed>
