<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">










  <meta name="google-site-verification" content="jR_bUEN846roQfh9C1VWdQ9eb9YaT7E_G142mMBeqow">












  
  
  
  

  
    
    
  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link rel="stylesheet" referrerpolicy="no-referrer" type="text/css" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Lato:300,300italic,400,400italic,700,700italic|Raleway:300,300italic,400,400italic,700,700italic|Parisienne:300,300italic,400,400italic,700,700italic|Consolas:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext">
  






<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-next.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png?v=7.1.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png?v=7.1.2">


  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg?v=7.1.2" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '7.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":true,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="A learning note of the coursera specialization Tensorflow in practice given by deeplearning.ai.  Course 1: Introduction to TensorFlow for AI, ML and DL Course 2: Convolutional Neural Networks in Tenso">
<meta name="keywords" content="Coding,TensorFlow">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow in Practice Learning Note">
<meta property="og:url" content="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/index.html">
<meta property="og:site_name" content="Yorozuya">
<meta property="og:description" content="A learning note of the coursera specialization Tensorflow in practice given by deeplearning.ai.  Course 1: Introduction to TensorFlow for AI, ML and DL Course 2: Convolutional Neural Networks in Tenso">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/shoes.png">
<meta property="og:image" content="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/horse-vs-human.jpeg">
<meta property="og:image" content="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/word-embedding.png">
<meta property="og:image" content="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/sarcasm-lstm-acc.png">
<meta property="og:image" content="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/sarcasm-conv1d-acc.png">
<meta property="og:image" content="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/sarcasm-lstm-loss.png">
<meta property="og:image" content="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/sarcasm-conv1d-loss.png">
<meta property="og:image" content="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/sentiment-acc-no-dropout.png">
<meta property="og:image" content="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/sentiment-acc.png">
<meta property="og:image" content="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/sentiment-loss-no-dropout.png">
<meta property="og:image" content="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/sentiment-loss.png">
<meta property="og:image" content="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/learning-rate.png">
<meta property="og:updated_time" content="2019-09-07T17:02:45.072Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tensorflow in Practice Learning Note">
<meta name="twitter:description" content="A learning note of the coursera specialization Tensorflow in practice given by deeplearning.ai.  Course 1: Introduction to TensorFlow for AI, ML and DL Course 2: Convolutional Neural Networks in Tenso">
<meta name="twitter:image" content="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/shoes.png">



  <link rel="alternate" href="/atom.xml" title="Yorozuya" type="application/atom+xml">



  
  
  <link rel="canonical" href="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Tensorflow in Practice Learning Note | Yorozuya</title>
  




  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-74632317-1"></script>
  <script>
    var host = window.location.hostname;
    if (host !== "localhost" || !true) {
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-74632317-1');
    }
  </script>









  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>


  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><link rel='stylesheet' href='/css/prism.css'><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yorozuya</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>Search</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hao Ren">
      <meta itemprop="description" content="Machine Learning Engineer @ Criteo">
      <meta itemprop="image" content="/uploads/avatar.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yorozuya">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Tensorflow in Practice Learning Note

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-03-11 23:03:33" itemprop="dateCreated datePublished" datetime="2019-03-11T23:03:33+01:00">2019-03-11</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/2019/03/11/tensorflow-specialization-learning-note/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/2019/03/11/tensorflow-specialization-learning-note/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/2019/03/11/tensorflow-specialization-learning-note/" class="leancloud_visitors" data-flag-title="Tensorflow in Practice Learning Note">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>A learning note of the coursera specialization <a href="https://www.deeplearning.ai/tensorflow-in-practice/" target="_blank" rel="noopener">Tensorflow in practice</a> given by <a href="http://deeplearning.ai" target="_blank" rel="noopener">deeplearning.ai</a>.</p>
<ul>
<li>Course 1: Introduction to TensorFlow for AI, ML and DL</li>
<li>Course 2: Convolutional Neural Networks in TensorFlow</li>
<li>Course 3: Natural Language Processing in TensorFlow</li>
<li>Course 4: Sequences, Time Series and Prediction</li>
</ul>
<a id="more"></a>
<h1 id="c1w1-a-new-programming-paradigm"><a class="markdownIt-Anchor" href="#c1w1-a-new-programming-paradigm"></a> C1W1: A New Programming Paradigm</h1>
<h2 id="note"><a class="markdownIt-Anchor" href="#note"></a> Note</h2>
<h3 id="new-programming-paradigm"><a class="markdownIt-Anchor" href="#new-programming-paradigm"></a> New programming paradigm</h3>
<table>
<thead>
<tr>
<th></th>
<th>input</th>
<th>output</th>
</tr>
</thead>
<tbody>
<tr>
<td>Triditional Programming</td>
<td>Rules, Data</td>
<td>Answers</td>
</tr>
<tr>
<td>Machine Learning</td>
<td>Answers, Data</td>
<td>Rules</td>
</tr>
</tbody>
</table>
<h2 id="code"><a class="markdownIt-Anchor" href="#code"></a> Code</h2>
<h3 id="how-to-fit-a-line"><a class="markdownIt-Anchor" href="#how-to-fit-a-line"></a> How to fit a line</h3>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">import tensorflow as tf
import numpy as np
from tensorflow import keras
model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])
model.compile(optimizer=&#39;sgd&#39;, loss=&#39;mean_squared_error&#39;)
xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)
ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)
model.fit(xs, ys, epochs=500)
print(model.predict([10.0]))
</code></pre>
<p>The predicted value is not 19.0 but a little under. It is because neural networks deal with probabilities, so given the data that we fed the NN with, it calculated that there is a very high probability that the relationship between <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span></span> is <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi><mo>=</mo><mn>2</mn><mi>X</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">Y=2X-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord">2</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>, but with only 6 data points we can’t know for sure. As a result, the result for 10 is very close to 19, but not necessarily 19.</p>
<hr>
<h1 id="c1w2-introduction-to-computer-vision"><a class="markdownIt-Anchor" href="#c1w2-introduction-to-computer-vision"></a> C1W2: Introduction to Computer Vision</h1>
<h2 id="note-2"><a class="markdownIt-Anchor" href="#note-2"></a> Note</h2>
<h3 id="why-are-the-labels-numbers-instead-of-words"><a class="markdownIt-Anchor" href="#why-are-the-labels-numbers-instead-of-words"></a> Why are the labels numbers instead of words</h3>
<p>Using a number is a first step in avoiding bias – instead of labelling it with words in a specific language and excluding people who don’t speak that language! You can learn more about bias and techniques to avoid it <a href="https://developers.google.com/machine-learning/fairness-overview/" target="_blank" rel="noopener">here</a>.</p>
<h3 id="what-is-cross-entropy-ce"><a class="markdownIt-Anchor" href="#what-is-cross-entropy-ce"></a> What is cross entropy (CE)</h3>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi><mi>E</mi><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mrow><mi>C</mi><mo>−</mo><mn>1</mn></mrow></munderover><msub><mi>y</mi><mi>i</mi></msub><mo>⋅</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mover accent="true"><msub><mi>x</mi><mi>i</mi></msub><mo>⃗</mo></mover><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">CE = - \sum_{i=0}^{C - 1} y_i \cdot log( f(\vec{x_i}) )
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.1060050000000006em;vertical-align:-1.277669em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283360000000004em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.300005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.07153em;">C</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.277669em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.714em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2355em;"><span class="overlay" style="height:0.714em;width:0.471em;"><svg width="0.471em" height="0.714em" style="width:0.471em" viewbox="0 0 471 714" preserveaspectratio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"/></svg></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p>
<p>where</p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span></span></span></span>: the number of classes</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><msub><mi>x</mi><mi>i</mi></msub><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{x_i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.864em;vertical-align:-0.15em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.714em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2355em;"><span class="overlay" style="height:0.714em;width:0.471em;"><svg width="0.471em" height="0.714em" style="width:0.471em" viewbox="0 0 471 714" preserveaspectratio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"/></svg></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span>: the feature vector of the example <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>: the label of the example <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.65952em;vertical-align:0em;"></span><span class="mord mathdefault">i</span></span></span></span></li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span></span></span></span>: the learned prediction function which takes the feacture vector <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><msub><mi>x</mi><mi>i</mi></msub><mo>⃗</mo></mover></mrow><annotation encoding="application/x-tex">\vec{x_i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.864em;vertical-align:-0.15em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.714em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2355em;"><span class="overlay" style="height:0.714em;width:0.471em;"><svg width="0.471em" height="0.714em" style="width:0.471em" viewbox="0 0 471 714" preserveaspectratio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"/></svg></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span> and returns the probability of being class <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li>
</ul>
<p>When <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mo>=</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">c = 2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">c</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">2</span></span></span></span></p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi><mi>E</mi><mo>=</mo><mo>−</mo><mo fence="false">[</mo><msub><mi>y</mi><mi>i</mi></msub><mo>⋅</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>⋅</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo fence="false">]</mo></mrow><annotation encoding="application/x-tex">CE = - \big[ y_i \cdot log( p_i ) + (1 - y_i) \cdot log( 1 - p_i ) \big]
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.20001em;vertical-align:-0.35001em;"></span><span class="mord">−</span><span class="mord"><span class="delimsizing size1">[</span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.20001em;vertical-align:-0.35001em;"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="delimsizing size1">]</span></span></span></span></span></span></p>
<h3 id="difference-between-categorical_crossentropy-and-sparse_categorical_crossentropy"><a class="markdownIt-Anchor" href="#difference-between-categorical_crossentropy-and-sparse_categorical_crossentropy"></a> Difference between <code>categorical_crossentropy</code> and <code>sparse_categorical_crossentropy</code></h3>
<ul>
<li>If your targets are one-hot encoded, use categorical_crossentropy.<br>
Examples of one-hot encodings:</li>
</ul>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-none">[1,0,0]
[0,1,0]
[0,0,1]
</code></pre>
<ul>
<li>But if your targets are integers, use sparse_categorical_crossentropy.<br>
Examples of integer encodings (for the sake of completion):</li>
</ul>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-none">1
2
3
</code></pre>
<h2 id="code-2"><a class="markdownIt-Anchor" href="#code-2"></a> Code</h2>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python"># Early stopping
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get(&#39;loss&#39;)&lt;0.4):
      print(&quot;\nReached 60% accuracy so cancelling training!&quot;)
      self.model.stop_training = True

callbacks = myCallback()

mnist = tf.keras.datasets.fashion_mnist
(training_images, training_labels), (test_images, test_labels) = mnist.load_data()
# Data normalization
training_images  = training_images &#x2F; 255.0
test_images = test_images &#x2F; 255.0
model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), 
                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), 
                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])
model.compile(optimizer = &#39;adam&#39;,
              loss = &#39;sparse_categorical_crossentropy&#39;,
              metrics=[&#39;accuracy&#39;])
model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])
model.evaluate(test_images, test_labels)
</code></pre>
<hr>
<h1 id="c1w3-enhancing-vision-with-convolutional-neural-networks"><a class="markdownIt-Anchor" href="#c1w3-enhancing-vision-with-convolutional-neural-networks"></a> C1W3: Enhancing Vision with Convolutional Neural Networks</h1>
<h2 id="note-3"><a class="markdownIt-Anchor" href="#note-3"></a> Note</h2>
<h3 id="convolution-layer"><a class="markdownIt-Anchor" href="#convolution-layer"></a> Convolution Layer</h3>
<p>Each kernal is an edge detector which is perfect for computer vision, because often it’s features that can get highlighted like this that distinguish one item for another, and the amount of information needed is then much less…because you’ll just train on the highlighted features.</p>
<h3 id="maxpooling-layer"><a class="markdownIt-Anchor" href="#maxpooling-layer"></a> MaxPooling Layer</h3>
<p>The convolution layer is followed by a MaxPooling layer which is then designed to compress the image, while maintaining the content of the features that were highlighted by the convolution</p>
<h3 id="why-cnn-works"><a class="markdownIt-Anchor" href="#why-cnn-works"></a> Why CNN works</h3>
<p>CNN tries different filters on the image and learning which ones work when looking at the training data. As a result, when it works, you’ll have greatly reduced information passing through the network, but because it isolates and identifies features, you can also get increased accuracy</p>
<h2 id="code-3"><a class="markdownIt-Anchor" href="#code-3"></a> Code</h2>
<h3 id="model"><a class="markdownIt-Anchor" href="#model"></a> Model</h3>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-Python"># Reshape to a 4D tensor, otherwise the Convolutions do not recognize the shape
training_images=training_images.reshape(60000, 28, 28, 1)
training_images=training_images &#x2F; 255.0
test_images = test_images.reshape(10000, 28, 28, 1)
test_images=test_images&#x2F;255.0

# 2-convolution-layer NN
model = tf.keras.models.Sequential([
  # default: strides = 1, padding = &#39;valid&#39;
  tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;, input_shape=(28, 28, 1)), 
  # default: strides = None (same as pool_size), padding = &#39;valid&#39;
  tf.keras.layers.MaxPooling2D(2, 2),
  tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;), 
  tf.keras.layers.MaxPooling2D(2,2),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation=&#39;relu&#39;),
  tf.keras.layers.Dense(10, activation=&#39;softmax&#39;)
])
</code></pre>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-none">_________________________________________________________________ || 
Layer (type)                 Output Shape              Param #    || Comments
================================================================= || 
conv2d (Conv2D)              (None, 26, 26, 64)        640        || = 64 x (3 x 3 x 1 + 1)
_________________________________________________________________ || 
max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0          || 
_________________________________________________________________ || 
conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928      || = 64 x (3 x 3 x 64 + 1)
_________________________________________________________________ || 
max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0          || 
_________________________________________________________________ || 
flatten_1 (Flatten)          (None, 1600)              0          || 
_________________________________________________________________ || 
dense_2 (Dense)              (None, 128)               204928     || = 128 x (1600 + 1)
_________________________________________________________________ || 
dense_3 (Dense)              (None, 10)                1290       || = 10 * (128 + 1)
================================================================= || 
Total params: 243,786
Trainable params: 243,786
Non-trainable params: 0
</code></pre>
<h3 id="how-to-compute-output-size"><a class="markdownIt-Anchor" href="#how-to-compute-output-size"></a> How to compute output size</h3>
<p>Convolution layer</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo stretchy="false">(</mo><mi>n</mi><mo>+</mo><mn>2</mn><mi>p</mi><mo>−</mo><mi>f</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>×</mo><mo stretchy="false">(</mo><mi>n</mi><mo>+</mo><mn>2</mn><mi>p</mi><mo>−</mo><mi>f</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(n + 2p - f + 1) \times (n + 2p - f + 1)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord">2</span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord">2</span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span></p>
<p>MaxPooling layer</p>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi><mi>l</mi><mi>o</mi><mi>o</mi><mi>r</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>h</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>−</mo><mi>f</mi></mrow><mi>s</mi></mfrac><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>×</mo><mi>F</mi><mi>l</mi><mi>o</mi><mi>o</mi><mi>r</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo>−</mo><mi>f</mi></mrow><mi>s</mi></mfrac><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Floor(\frac{height - f}{s} + 1) \times Floor(\frac{weight - f}{s} + 1)
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.0574399999999997em;vertical-align:-0.686em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">s</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="mord mathdefault">e</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">h</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.0574399999999997em;vertical-align:-0.686em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">s</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord mathdefault">e</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">h</span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span></p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">n</span></span></span></span>: input size</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span></span></span></span>: padding size</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span></span></span></span>: filter size</li>
</ul>
<h3 id="two-kinds-of-padding"><a class="markdownIt-Anchor" href="#two-kinds-of-padding"></a> Two kinds of padding:</h3>
<ul>
<li>Valid: no padding</li>
</ul>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>p</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">p = 0
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span></span></p>
<ul>
<li>Same: results in padding the input such that the output has the same length as the original input</li>
</ul>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>n</mi><mo>+</mo><mn>2</mn><mi>p</mi><mo>−</mo><mi>f</mi><mo>+</mo><mn>1</mn><mo>=</mo><mi>n</mi><mtext>  </mtext><mo>⟹</mo><mtext>  </mtext><mi>p</mi><mo>=</mo><mo stretchy="false">(</mo><mi>f</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">n + 2p - f + 1 = n \implies p = (f - 1) / 2
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord">2</span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.549em;vertical-align:-0.024em;"></span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">⟹</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord">/</span><span class="mord">2</span></span></span></span></span></p>
<p>where <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span></span></span></span> is almost always odd number</p>
<h3 id="how-to-compute-number-of-parameters"><a class="markdownIt-Anchor" href="#how-to-compute-number-of-parameters"></a> How to compute number of parameters</h3>
<p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi><mi>F</mi><mo>×</mo><mo stretchy="false">(</mo><mi>f</mi><mo>×</mo><mi>f</mi><mo>×</mo><mi>N</mi><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">NF \times (f \times f \times NC_{input} + 1 )
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span></p>
<ul>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi><mi>F</mi></mrow><annotation encoding="application/x-tex">NF</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mord mathdefault" style="margin-right:0.13889em;">F</span></span></span></span>: number of filters</li>
<li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">NC_{input}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">u</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>: number of input channels</li>
<li>Each filter has a bias term</li>
<li><a href="https://www.youtube.com/watch?v=KTB_OFoAQcc" target="_blank" rel="noopener">Convolutions Over Volume</a></li>
</ul>
<h3 id="visualizing-the-convolutions-and-pooling"><a class="markdownIt-Anchor" href="#visualizing-the-convolutions-and-pooling"></a> Visualizing the Convolutions and Pooling</h3>
<img src="/2019/03/11/tensorflow-specialization-learning-note/shoes.png" title="Layer outputs">
<p>Each row represents an itea. There are 3 shoes images here.<br>
The 4 columns represent the output of the first 4 layers (conv2d, max_pooling2d, conv2d_1, max_pooling2d_1).<br>
We can find the commonality for the same kind of items.</p>
<hr>
<h1 id="c1w4-using-real-world-images"><a class="markdownIt-Anchor" href="#c1w4-using-real-world-images"></a> C1W4: Using Real-world Images</h1>
<h2 id="note-4"><a class="markdownIt-Anchor" href="#note-4"></a> Note</h2>
<h3 id="imagegenerator"><a class="markdownIt-Anchor" href="#imagegenerator"></a> ImageGenerator</h3>
<ul>
<li>ImageGenerator can flow images from a directory and perform operations such as resizing them on the fly.</li>
<li>You can point it at a directory and then the <strong>sub-directories</strong> of that will automatically generate labels for you</li>
</ul>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-none">images
|-- training
|   |-- horse
|   |   |-- 1.jpg
|   |   |-- 2.jpg
|   |   `-- 3.jpg
|   `-- human
|       |-- 1.jpg
|       |-- 2.jpg
|       `-- 3.jpg
`-- validation
    |-- horse
    |   |-- 1.jpg
    |   |-- 2.jpg
    |   `-- 3.jpg
    `-- human
        |-- 1.jpg
        |-- 2.jpg
        `-- 3.jpg
</code></pre>
<p>If you point <code>ImageGenerator</code> to <strong>training</strong> directory, it will generate a stream of images labelled with horse or human</p>
<h3 id="mini-batch"><a class="markdownIt-Anchor" href="#mini-batch"></a> Mini-batch</h3>
<h4 id="why-mini-batch"><a class="markdownIt-Anchor" href="#why-mini-batch"></a> Why mini-batch</h4>
<p>For large neural networks with very large and highly redundant training sets, it is nearly always best to use mini-batch learning.</p>
<ul>
<li>The mini-batches may need to be quite big when adapting fancy methods.</li>
<li>Big mini-batches are more computationally efficient.</li>
</ul>
<h4 id="optimization"><a class="markdownIt-Anchor" href="#optimization"></a> Optimization</h4>
<ul>
<li>Momentum</li>
<li>RMSProp</li>
<li>Adam</li>
</ul>
<h2 id="code-4"><a class="markdownIt-Anchor" href="#code-4"></a> Code</h2>
<h3 id="model-2"><a class="markdownIt-Anchor" href="#model-2"></a> Model</h3>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">import tensorflow as tf
from tensorflow.keras.optimizers import RMSprop

model = tf.keras.models.Sequential([
    # Note the input shape is the desired size of the image 300x300 with 3 bytes color
    # This is the first convolution
    tf.keras.layers.Conv2D(16, (3,3), activation=&#39;relu&#39;, input_shape=(300, 300, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    # The second convolution
    tf.keras.layers.Conv2D(32, (3,3), activation=&#39;relu&#39;),
    tf.keras.layers.MaxPooling2D(2,2),
    # The third convolution
    tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;),
    tf.keras.layers.MaxPooling2D(2,2),
    # The fourth convolution
    tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;),
    tf.keras.layers.MaxPooling2D(2,2),
    # The fifth convolution
    tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;),
    tf.keras.layers.MaxPooling2D(2,2),
    # Flatten the results to feed into a DNN
    tf.keras.layers.Flatten(),
    # 512 neuron hidden layer
    tf.keras.layers.Dense(512, activation=&#39;relu&#39;),
    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class (&#39;horses&#39;) and 1 for the other (&#39;humans&#39;)
    tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)
])

# Train our model with the binary_crossentropy loss, 
# because it&#39;s a binary classification problem and our final activation is a sigmoid.
# [More details](http:&#x2F;&#x2F;www.cs.toronto.edu&#x2F;~tijmen&#x2F;csc321&#x2F;slides&#x2F;lecture_slides_lec6.pdf)
model.compile(loss=&#39;binary_crossentropy&#39;,
              optimizer=RMSprop(lr=0.001),
              metrics=[&#39;acc&#39;])

model.summary()
</code></pre>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-none">Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 298, 298, 16)      448       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 149, 149, 16)      0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 147, 147, 32)      4640      
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 73, 73, 32)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 71, 71, 64)        18496     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 35, 35, 64)        0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 33, 33, 64)        36928     
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64)          0         
_________________________________________________________________
flatten (Flatten)            (None, 3136)              0         
_________________________________________________________________
dense (Dense)                (None, 512)               1606144   
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 513       
=================================================================
Total params: 1,704,097
Trainable params: 1,704,097
Non-trainable params: 0
</code></pre>
<blockquote>
<p>The convolutions reduce the shape from 90000 (300 x 300) down to 3136</p>
</blockquote>
<h3 id="imagedatagenerator"><a class="markdownIt-Anchor" href="#imagedatagenerator"></a> ImageDataGenerator</h3>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python"># All images will be rescaled by 1.&#x2F;255
train_datagen = ImageDataGenerator(rescale=1&#x2F;255)
validation_datagen = ImageDataGenerator(rescale=1&#x2F;255)

# Flow training images in batches of 128 using train_datagen generator
train_generator = train_datagen.flow_from_directory(
        &#39;&#x2F;tmp&#x2F;horse-or-human&#x2F;&#39;,  # This is the source directory for training images
        target_size=(300, 300),  # All images will be resized to 150x150
        batch_size=128, # number of images for each batch
        # Since we use binary_crossentropy loss, we need binary labels
        class_mode=&#39;binary&#39;)

# Flow training images in batches of 128 using train_datagen generator
validation_generator = validation_datagen.flow_from_directory(
        &#39;&#x2F;tmp&#x2F;validation-horse-or-human&#x2F;&#39;,  # This is the source directory for validation images
        target_size=(300, 300),  # All images will be resized to 150x150
        batch_size=32, # number of images for each batch
        # Since we use binary_crossentropy loss, we need binary labels
        class_mode=&#39;binary&#39;)

history = model.fit_generator(
      train_generator,
      steps_per_epoch=8, # number of batches for each epoch durning training  
      epochs=15,
      verbose=1,
      validation_data = validation_generator,
      validation_steps=8) # number of batches for each epoch durning validation  
</code></pre>
<h3 id="visualizing-intermediate-representations"><a class="markdownIt-Anchor" href="#visualizing-intermediate-representations"></a> Visualizing Intermediate Representations</h3>
<img src="/2019/03/11/tensorflow-specialization-learning-note/horse-vs-human.jpeg" title="Intermediate outputs">
<p>As you can see we go from the raw pixels of the images to increasingly abstract and compact representations. The representations downstream start highlighting what the network pays attention to, and they show fewer and fewer features being “activated”; most are set to zero. This is called “sparsity.” Representation sparsity is a key feature of deep learning.</p>
<p>These representations carry increasingly less information about the original pixels of the image, but increasingly refined information about the class of the image. You can think of a convnet (or a deep network in general) as an information distillation pipeline.</p>
<h1 id="c2w1-exploring-a-larger-dataset"><a class="markdownIt-Anchor" href="#c2w1-exploring-a-larger-dataset"></a> C2W1: Exploring a Larger Dataset</h1>
<h2 id="note-5"><a class="markdownIt-Anchor" href="#note-5"></a> Note</h2>
<ul>
<li>Data: <a href="https://www.kaggle.com/c/dogs-vs-cats" target="_blank" rel="noopener">https://www.kaggle.com/c/dogs-vs-cats</a></li>
<li><code>model.layers</code> API allows you to inspect the impact of convolutions on the images.</li>
</ul>
<h2 id="code-5"><a class="markdownIt-Anchor" href="#code-5"></a> Code</h2>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">import numpy as np
import random
from   tensorflow.keras.preprocessing.image import img_to_array, load_img

# Let&#39;s define a new Model that will take an image as input, and will output
# intermediate representations for all layers in the previous model after
# the first.
successive_outputs = [layer.output for layer in model.layers[1:]]

#visualization_model = Model(img_input, successive_outputs)
visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)

# Let&#39;s prepare a random input image of a cat or dog from the training set.
cat_img_files = [os.path.join(train_cats_dir, f) for f in train_cat_fnames]
dog_img_files = [os.path.join(train_dogs_dir, f) for f in train_dog_fnames]

img_path = random.choice(cat_img_files + dog_img_files)
img = load_img(img_path, target_size=(150, 150))  # this is a PIL image

x   = img_to_array(img)                           # Numpy array with shape (150, 150, 3)
x   = x.reshape((1,) + x.shape)                   # Numpy array with shape (1, 150, 150, 3)

# Rescale by 1&#x2F;255
x &#x2F;= 255.0

# Let&#39;s run our image through our network, thus obtaining all
# intermediate representations for this image.
successive_feature_maps = visualization_model.predict(x)

# These are the names of the layers, so can have them as part of our plot
layer_names = [layer.name for layer in model.layers]

# -----------------------------------------------------------------------
# Now let&#39;s display our representations
# -----------------------------------------------------------------------
for layer_name, feature_map in zip(layer_names, successive_feature_maps):
  
  if len(feature_map.shape) == 4:
    
    #-------------------------------------------
    # Just do this for the conv &#x2F; maxpool layers, not the fully-connected layers
    #-------------------------------------------
    n_features = feature_map.shape[-1]  # number of features in the feature map
    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)
    
    # We will tile our images in this matrix
    display_grid = np.zeros((size, size * n_features))
    
    #-------------------------------------------------
    # Postprocess the feature to be visually palatable
    #-------------------------------------------------
    for i in range(n_features):
      x  = feature_map[0, :, :, i]
      x -= x.mean()
      x &#x2F;= x.std ()
      x *=  64
      x += 128
      x  = np.clip(x, 0, 255).astype(&#39;uint8&#39;)
      display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid

    #-----------------
    # Display the grid
    #-----------------

    scale = 20. &#x2F; n_features
    plt.figure( figsize=(scale * n_features, scale) )
    plt.title ( layer_name )
    plt.grid  ( False )
    plt.imshow( display_grid, aspect=&#39;auto&#39;, cmap=&#39;viridis&#39; ) 
</code></pre>
<h1 id="c2w2-augmentation-a-technique-to-avoid-overfitting"><a class="markdownIt-Anchor" href="#c2w2-augmentation-a-technique-to-avoid-overfitting"></a> C2W2: Augmentation: A technique to avoid overfitting</h1>
<h2 id="note-6"><a class="markdownIt-Anchor" href="#note-6"></a> Note</h2>
<h3 id="image-augmentation"><a class="markdownIt-Anchor" href="#image-augmentation"></a> Image augmentation</h3>
<ul>
<li>
<p>Image augmentation implementation in Keras: <a href="https://keras.io/preprocessing/image/" target="_blank" rel="noopener">https://keras.io/preprocessing/image/</a></p>
</li>
<li>
<p>Image generator library lets you load the images into memory, process the images and then steam that to the training set to the neural network we will ultimatedly learn on.The preprocessing doesn’t require you to edit your raw images, nor does it amend them for you on-disk. It does it in-memory as it’s performing the training, allowing you to experiment without impacting your dataset.</p>
</li>
<li>
<p>As we start training, we’ll initially see that the accuracy is lower than with the non-augmented version. This is because of the random effects of the different image processing that’s being done. As it runs for a few more epochs, you’ll see the accuracy slowly climbing.</p>
</li>
<li>
<p>The image augmentation introduces a random element to the training images but if the validation set doesn’t have the same randomness, then its results can fluctuate. You don’t just need a broad set of images for training, you also need them for testing or the image augmentation won’t help you very much.(which does NOT mean that you should augment your validation set, see below)</p>
</li>
<li>
<p>Validation dataset should not be augmented: the validation set is used to estimate how your method works on real world data, thus it should only contain real world data. Adding augmented data will not improve the accuracy of the validation. It will at best say something about how well your method responds to the data augmentation, and at worst ruin the validation results and interpretability. As the validation accuracy is no longer a good proxy for the accuracy on new unseen data if you augment the validation data</p>
</li>
</ul>
<h2 id="code-6"><a class="markdownIt-Anchor" href="#code-6"></a> Code</h2>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">train_datagen = ImageDataGenerator(
      rescale=1.&#x2F;255,
      rotation_range=40,
      width_shift_range=0.2,
      height_shift_range=0.2,
      shear_range=0.2,
      zoom_range=0.2,
      horizontal_flip=True,
      fill_mode=&#39;nearest&#39;)
</code></pre>
<h1 id="c2w3-transfer-learning"><a class="markdownIt-Anchor" href="#c2w3-transfer-learning"></a> C2W3: Transfer Learning</h1>
<h2 id="note-7"><a class="markdownIt-Anchor" href="#note-7"></a> Note</h2>
<h3 id="what-is-transfer-learning"><a class="markdownIt-Anchor" href="#what-is-transfer-learning"></a> What is transfer learning</h3>
<p>You can take an existing model, freeze many of its layers to prevent them being retrained, and effectively ‘remember’ the convolutions it was trained on to fit images, then added your own DNN underneath this so that you could retrain on your images using the convolutions from the other model.</p>
<h3 id="why-dropout-can-do-the-regularization"><a class="markdownIt-Anchor" href="#why-dropout-can-do-the-regularization"></a> Why dropout can do the regularization</h3>
<p>The idea behind Dropouts is that they remove a random number of neurons in your neural network. This works very well for two reasons:</p>
<ul>
<li>
<p>The first is that neighboring neurons often end up with similar weights, which can lead to overfitting, so dropping some out at random can remove this.</p>
</li>
<li>
<p>The second is that often a neuron can over-weigh the input from a neuron in the previous layer, and can over specialize as a result. It can not rely on any of the input which will be randomly dropped, instead, it will spread the weights, by which the weights will be shrinked.</p>
</li>
</ul>
<h2 id="code-7"><a class="markdownIt-Anchor" href="#code-7"></a> Code</h2>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">from tensorflow.keras import layers
from tensorflow.keras import Model
from tensorflow.keras.optimizers import RMSprop

from tensorflow.keras.applications.inception_v3 import InceptionV3

local_weights_file = &#39;&#x2F;tmp&#x2F;inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5&#39;

pre_trained_model = InceptionV3(input_shape = (150, 150, 3), 
                                include_top = False,  # whether to include the fully-connected layer at the top of the network.
                                weights = None) # one of None (random initialization) or &#39;imagenet&#39; (pre-training on ImageNet).

for layer in pre_trained_model.layers:
  layer.trainable = False

last_layer = pre_trained_model.get_layer(&#39;mixed7&#39;)
last_output = last_layer.output

# Flatten the output layer to 1 dimension
x = layers.Flatten()(last_output)
# Add a fully connected layer with 1,024 hidden units and ReLU activation
x = layers.Dense(1024, activation=&#39;relu&#39;)(x)
# Add a dropout rate of 0.2
x = layers.Dropout(0.2)(x)                  
# Add a final sigmoid layer for classification
x = layers.Dense  (1, activation=&#39;sigmoid&#39;)(x)           

model = Model( pre_trained_model.input, x) 

model.compile(optimizer = RMSprop(lr=0.0001), 
              loss = &#39;binary_crossentropy&#39;, 
              metrics = [&#39;acc&#39;])
</code></pre>
<h1 id="c2w4-multiclass-classification"><a class="markdownIt-Anchor" href="#c2w4-multiclass-classification"></a> C2W4: Multiclass Classification</h1>
<h2 id="note-8"><a class="markdownIt-Anchor" href="#note-8"></a> Note</h2>
<ul>
<li>Use CGI to generate images for Rock, Paper, Scissors</li>
</ul>
<h2 id="code-8"><a class="markdownIt-Anchor" href="#code-8"></a> Code</h2>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">train_generator = training_datagen.flow_from_directory(
	TRAINING_DIR,
	target_size=(150,150),
	class_mode=&#39;categorical&#39;
)

# Same for validation

model = tf.keras.models.Sequential([
    # Convolution layers
    # ...
    # Flatten the results to feed into a DNN
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dropout(0.5),
    # 512 neuron hidden layer
    tf.keras.layers.Dense(512, activation=&#39;relu&#39;),
    # 3 nodes with softmax
    tf.keras.layers.Dense(3, activation=&#39;softmax&#39;) 
])
</code></pre>
<p>Another way of using <code>fit_generator</code> API via <code>(images, labels)</code>, instead of via directory</p>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">history = model.fit_generator(train_datagen.flow(training_images, training_labels, batch_size=32),
                              steps_per_epoch=len(training_images) &#x2F; 32,
                              epochs=15,
                              validation_data=validation_datagen.flow(testing_images, testing_labels, batch_size=32),
                              validation_steps=len(testing_images) &#x2F; 32)
</code></pre>
<hr>
<h1 id="c3w1-sentiment-in-text"><a class="markdownIt-Anchor" href="#c3w1-sentiment-in-text"></a> C3W1: Sentiment in text</h1>
<h2 id="code-9"><a class="markdownIt-Anchor" href="#code-9"></a> Code</h2>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">from tensorflow.keras.preprocessing.text import Tokenizer
sentences = [
  &#39;I love my dog&#39;,
  &#39;I love my cat&#39;
]
tokenizer = Tokenizer(num_words = 100, oov_token=&#39;&lt;OOV&gt;&#39;)
tokenizer.fit_on_texts(sentences)

word_index = tokenizer.word_index
print(word_index)
</code></pre>
<p><strong>Remark:</strong></p>
<ul>
<li>If the number of distinct words is bigger than <code>num_words</code>, the tokenizer will do is take the top 100 words by volume</li>
<li><code>num_words</code> is optional. If it is not set, it will take all the words in the <code>sentences</code></li>
<li><code>oov_token</code> is used for words that aren’t in the word index</li>
<li>Punctuation like spaces and the comma, have actually been removed</li>
<li>Token is case sensitive =&gt; convert to lower case</li>
<li><code>word_index</code> is sorted by commonality</li>
</ul>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">sequences = tokenizer.texts_to_sequences(sentences)
</code></pre>
<p><strong>Remark:</strong><br>
If you train a neural network on a corpus of texts, and the text has a word index generated from it, then when you want to do inference with the train model, you’ll have to encode the text that you want to infer on with the same word index, otherwise it would be meaningless.</p>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">test_seq = tokenizer.texts_to_sequences(test_data)
</code></pre>
<p><strong>Remark:</strong><br>
New words which are not in the index will be lost in the sequences<br>
In the case:</p>
<ul>
<li>We need a very board corpus</li>
<li>We need to put a special value for unknown word <code>Tokenizer(num_words = 100, oov_token=&quot;&lt;OOV&gt;&quot;)</code></li>
</ul>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">from tensorflow.keras.preprocessing.sequence import pad_sequences
padded = pad_sequences(sequences)
</code></pre>
<p><strong>Remark:</strong><br>
Pad leading zeros to fill the size of the longest sequence</p>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">padded = pad_sequences(sequences, padding=&#39;post&#39;, truncating=&#39;post&#39;, maxlen=5)
</code></pre>
<p><strong>Remark:</strong></p>
<ul>
<li>If you only want your sentences to have a maximum of five words. You can say <code>maxlen=5</code></li>
<li>Sentences longer than the <code>maxlen</code> lose information from the beginning by default</li>
<li>If you want to lose from the end instead, you can do so with the <code>truncating</code> parameter</li>
</ul>
<hr>
<h1 id="c3w2-word-embeddings"><a class="markdownIt-Anchor" href="#c3w2-word-embeddings"></a> C3W2: Word Embeddings</h1>
<h2 id="note-9"><a class="markdownIt-Anchor" href="#note-9"></a> Note</h2>
<h3 id="why-subwords-works-poorly"><a class="markdownIt-Anchor" href="#why-subwords-works-poorly"></a> Why subwords works poorly</h3>
<p>Not only do the meanings of the words matter, but also the sequence in which they are found.<br>
Subwords are meaningless and our neural network does not take the order of the words into account.<br>
This is where RNN comes to play.</p>
<h2 id="code-10"><a class="markdownIt-Anchor" href="#code-10"></a> Code</h2>
<h3 id="check-tf-version"><a class="markdownIt-Anchor" href="#check-tf-version"></a> Check TF version</h3>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">import tensorflow as tf
print(tf.__version__)
</code></pre>
<p><strong>Remark:</strong></p>
<ul>
<li>Use python3</li>
<li>If the version of tensorflow is 1.x, you should do <code>tf.enable_eager_execution()</code> which is default in tensorflow 2.x</li>
</ul>
<h3 id="download-imdb_reviews-via-tensorflow-datasets"><a class="markdownIt-Anchor" href="#download-imdb_reviews-via-tensorflow-datasets"></a> Download <code>imdb_reviews</code> via <code>tensorflow-datasets</code></h3>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">!pip install -q tensorflow-datasets
import tensorflow_datasets as tfds
imdb, info = tfds.load(&quot;imdb_reviews&quot;, with_info=True, as_supervised=True)
train_data, test_data = imdb[&#39;train&#39;], imdb[&#39;test&#39;]
</code></pre>
<p><strong>Remark:</strong></p>
<ul>
<li>More information about this API: <a href="https://www.tensorflow.org/datasets/api_docs/python/tfds/load" target="_blank" rel="noopener">https://www.tensorflow.org/datasets/api_docs/python/tfds/load</a></li>
</ul>
<h3 id="prepare-dataset"><a class="markdownIt-Anchor" href="#prepare-dataset"></a> Prepare dataset</h3>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

vocab_size = 10000
embedding_dim = 16
max_length = 120
trunc_type = &#39;post&#39;
padding_type = &#39;post&#39;
oov_tok = &#39;&lt;OOV&gt;&#39;

# train_sentences is a list of string
tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(train_sentences)
word_index = tokenizer.word_index
train_sequences = tokenizer.texts_to_sequences(train_sentences)
train_padded = pad_sequences(train_sequences, 
                             padding=padding_type, 
                             truncating=trunc_type, 
                             maxlen=max_length)
# validation_sentences is a list of string
validation_sequences = tokenizer.texts_to_sequences(validation_sentences)
validation_padded = pad_sequences(validation_sequences, 
                             padding=padding_type, 
                             truncating=trunc_type, 
                             maxlen=max_length)
# label is a list of string
label_tokenizer = Tokenizer()
label_tokenizer.fit_on_texts(labels)
training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))
validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))
</code></pre>
<p><strong>Remark:</strong></p>
<ul>
<li>the number of unique label is always very small, no need to set <code>num_words</code> and <code>oov_token</code></li>
<li>Once labels are parsed into a list, we need to convert the list into numpy array which is required by <code>tf.keras</code> APIs used below</li>
</ul>
<h3 id="train-word-embedding-label"><a class="markdownIt-Anchor" href="#train-word-embedding-label"></a> Train word embedding label</h3>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    # tf.keras.layers.Flatten(),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(24, activation=&#39;relu&#39;),
    tf.keras.layers.Dense(6, activation=&#39;softmax&#39;),
])

model.compile(loss=&#39;sparse_categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;])

num_epochs = 30
history = model.fit(train_padded, training_label_seq, 
                    epochs=num_epochs, 
                    validation_data=(validation_padded, validation_label_seq), 
                    verbose=2)
</code></pre>
<p><strong>Remark:</strong></p>
<ul>
<li><code>Flatten()</code> more parameters =&gt; more accurate</li>
<li><code>GlobalAveragePooling1D</code> less parameters =&gt; less accurate but still good</li>
<li><code>GlobalAveragePooling1D</code> averages across the vector to flatten it out</li>
<li>Check out the model summary below</li>
</ul>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-none">Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 120, 16)           160000    
_________________________________________________________________
flatten (Flatten)            (None, 1920)              0         
_________________________________________________________________
dense (Dense)                (None, 6)                 11526     
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 7         
=================================================================
Total params: 171,533
Trainable params: 171,533
Non-trainable params: 0
_________________________________________________________________
</code></pre>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-none">Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 120, 16)           160000    
_________________________________________________________________
global_average_pooling1d (Gl (None, 16)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 6)                 102       
_________________________________________________________________
dense_3 (Dense)              (None, 1)                 7         
=================================================================
Total params: 160,109
Trainable params: 160,109
Non-trainable params: 0
</code></pre>
<span itemprop="image" itemscope itemtype="http://schema.org/ImageObject"><img itemprop="url image" src="word-embedding.png" class="full-image" alt title="word embedding structure" style="max-width: none; width: 100%;"><meta itemprop="width" content="auto"><meta itemprop="height" content="auto"></span>
<p>As shown in the figure above, here is how this network works:</p>
<ol>
<li>Each word in one input sequence is transformed into a one-hot coding encoding vector, which is why Embedding layer take <code>vocab_size</code> as a parameter.</li>
<li>Each one-hot vector passes through the same embedding layer, it will be transformed into 16-dim vector. For a sequence, we have 120 such vectors.</li>
<li>Instead of flatten these 120 vectors, we take average of them. So the output is still a 16-dim vector.</li>
<li>The following 2 dense layer is straightforward.</li>
</ol>
<p><strong>Remark:</strong><br>
Global Average Pooling (GAP) is generally better flatten layer in the structure above, because it only needs less weight which leads to some extent of regularization and can accelarate the training as well.</p>
<h3 id="word-embedding-visualization"><a class="markdownIt-Anchor" href="#word-embedding-visualization"></a> Word embedding visualization</h3>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

def decode_sentence(text):
    return &#39; &#39;.join([reverse_word_index.get(i, &#39;?&#39;) for i in text])

e = model.layers[0]
weights = e.get_weights()[0]
print(weights.shape) # shape: (vocab_size, embedding_dim)

import io

out_v = io.open(&#39;vecs.tsv&#39;, &#39;w&#39;, encoding=&#39;utf-8&#39;)
out_m = io.open(&#39;meta.tsv&#39;, &#39;w&#39;, encoding=&#39;utf-8&#39;)
for word_num in range(1, vocab_size):
  word = reverse_word_index[word_num]
  embeddings = weights[word_num]
  out_m.write(word + &quot;\n&quot;)
  out_v.write(&#39;\t&#39;.join([str(x) for x in embeddings]) + &quot;\n&quot;)
out_v.close()
out_m.close()
</code></pre>
<p><strong>Remark:</strong></p>
<ul>
<li>Upload these two files to <a href="https://projector.tensorflow.org" target="_blank" rel="noopener">https://projector.tensorflow.org</a></li>
<li>You can check if the close words have similiar semantics via UI</li>
</ul>
<h1 id="c3w3-sequence-models"><a class="markdownIt-Anchor" href="#c3w3-sequence-models"></a> C3W3: Sequence models</h1>
<h2 id="note-10"><a class="markdownIt-Anchor" href="#note-10"></a> Note</h2>
<ul>
<li>In terms of loss and accuracy curves, 2-layer LSTM is more smooth.</li>
<li>LSTM is more likely to overfit than flatten and averaged layer.</li>
<li>In this week, we tried B-LSTM, B-GRU and Conv1D models. All of them have over-fitting issue, it is natually because there are words which are out of vocabulary. They can not learning during training and leads to the over-fitting.</li>
</ul>
<h3 id="model-comparison"><a class="markdownIt-Anchor" href="#model-comparison"></a> Model comparison</h3>
<h4 id="imdb-subwords-8k"><a class="markdownIt-Anchor" href="#imdb-subwords-8k"></a> IMDB Subwords 8K</h4>
<p>Training takes too long to run in colab, so no plots.</p>
<div class="tabs" id="imdb-subwords-8k"><ul class="nav-tabs"><li class="tab active"><a href="#imdb-subwords-8k-1">Single Layer LSTM</a></li><li class="tab"><a href="#imdb-subwords-8k-2">Multiple Layer LSTM</a></li><li class="tab"><a href="#imdb-subwords-8k-3">Multiple Layer GRU</a></li></ul><div class="tab-content"><div class="tab-pane active" id="imdb-subwords-8k-1"><!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">model = tf.keras.Sequential([
    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(64, activation=&#39;relu&#39;),
    tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)
])
</code></pre>
<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-none">Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, None, 64)          523840    
_________________________________________________________________
bidirectional (Bidirectional (None, 128)               66048     
_________________________________________________________________
dense (Dense)                (None, 64)                8256      
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 65        
=================================================================
Total params: 598,209
Trainable params: 598,209
Non-trainable params: 0
_________________________________________________________________
</code></pre></div><div class="tab-pane" id="imdb-subwords-8k-2"><!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">model = tf.keras.Sequential([
    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(64, activation=&#39;relu&#39;),
    tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)
])
</code></pre>
<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-none">Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, None, 64)          523840    
_________________________________________________________________
bidirectional (Bidirectional (None, None, 128)         66048     
_________________________________________________________________
bidirectional_1 (Bidirection (None, 64)                41216     
_________________________________________________________________
dense (Dense)                (None, 64)                4160      
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 65        
=================================================================
Total params: 635,329
Trainable params: 635,329
Non-trainable params: 0
_________________________________________________________________
</code></pre></div><div class="tab-pane" id="imdb-subwords-8k-3"><!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">model = tf.keras.Sequential([
    tf.keras.layers.Embedding(tokenizer.vocab_size, 64),
    tf.keras.layers.Conv1D(128, 5, activation=&#39;relu&#39;),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(64, activation=&#39;relu&#39;),
    tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)
])
</code></pre>
<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-none">Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, None, 64)          523840    
_________________________________________________________________
conv1d (Conv1D)              (None, None, 128)         41088     
_________________________________________________________________
global_average_pooling1d (Gl (None, 128)               0         
_________________________________________________________________
dense (Dense)                (None, 64)                8256      
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 65        
=================================================================
Total params: 573,249
Trainable params: 573,249
Non-trainable params: 0
_________________________________________________________________
</code></pre></div></div></div>
<h4 id="sarcasm"><a class="markdownIt-Anchor" href="#sarcasm"></a> Sarcasm</h4>
<div class="tabs" id="sarcasm"><ul class="nav-tabs"><li class="tab active"><a href="#sarcasm-1">Bidirectional LSTM</a></li><li class="tab"><a href="#sarcasm-2">1D Convolutional Layer</a></li></ul><div class="tab-content"><div class="tab-pane active" id="sarcasm-1"><!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
    tf.keras.layers.Dense(24, activation=&#39;relu&#39;),
    tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)
])
</code></pre>
<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-none">Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 120, 16)           16000     
_________________________________________________________________
bidirectional (Bidirectional (None, 64)                12544     
_________________________________________________________________
dense (Dense)                (None, 24)                1560      
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 25        
=================================================================
Total params: 30,129
Trainable params: 30,129
Non-trainable params: 0
_________________________________________________________________
</code></pre></div><div class="tab-pane" id="sarcasm-2"><!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.Conv1D(128, 5, activation=&#39;relu&#39;),
    tf.keras.layers.GlobalMaxPooling1D(),
    tf.keras.layers.Dense(24, activation=&#39;relu&#39;),
    tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)
])
</code></pre>
<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-none">Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 120, 16)           16000     
_________________________________________________________________
conv1d (Conv1D)              (None, 116, 128)          10368     
_________________________________________________________________
global_max_pooling1d (Global (None, 128)               0         
_________________________________________________________________
dense (Dense)                (None, 24)                3096      
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 25        
=================================================================
Total params: 29,489
Trainable params: 29,489
Non-trainable params: 0
_________________________________________________________________
</code></pre></div></div></div>
<table>
<thead>
<tr>
<th></th>
<th>Bidirectional LSTM</th>
<th>1D Convolutional Layer</th>
</tr>
</thead>
<tbody>
<tr>
<td>Time per epoch</td>
<td>85s</td>
<td>3s</td>
</tr>
<tr>
<td>Accuracy</td>
<td><span itemprop="image" itemscope itemtype="http://schema.org/ImageObject"><img itemprop="url image" src="sarcasm-lstm-acc.png" class="full-image" alt title><meta itemprop="width" content="auto"><meta itemprop="height" content="auto"></span></td>
<td><span itemprop="image" itemscope itemtype="http://schema.org/ImageObject"><img itemprop="url image" src="sarcasm-conv1d-acc.png" class="full-image" alt title><meta itemprop="width" content="auto"><meta itemprop="height" content="auto"></span></td>
</tr>
<tr>
<td>Loss</td>
<td><span itemprop="image" itemscope itemtype="http://schema.org/ImageObject"><img itemprop="url image" src="sarcasm-lstm-loss.png" class="full-image" alt title><meta itemprop="width" content="auto"><meta itemprop="height" content="auto"></span></td>
<td><span itemprop="image" itemscope itemtype="http://schema.org/ImageObject"><img itemprop="url image" src="sarcasm-conv1d-loss.png" class="full-image" alt title><meta itemprop="width" content="auto"><meta itemprop="height" content="auto"></span></td>
</tr>
</tbody>
</table>
<h2 id="code-11"><a class="markdownIt-Anchor" href="#code-11"></a> Code</h2>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Conv1D(64, 5, activation=&#39;relu&#39;),
    tf.keras.layers.MaxPooling1D(pool_size=4),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;)
])
model.compile(loss=&#39;binary_crossentropy&#39;,optimizer=&#39;adam&#39;,metrics=[&#39;accuracy&#39;])
model.summary()

num_epochs = 50
history = model.fit(training_sequences, 
                    training_labels, 
                    epochs=num_epochs, 
                    validation_data=(test_sequences, test_labels), 
                    verbose=2)
</code></pre>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-none">_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 16, 100)           13802600  
_________________________________________________________________
dropout (Dropout)            (None, 16, 100)           0         
_________________________________________________________________
conv1d (Conv1D)              (None, 12, 64)            32064     
_________________________________________________________________
max_pooling1d (MaxPooling1D) (None, 3, 64)             0         
_________________________________________________________________
lstm (LSTM)                  (None, 64)                33024     
_________________________________________________________________
dense (Dense)                (None, 1)                 65        
=================================================================
Total params: 13,867,753
Trainable params: 65,153
Non-trainable params: 13,802,600
_________________________________________________________________
</code></pre>
<p>Applying regularization techniques like drop out can overcome overfitting. We can see from the figures below that the validation loss does not increase sharply!</p>
<table>
<thead>
<tr>
<th></th>
<th>Without Dropout</th>
<th>With Dropout</th>
</tr>
</thead>
<tbody>
<tr>
<td>Accuracy</td>
<td><span itemprop="image" itemscope itemtype="http://schema.org/ImageObject"><img itemprop="url image" src="sentiment-acc-no-dropout.png" class="full-image" alt title style="max-width: none; width: 100%;"><meta itemprop="width" content="auto"><meta itemprop="height" content="auto"></span></td>
<td><span itemprop="image" itemscope itemtype="http://schema.org/ImageObject"><img itemprop="url image" src="sentiment-acc.png" class="full-image" alt title style="max-width: none; width: 100%;"><meta itemprop="width" content="auto"><meta itemprop="height" content="auto"></span></td>
</tr>
<tr>
<td>Loss</td>
<td><span itemprop="image" itemscope itemtype="http://schema.org/ImageObject"><img itemprop="url image" src="sentiment-loss-no-dropout.png" class="full-image" alt title style="max-width: none; width: 100%;"><meta itemprop="width" content="auto"><meta itemprop="height" content="auto"></span></td>
<td><span itemprop="image" itemscope itemtype="http://schema.org/ImageObject"><img itemprop="url image" src="sentiment-loss.png" class="full-image" alt title style="max-width: none; width: 100%;"><meta itemprop="width" content="auto"><meta itemprop="height" content="auto"></span></td>
</tr>
</tbody>
</table>
<h1 id="c3w4-sequence-models-and-literature"><a class="markdownIt-Anchor" href="#c3w4-sequence-models-and-literature"></a> C3W4: Sequence models and literature</h1>
<h2 id="note-11"><a class="markdownIt-Anchor" href="#note-11"></a> Note</h2>
<p>When you have very large bodies of text with many many words, the word based prediction does not work well. Because the number of unique words in the collection is very big, and there are over millions of sequences generated using the algorithm. So the labels alone would require the storage of many terabytes of RAM.</p>
<p>A better approache is character-based prediction. The full number of unique characters in a corpus is far less than the full number of unique words, at least in English. So the same principles that you use to predict words can be used to apply here.</p>
<h2 id="code-12"><a class="markdownIt-Anchor" href="#code-12"></a> Code</h2>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">corpus = data.lower().split(&quot;\n&quot;)

tokenizer.fit_on_texts(corpus)
total_words = len(tokenizer.word_index) + 1 # Add 1 for OOV

# create input sequences using list of tokens
input_sequences = []
for line in corpus:
	token_list = tokenizer.texts_to_sequences([line])[0]
	for i in range(1, len(token_list)):
		n_gram_sequence = token_list[:i+1]
		input_sequences.append(n_gram_sequence)


# pad sequences 
max_sequence_len = max([len(x) for x in input_sequences])
input_sequences = np.array(pad_sequences(input_sequences, 
                                         maxlen=max_sequence_len, 
                                         padding=&#39;pre&#39;))

# create predictors and label
predictors, label = input_sequences[:,:-1],input_sequences[:,-1]

label = tensorflow.keras.utils.to_categorical(label, num_classes=total_words)

model = Sequential()
# input_length: minus 1 since the last word is the label
model.add(Embedding(total_words, 100, input_length=max_sequence_len-1)) 
model.add(Bidirectional(LSTM(150, return_sequences = True)))
model.add(Dropout(0.2))
model.add(LSTM(100))
model.add(Dense(total_words&#x2F;2, activation=&#39;relu&#39;, kernel_regularizer=regularizers.l2(0.01)))
model.add(Dense(total_words, activation=&#39;softmax&#39;))
model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;])
print(model.summary())

history = model.fit(predictors, label, epochs=100, verbose=1)
</code></pre>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-none">Model: &quot;sequential_1&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 10, 100)           321100    
_________________________________________________________________
bidirectional (Bidirectional (None, 10, 300)           301200    
_________________________________________________________________
dropout (Dropout)            (None, 10, 300)           0         
_________________________________________________________________
lstm_1 (LSTM)                (None, 100)               160400    
_________________________________________________________________
dense (Dense)                (None, 1605)              162105    
_________________________________________________________________
dense_1 (Dense)              (None, 3211)              5156866   
=================================================================
Total params: 6,101,671
Trainable params: 6,101,671
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<h1 id="c4w1-sequences-and-prediction"><a class="markdownIt-Anchor" href="#c4w1-sequences-and-prediction"></a> C4W1: Sequences and Prediction</h1>
<h2 id="note-12"><a class="markdownIt-Anchor" href="#note-12"></a> Note</h2>
<p>Imputation: Fill data in the pase or fill the missing data<br>
Trends: upward or downward<br>
Seasonalities: repeated patterns<br>
Autocorrelation: correlated with a delayed copy of itself (lag)<br>
Noise：random / occasional values<br>
Combination of all the above<br>
Non-stationary time series: the behavior changed, it should be trained by using time window</p>
<h3 id="split-training-period-validation-period-test-period"><a class="markdownIt-Anchor" href="#split-training-period-validation-period-test-period"></a> Split training period, validation period, test period</h3>
<ul>
<li>
<p>Fixed partition:<br>
If test period is the most recent dataset which has a strong signal for the future, it should be used to train the model, otherwise the model may not be optimal. So it is quite common to use just a training period and a validation period for model training, and the test set is in the future</p>
</li>
<li>
<p>Roll-forward partition:<br>
At each iteration, we train the model on a training period. And we use it to forecast the following day, or the following week, in the validation period. It can been seen as doing fixed partitioning a number of times, and then continually refining the model as such</p>
</li>
</ul>
<h3 id="metric"><a class="markdownIt-Anchor" href="#metric"></a> Metric</h3>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">mse = np.square(errors).mean()
mae = np.abs(errors).mean()
</code></pre>
<p><strong>mse</strong> penalize more large errors than <strong>mae</strong> does.<br>
if large errors are potentially dangerous and they cost you much more than smaller errors, then you may prefer the mse. But if your gain or your loss is just proportional to the size of the error, then the mae may be better.</p>
<h3 id="moving-average-and-differencing"><a class="markdownIt-Anchor" href="#moving-average-and-differencing"></a> Moving average and differencing</h3>
<ol>
<li>Use differencing to cancel out the seasonality and trends</li>
<li>Use moving average to forecast the difference time series</li>
<li>Use moving average to past time series</li>
<li>Add back the smoothed differece to the smoothed past time series</li>
</ol>
<h3 id="trailing-windows-and-centered-windows"><a class="markdownIt-Anchor" href="#trailing-windows-and-centered-windows"></a> Trailing windows and centered windows</h3>
<p>Moving averages using centered windows can be more accurate than using trailing windows. But we can’t use centered windows to smooth present values since we don’t know future values. However, to smooth past values we can afford to use centered windows.</p>
<h2 id="code-13"><a class="markdownIt-Anchor" href="#code-13"></a> Code</h2>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">from tensorflow import keras
def moving_average_forecast(series, window_size):
  &quot;&quot;&quot;Forecasts the mean of the last few values.
     If window_size=1, then this is equivalent to naive forecast&quot;&quot;&quot;
  forecast = []
  for time in range(len(series) - window_size):
    forecast.append(series[time:time + window_size].mean())
  return np.array(forecast)x
print(keras.metrics.mean_squared_error(x_valid, naive_forecast).numpy())
print(keras.metrics.mean_absolute_error(x_valid, naive_forecast).numpy())
</code></pre>
<h1 id="c4w2-deep-neural-networks-for-time-series"><a class="markdownIt-Anchor" href="#c4w2-deep-neural-networks-for-time-series"></a> C4W2: Deep Neural Networks for Time Series</h1>
<h2 id="note-13"><a class="markdownIt-Anchor" href="#note-13"></a> Note</h2>
<h3 id="preparing-feature-and-labels"><a class="markdownIt-Anchor" href="#preparing-feature-and-labels"></a> Preparing feature and labels</h3>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">dataset = tf.data.Dataset.range(10)
dataset = dataset.window(5, shift=1, drop_remainder=True)
dataset = dataset.flat_map(lambda window: window.batch(5))
dataset = dataset.map(lambda window: (window[:-1], window[-1:]))
dataset = dataset.shuffle(buffer_size=10)
dataset = dataset.batch(2).prefetch(1)
for x,y in dataset:
  print(&quot;x = &quot;, x.numpy())
  print(&quot;y = &quot;, y.numpy())
</code></pre>
<ul>
<li>On line 3, each <code>window</code> is an instance of class <code>tensorflow.python.data.ops.dataset_ops._VariantDataset</code> containing 5 elements. But We need to convert it into a tensor, so we just cut it to batches by 5 elements. This is why we have <code>window.batch(5)</code></li>
<li>On line 5, <code>shuffle</code> fills a buffer with <code>buffer_size</code> elements, then randomly samples elements from this buffer, replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or equal to the full size of the dataset is required and the downside is that it really takes long time. If you don’t care about perfect shuffling, choosing a small number of buffer will just speed things up. You can even <code>buffer_size</code> is set to 1, in this case, no shuffle will happen here</li>
<li>On line 6, according to the <a href="https://www.tensorflow.org/guide/performance/datasets" target="_blank" rel="noopener">tensorflow doc</a>:<br>
The <code>tf.data</code> API provides a software pipelining mechanism through the <code>tf.data.Dataset.prefetch</code> transformation, which can be used to decouple the time data is produced from the time it is consumed. In particular, the transformation uses a background thread and an internal buffer to prefetch elements from the input dataset ahead of the time they are requested. Thus, to achieve the pipelining effect illustrated above, you can add <code>prefetch(1)</code> as the final transformation to your dataset pipeline (or <code>prefetch(n)</code> if a single training step consumes n elements).</li>
</ul>
<h3 id="sequence-bias"><a class="markdownIt-Anchor" href="#sequence-bias"></a> Sequence Bias</h3>
<p>Sequence bias is when the order of things can impact the selection of things. For example, if I were to ask you your favorite TV show, and listed “Game of Thrones”, “Killing Eve”, “Travellers” and “Doctor Who” in that order, you’re probably more likely to select ‘Game of Thrones’ as you are familiar with it, and it’s the first thing you see. Even if it is equal to the other TV shows. So, when training data in a dataset, we don’t want the sequence to impact the training in a similar way, so it’s good to shuffle them up.</p>
<h3 id="find-the-best-learning-rate"><a class="markdownIt-Anchor" href="#find-the-best-learning-rate"></a> Find the best learning rate</h3>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">lr_schedule = tf.keras.callbacks.LearningRateScheduler(
    lambda epoch: 1e-8 * 10**(epoch &#x2F; 20))
optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)
model.compile(loss=&quot;mse&quot;, optimizer=optimizer)
history = model.fit(dataset, epochs=100, callbacks=[lr_schedule], verbose=0)
# plot the loss per epoch against the learning rate per epoch
lrs = 1e-8 * (10 ** (np.arange(100) &#x2F; 20))
plt.semilogx(lrs, history.history[&quot;loss&quot;])
plt.axis([1e-8, 1e-3, 0, 300])
</code></pre>
<span itemprop="image" itemscope itemtype="http://schema.org/ImageObject"><img itemprop="url image" src="learning-rate.png" class="full-image" alt title><meta itemprop="width" content="auto"><meta itemprop="height" content="auto"></span>
<p>Here, the best learning rate is around 7e-6, because it is the lowest point of the curve where it’s still relatively stable.</p>
<h1 id="c4w3-recurrent-neural-networks-for-time-series"><a class="markdownIt-Anchor" href="#c4w3-recurrent-neural-networks-for-time-series"></a> C4W3: Recurrent Neural Networks for Time Series</h1>
<h2 id="note-14"><a class="markdownIt-Anchor" href="#note-14"></a> Note</h2>
<p>For numeric series, things such as closer numbers in the series might have a greater impact than those further away from our target value.</p>
<p>In some cases, you might want to input a sequence, but you don’t want to output on and you just want to get a single vector for each instance in the batch. This is typically called a sequence to vector RNN. But in reality, all you do is ignore all of the outputs, except the last one. When using Keras in TensorFlow, this is the default behavior.</p>
<p>If you want the recurrent layer to output a sequence, you have to specify <code>return_sequences=True</code> when creating the layer. You’ll need to do this when you stack one RNN layer on top of another.</p>
<p>(huber loss)[<a href="https://en.wikipedia.org/wiki/Huber_loss" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Huber_loss</a>]<br>
The Huber function is a loss function that’s less sensitive to outliers and as this data can get a little bit noisy, it’s worth giving it a shot.</p>
<h2 id="code-14"><a class="markdownIt-Anchor" href="#code-14"></a> Code</h2>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">tf.keras.backend.clear_session()
dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)

model = tf.keras.models.Sequential([
  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1)),
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 100.0)
])

model.compile(loss=&quot;mse&quot;, optimizer=tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9),metrics=[&quot;mae&quot;])
history = model.fit(dataset,epochs=500,verbose=1)
</code></pre>
<p>Note:<br>
The last lambda layer is used to scale up the outputs by 100, which helps training. The default activation function in the RNN layers is tanH which is the hyperbolic tangent activation. This outputs values between negative one and one. Since the time series values are in that order usually in the 10s like 40s, 50s, 60s, and 70s, then scaling up the outputs to the same ballpark can help us with learning.</p>
<h1 id="c4w4-real-world-time-series-data"><a class="markdownIt-Anchor" href="#c4w4-real-world-time-series-data"></a> C4W4: Real-world time series data</h1>
<h2 id="note-15"><a class="markdownIt-Anchor" href="#note-15"></a> Note</h2>

<!-- Has Prism -->
<pre class="line-numbers" style><code class="language-python">model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters=32, kernel_size=5,
                      strides=1, padding=&quot;causal&quot;,
                      activation=&quot;relu&quot;,
                      input_shape=[None, 1]),
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),
  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),
  tf.keras.layers.Dense(1),
  tf.keras.layers.Lambda(lambda x: x * 200)
])
</code></pre>
<p><code>padding=&quot;causal&quot;</code><br>
This simply pads the layer’s input with zeros in the front so that we can also predict the values of early time steps in the window</p>
<p>A good explanation (here)[<a href="https://theblog.github.io/post/convolution-in-autoregressive-neural-networks/" target="_blank" rel="noopener">https://theblog.github.io/post/convolution-in-autoregressive-neural-networks/</a>]</p>

      
    </div>

    
      


    

    
    
    

    

    
      
    
    

    
      <div>
        




  



<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Hao Ren</li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    
    <a href="http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/" title="Tensorflow in Practice Learning Note">http://invkrh.me/2019/03/11/tensorflow-specialization-learning-note/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.</li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Coding/" rel="tag"># Coding</a>
          
            <a href="/tags/TensorFlow/" rel="tag"># TensorFlow</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/02/23/suan-xiang-feng-mi-ji/" rel="next" title="蒜香蜂蜜鸡">
                <i class="fa fa-chevron-left"></i> 蒜香蜂蜜鸡
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/13/google-hash-code-2019/" rel="prev" title="Google Hash Code 2019">
                Google Hash Code 2019 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/uploads/avatar.jpeg" alt="Hao Ren">
            
              <p class="site-author-name" itemprop="name">Hao Ren</p>
              <div class="site-description motion-element" itemprop="description">Machine Learning Engineer @ Criteo</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">7</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">8</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/invkrh" title="GitHub &rarr; https://github.com/invkrh" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://twitter.com/invkrh" title="Twitter &rarr; https://twitter.com/invkrh" rel="noopener" target="_blank"><i class="fa fa-fw fa-twitter"></i>Twitter</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:invkrh@gmail.com" title="E-Mail &rarr; mailto:invkrh@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          
             <div class="cc-license motion-element" itemprop="license">
              
              
                
              
              
              
              <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
             </div>
          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#c1w1-a-new-programming-paradigm"><span class="nav-text"> C1W1: A New Programming Paradigm</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#note"><span class="nav-text"> Note</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#new-programming-paradigm"><span class="nav-text"> New programming paradigm</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code"><span class="nav-text"> Code</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#how-to-fit-a-line"><span class="nav-text"> How to fit a line</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#c1w2-introduction-to-computer-vision"><span class="nav-text"> C1W2: Introduction to Computer Vision</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#note-2"><span class="nav-text"> Note</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#why-are-the-labels-numbers-instead-of-words"><span class="nav-text"> Why are the labels numbers instead of words</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#what-is-cross-entropy-ce"><span class="nav-text"> What is cross entropy (CE)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#difference-between-categorical_crossentropy-and-sparse_categorical_crossentropy"><span class="nav-text"> Difference between categorical_crossentropy and sparse_categorical_crossentropy</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-2"><span class="nav-text"> Code</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#c1w3-enhancing-vision-with-convolutional-neural-networks"><span class="nav-text"> C1W3: Enhancing Vision with Convolutional Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#note-3"><span class="nav-text"> Note</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#convolution-layer"><span class="nav-text"> Convolution Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#maxpooling-layer"><span class="nav-text"> MaxPooling Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-cnn-works"><span class="nav-text"> Why CNN works</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-3"><span class="nav-text"> Code</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#model"><span class="nav-text"> Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#how-to-compute-output-size"><span class="nav-text"> How to compute output size</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#two-kinds-of-padding"><span class="nav-text"> Two kinds of padding:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#how-to-compute-number-of-parameters"><span class="nav-text"> How to compute number of parameters</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#visualizing-the-convolutions-and-pooling"><span class="nav-text"> Visualizing the Convolutions and Pooling</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#c1w4-using-real-world-images"><span class="nav-text"> C1W4: Using Real-world Images</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#note-4"><span class="nav-text"> Note</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#imagegenerator"><span class="nav-text"> ImageGenerator</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mini-batch"><span class="nav-text"> Mini-batch</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#why-mini-batch"><span class="nav-text"> Why mini-batch</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#optimization"><span class="nav-text"> Optimization</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-4"><span class="nav-text"> Code</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#model-2"><span class="nav-text"> Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#imagedatagenerator"><span class="nav-text"> ImageDataGenerator</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#visualizing-intermediate-representations"><span class="nav-text"> Visualizing Intermediate Representations</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#c2w1-exploring-a-larger-dataset"><span class="nav-text"> C2W1: Exploring a Larger Dataset</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#note-5"><span class="nav-text"> Note</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-5"><span class="nav-text"> Code</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#c2w2-augmentation-a-technique-to-avoid-overfitting"><span class="nav-text"> C2W2: Augmentation: A technique to avoid overfitting</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#note-6"><span class="nav-text"> Note</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#image-augmentation"><span class="nav-text"> Image augmentation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-6"><span class="nav-text"> Code</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#c2w3-transfer-learning"><span class="nav-text"> C2W3: Transfer Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#note-7"><span class="nav-text"> Note</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#what-is-transfer-learning"><span class="nav-text"> What is transfer learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#why-dropout-can-do-the-regularization"><span class="nav-text"> Why dropout can do the regularization</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-7"><span class="nav-text"> Code</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#c2w4-multiclass-classification"><span class="nav-text"> C2W4: Multiclass Classification</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#note-8"><span class="nav-text"> Note</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-8"><span class="nav-text"> Code</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#c3w1-sentiment-in-text"><span class="nav-text"> C3W1: Sentiment in text</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#code-9"><span class="nav-text"> Code</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#c3w2-word-embeddings"><span class="nav-text"> C3W2: Word Embeddings</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#note-9"><span class="nav-text"> Note</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#why-subwords-works-poorly"><span class="nav-text"> Why subwords works poorly</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-10"><span class="nav-text"> Code</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#check-tf-version"><span class="nav-text"> Check TF version</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#download-imdb_reviews-via-tensorflow-datasets"><span class="nav-text"> Download imdb_reviews via tensorflow-datasets</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#prepare-dataset"><span class="nav-text"> Prepare dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#train-word-embedding-label"><span class="nav-text"> Train word embedding label</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#word-embedding-visualization"><span class="nav-text"> Word embedding visualization</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#c3w3-sequence-models"><span class="nav-text"> C3W3: Sequence models</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#note-10"><span class="nav-text"> Note</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#model-comparison"><span class="nav-text"> Model comparison</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#imdb-subwords-8k"><span class="nav-text"> IMDB Subwords 8K</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#sarcasm"><span class="nav-text"> Sarcasm</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-11"><span class="nav-text"> Code</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#c3w4-sequence-models-and-literature"><span class="nav-text"> C3W4: Sequence models and literature</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#note-11"><span class="nav-text"> Note</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-12"><span class="nav-text"> Code</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#c4w1-sequences-and-prediction"><span class="nav-text"> C4W1: Sequences and Prediction</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#note-12"><span class="nav-text"> Note</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#split-training-period-validation-period-test-period"><span class="nav-text"> Split training period, validation period, test period</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#metric"><span class="nav-text"> Metric</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#moving-average-and-differencing"><span class="nav-text"> Moving average and differencing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#trailing-windows-and-centered-windows"><span class="nav-text"> Trailing windows and centered windows</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-13"><span class="nav-text"> Code</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#c4w2-deep-neural-networks-for-time-series"><span class="nav-text"> C4W2: Deep Neural Networks for Time Series</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#note-13"><span class="nav-text"> Note</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#preparing-feature-and-labels"><span class="nav-text"> Preparing feature and labels</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sequence-bias"><span class="nav-text"> Sequence Bias</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#find-the-best-learning-rate"><span class="nav-text"> Find the best learning rate</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#c4w3-recurrent-neural-networks-for-time-series"><span class="nav-text"> C4W3: Recurrent Neural Networks for Time Series</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#note-14"><span class="nav-text"> Note</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#code-14"><span class="nav-text"> Code</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#c4w4-real-world-time-series-data"><span class="nav-text"> C4W4: Real-world time series data</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#note-15"><span class="nav-text"> Note</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hao Ren</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v7.1.2</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.2"></script>

  <script src="/js/motion.js?v=7.1.2"></script>



  
  


  <script src="/js/schemes/muse.js?v=7.1.2"></script>




  
  <script src="/js/scrollspy.js?v=7.1.2"></script>
<script src="/js/post-details.js?v=7.1.2"></script>



  


  <script src="/js/next-boot.js?v=7.1.2"></script>


  
  <script src="/js/js.cookie.js?v=7.1.2"></script>
  <script src="/js/scroll-cookie.js?v=7.1.2"></script>


  

  

  
  

  

<script src="//cdn.jsdelivr.net/npm/leancloud-storage@latest/dist/av-min.js"></script>



<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'iV2jaxdX6MY6HWbxpm1WeGfa-MdYXbMMI',
    appKey: 'jECEbbCLVpkrgzQxGeNA9B3w',
    placeholder: 'Please fill your email for reply notification',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: true,
    lang: 'en' || 'zh-cn'
  });
</script>




  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
  

  
  

  
    
      <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css">

  
    <script src="//cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/copy-tex.min.js"></script>
  
  
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/copy-tex.min.css">
  


    
  


  

  

  

  

  

  

  

  

  

  

  

  
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.0/clipboard.min.js"></script><!-- hexo-inject:begin --><script type='text/javascript' src='/js/prism.js'></script><!-- hexo-inject:end -->
</body>
</html>
