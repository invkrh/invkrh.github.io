<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Linear Algebra Proofs]]></title>
    <url>%2F2019%2F06%2F11%2Flinear-algebra-proofs%2F</url>
    <content type="text"><![CDATA[A list of useful proofs on linear algebra Note: matrix cookbook is an extremely useful material when you deal with derivatives of matrices and vectors. 1. Positive definite symmetric matrix A real symmetric n×nn \times nn×n matrix AAA is called positive definite if xTAx&gt;0x^TAx&gt;0xTAx&gt;0 for all non-zero vectors x∈Rnx \in \mathbb{R}^nx∈Rn, then the following holds: the eigenvalues of A are all positive. AAA is invertible. A−1A^{−1}A−1 is symmetric. A−1A^{−1}A−1 is positive-definite. Proof 1.1: the eigenvalues of A are all positive Let λ\lambdaλ be a (real) eigenvalue of AAA and let xxx be a corresponding real eigenvector. That is, we have Ax=λxAx=\lambda x Ax=λx Then we multiply by xTx^TxT on left and obtain, xTAx=λxTx=λ∥x∥2\begin{aligned} x^T A x &amp;=\lambda x^T x \\ &amp;=\lambda\|x\|^{2} \end{aligned} xTAx​=λxTx=λ∥x∥2​ The left hand side is positive as AAA is positive definite and xxx is a nonzero vector as it is an eigenvector. Since the norm ∥x∥2\|x\|^2∥x∥2 is positive, we must have λ\lambdaλ is positive. It follows that every eigenvalue λ\lambdaλ of AAA is real. Proof 1.2: A is invertible Since proof 1, the matrix AAA does not have 0 as an eigenvalue We can prove this by contradiction: If Ax=0⋅xAx=0 \cdot xAx=0⋅x for some x≠0x \neq 0x​=0 then by definition of eigenvalues (non-invertible), xxx is an eigenvector with eigenvalue λ=0\lambda = 0λ=0 Alternatively, we can prove this by using determinent det⁡A=∏inλi&gt;0\operatorname{det} A = \prod_i^n \lambda_i \gt 0 detA=i∏n​λi​&gt;0 Proof 1.3: the inverse of A is symmetric Let AAA be the invertible symmetric matrix, then AA−1=IAA^{-1} = IAA−1=I and A=ATA = A^TA=AT. Since I=ITI = I^TI=IT, AA−1=(AA−1)TA A^{-1}=\left(A A^{-1}\right)^{T} AA−1=(AA−1)T Since (AB)T=BTAT(A B)^{T}=B^{T} A^{T}(AB)T=BTAT, AA−1=(A−1)TATA A^{-1}=\left(A^{-1}\right)^{T} A^{T} AA−1=(A−1)TAT Since AA−1=A−1A=IA A^{-1}=A^{-1} A=IAA−1=A−1A=I, we rearrange the left side, A−1A=(A−1)TATA^{-1} A=\left(A^{-1}\right)^{T} A^{T} A−1A=(A−1)TAT Since A=ATA = A^TA=AT, we substitute the right side, A−1A=(A−1)TAA−1A(A−1)=(A−1)TA(A−1)A−1I=(A−1)TIA−1=(A−1)T\begin{aligned} A^{-1} A &amp;=\left(A^{-1}\right)^{T} A \\ A^{-1} A\left(A^{-1}\right) &amp;=\left(A^{-1}\right)^{T} A\left(A^{-1}\right) \\ A^{-1} I &amp;=\left(A^{-1}\right)^{T} I \\ A^{-1} &amp;=\left(A^{-1}\right)^{T} \end{aligned} A−1AA−1A(A−1)A−1IA−1​=(A−1)TA=(A−1)TA(A−1)=(A−1)TI=(A−1)T​ Proof 1.4: the inverse of A is positive-definite 2. Matrix derivative Proof 2.1 Knowing that AAA is an invertible symmetric matrix, then ∂log⁡det⁡(A)∂A=A−1\dfrac{\partial \log \operatorname{det}(A)}{\partial A} = A^{-1}∂A∂logdet(A)​=A−1 Since the chain rule, ∂log⁡det⁡(A)∂A=1det⁡(A)∂det⁡(A)∂A\begin{aligned} \frac{\partial \log \operatorname{det}(A)}{\partial A} &amp;= \frac{1}{\operatorname{det}(A)} \frac{\partial \operatorname{det}(A)}{\partial A} \end{aligned} ∂A∂logdet(A)​​=det(A)1​∂A∂det(A)​​ According to the cookbook: ∂det⁡(A)∂A=det⁡(A)(A−1)T\dfrac{\partial \operatorname{det}(A)}{\partial A}=\operatorname{det}(A)\left(A^{-1}\right)^{T}∂A∂det(A)​=det(A)(A−1)T, ∂log⁡det⁡(A)∂A=(A−1)T\frac{\partial \log \operatorname{det}(A)}{\partial A} = \left(A^{-1}\right)^{T} ∂A∂logdet(A)​=(A−1)T Since the proof 1, ∂log⁡det⁡(A)∂A=A−1\frac{\partial \log \operatorname{det}(A)}{\partial A} = A^{-1} ∂A∂logdet(A)​=A−1 Q.E.D.]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Linear Algebra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Expectation-Maximization Algorithm Explained]]></title>
    <url>%2F2019%2F05%2F14%2Fexpectation-maximization-explained%2F</url>
    <content type="text"><![CDATA[The EM algorithm is used to find (local) maximum likelihood parameters of a statistical model in cases where the equations cannot be solved directly. Typically these models involve latent variables in addition to unknown parameters and known data observations. In this post, we will talk about how the algorithm works and then prove its correctness, finally we will show a concrete yet the most common use case where the algorithm is applied. Description The typical setting of the EM algorithm is a parameter estimation problem in which we have A training set X={x1,…,xn}\mathbf{X} = \{ x_1, \ldots, x_n \}X={x1​,…,xn​} consisting of nnn independent observed data points. Each may be discrete or continuous. Associated with each data point may be a vector of observations Y={y1,…,yn}\mathbf{Y} = \{ y_1, \ldots, y_n \}Y={y1​,…,yn​}. A set of unobserved latent data or missing values Z={z1,…,zn}\mathbf{Z} = \{ z_1, \ldots, z_n \}Z={z1​,…,zn​} associated with each data point. They are discrete, drawn from a fixed number of values, and with one latent variable per observed unit. A vector of unknown parameters θ\boldsymbol{\theta}θ which are continuous, and are of two kinds: Parameters that are associated with all data points Those associated with a specific value of a latent variable (i.e., associated with all data points which corresponding latent variable has that value). We wish to fit the parameters of a model p(x;θ)p(x ; \boldsymbol{\theta})p(x;θ) (may or may not include yyy) and the log marginal likelihood function of all data points to maximize is given by L(θ;X)= ∑i=1nlog⁡p(xi;θ)= ∑i=1nlog⁡∑zip(xi,zi;θ)\begin{aligned} &amp; L(\boldsymbol{\theta};\mathbf{X}) \\ = \ \ &amp;\sum_{i=1}^{n} \log p(x_i ; \boldsymbol{\theta}) \\ = \ \ &amp;\sum_{i=1}^{n} \log \sum_{z_i} p(x_i, z_i ; \boldsymbol{\theta}) \end{aligned} = = ​L(θ;X)i=1∑n​logp(xi​;θ)i=1∑n​logzi​∑​p(xi​,zi​;θ)​ However, maximizing L(θ;X)L(\boldsymbol{\theta};\mathbf{X})L(θ;X) explicityly might be difficult because Z\mathbf{Z}Z are the latent random variables. So let’s refine it by Bayes’ theorem: ∑i=1nlog⁡p(xi;θ)= ∑i=1n(log⁡p(xi,zi;θ)−log⁡p(zi∣xi;θ))= ∑i=1nlog⁡p(xi,zi;θ)−∑i=1nlog⁡p(zi∣xi;θ)\begin{aligned} &amp; \sum_{i=1}^{n} \log p(x_i ; \boldsymbol{\theta}) \\ = \ \ &amp; \sum_{i=1}^{n} (\log p(x_i, z_i ; \boldsymbol{\theta}) - \log p(z_i | x_i; \boldsymbol{\theta})) \\ = \ \ &amp; \sum_{i=1}^{n} \log p(x_i, z_i ; \boldsymbol{\theta}) - \sum_{i=1}^{n} \log p(z_i | x_i; \boldsymbol{\theta}) \end{aligned} = = ​i=1∑n​logp(xi​;θ)i=1∑n​(logp(xi​,zi​;θ)−logp(zi​∣xi​;θ))i=1∑n​logp(xi​,zi​;θ)−i=1∑n​logp(zi​∣xi​;θ)​ Denote: L(θ;X,Z)=∑i=1nlog⁡p(xi,zi;θ)L(\boldsymbol{\theta}; \mathbf{X}, \mathbf{Z}) = \sum_{i=1}^{n} \log p(x_i, z_i ; \boldsymbol{\theta}) L(θ;X,Z)=i=1∑n​logp(xi​,zi​;θ) Then: L(θ;X)=L(θ;X,Z)−∑i=1nlog⁡p(zi∣xi;θ)L(\boldsymbol{\theta};\mathbf{X}) = L(\boldsymbol{\theta}; \mathbf{X}, \mathbf{Z}) - \sum_{i=1}^{n} \log p(z_i | x_i; \boldsymbol{\theta}) L(θ;X)=L(θ;X,Z)−i=1∑n​logp(zi​∣xi​;θ) This equation will be used to prove the correctness. Let’s denote it as equation (eq.∗)(eq.*)(eq.∗) The EM algorithm seeks to find the MLE of the marginal likelihood by iteratively applying these two steps: Expectation step (E step) Define Q(θ∣θ(t))Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)})Q(θ∣θ(t)) as the expected value of the log likelihood function of θ\boldsymbol{\theta}θ, with respect to the current conditional distribution of Z\mathbf{Z}Z, given X\mathbf{X}X and the current estimates of the parameters θ(t)\boldsymbol{\theta}^{(t)}θ(t) Q(θ∣θ(t))= Ez∣x;θ(t)[L(θ;X,Z)]= Ez∣x;θ(t)[∑i=1nlog⁡p(xi,zi;θ)]= ∑i=1nEzi∣xi;θ(t)[log⁡p(xi,zi;θ)]= ∑i=1n∑zip(zi∣xi;θ(t))log⁡p(xi,zi;θ)\begin{aligned} &amp; Q\left(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)} \right) \\ = \ \ &amp; \mathrm{E}_{z \mid x; \boldsymbol{\theta}^{(t)}} \left [ L(\boldsymbol{\theta}; \mathbf{X}, \mathbf{Z}) \right ] \\ = \ \ &amp; \mathrm{E}_{z \mid x; \boldsymbol{\theta}^{(t)}} \left [ \sum_{i=1}^{n} \log p(x_i, z_i ; \boldsymbol{\theta}) \right ] \\ = \ \ &amp; \sum_{i=1}^{n} \mathrm{E}_{z_i \mid x_i; \boldsymbol{\theta}^{(t)}} \left [ \log p(x_i, z_i ; \boldsymbol{\theta}) \right ] \\ = \ \ &amp; \sum_{i=1}^{n} \sum_{z_i} p(z_i \mid x_i; \boldsymbol{\theta}^{(t)}) \log p(x_i, z_i ; \boldsymbol{\theta}) \end{aligned} = = = = ​Q(θ∣θ(t))Ez∣x;θ(t)​[L(θ;X,Z)]Ez∣x;θ(t)​[i=1∑n​logp(xi​,zi​;θ)]i=1∑n​Ezi​∣xi​;θ(t)​[logp(xi​,zi​;θ)]i=1∑n​zi​∑​p(zi​∣xi​;θ(t))logp(xi​,zi​;θ)​ Maximization step (M step) Find the parameters that maximize this quantity: θ(t+1)=arg⁡max⁡θ Q(θ∣θ(t))\boldsymbol{\theta}^{(t+1)} = \underset{\boldsymbol{\theta}}{\arg \max} \, Q\left(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}\right) θ(t+1)=θargmax​Q(θ∣θ(t)) Proof of Correctness The trick of EM algorithm is to improve Q(θ∣θ(t))Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)})Q(θ∣θ(t)) rather than directly improving L(θ;X)L(\boldsymbol{\theta};\mathbf{X})L(θ;X). Here is shown that improvements to the former imply improvements to the latter. We take the expectation over possible values of the unknown data ziz_izi​ under the current parameter estimate θ(t)\boldsymbol{\theta}^{(t)}θ(t) for both sides of (eq.∗)(eq.*)(eq.∗) equation by multiplying p(zi∣xi;θ(t))p(z_i \mid x_i; \boldsymbol{\theta}^{(t)})p(zi​∣xi​;θ(t)) and summing (or integrating) over zzz . The left-hand side is the expectation of a constant since it is independent of ziz_izi​, so we get: L(θ;X)= Ez∣x;θ(t)[L(θ;X,Z)]−Ez∣x;θ(t)[∑i=1nlog⁡p(zi∣xi;θ)]= Q(θ∣θ(t))−∑i=1n∑zip(zi∣xi;θ(t))log⁡p(zi∣xi;θ)= Q(θ∣θ(t))+H(θ∣θ(t))\begin{aligned} &amp; L(\boldsymbol{\theta};\mathbf{X}) \\ = \ \ &amp; \mathrm{E}_{z \mid x; \boldsymbol{\theta}^{(t)}} \left [ L(\boldsymbol{\theta}; \mathbf{X}, \mathbf{Z}) \right ] - \mathrm{E}_{z \mid x; \boldsymbol{\theta}^{(t)}} \left[\sum_{i=1}^{n} \log p(z_i | x_i; \boldsymbol{\theta}) \right] \\ = \ \ &amp; Q \left(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}\right) - \sum_{i=1}^{n}\sum_{z_i} p \left( z_i \mid x_i; \boldsymbol{\theta}^{(t)} \right) \log p(z_i \mid x_i ; \boldsymbol{\theta}) \\ = \ \ &amp; Q \left( \boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)} \right) + H \left( \boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)} \right) \end{aligned} = = = ​L(θ;X)Ez∣x;θ(t)​[L(θ;X,Z)]−Ez∣x;θ(t)​[i=1∑n​logp(zi​∣xi​;θ)]Q(θ∣θ(t))−i=1∑n​zi​∑​p(zi​∣xi​;θ(t))logp(zi​∣xi​;θ)Q(θ∣θ(t))+H(θ∣θ(t))​ where Hi(θ∣θ(t))H_i \left(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}\right)Hi​(θ∣θ(t)) is defined by the negated sum it is replacing. This last equation holds for every value of θ\boldsymbol{\theta}θ including θ=θ(t)\boldsymbol{\theta} = \boldsymbol{\theta}^{(t)}θ=θ(t), L(θ(t);X)=Q(θ(t)∣θ(t))+H(θ(t)∣θ(t))L\left( \boldsymbol{\theta}^{(t)};\mathbf{X} \right) = Q \left( \boldsymbol{\theta}^{(t)} \mid \boldsymbol{\theta}^{(t)} \right) + H \left( \boldsymbol{\theta}^{(t)} \mid \boldsymbol{\theta}^{(t)} \right) L(θ(t);X)=Q(θ(t)∣θ(t))+H(θ(t)∣θ(t)) and subtracting this last equation from the previous equation gives L(θ;X)−L(θ(t);X)=Q(θ∣θ(t))−Q(θ(t)∣θ(t))+H(θ∣θ(t))−H(θ(t)∣θ(t))L(\boldsymbol{\theta};\mathbf{X}) - L\left( \boldsymbol{\theta}^{(t)};\mathbf{X} \right) = Q \left( \boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)} \right) - Q \left( \boldsymbol{\theta}^{(t)} \mid \boldsymbol{\theta}^{(t)} \right) + H \left( \boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)} \right) - H \left( \boldsymbol{\theta}^{(t)} \mid \boldsymbol{\theta}^{(t)} \right) L(θ;X)−L(θ(t);X)=Q(θ∣θ(t))−Q(θ(t)∣θ(t))+H(θ∣θ(t))−H(θ(t)∣θ(t)) Note that H(θ∣θ(t))−H(θ(t)∣θ(t))= ∑i=1n−∑zip(zi∣xi;θ(t))log⁡p(zi∣xi;θ)⏟cross−entropy(zi∣xi;θ(t),zi∣xi;θ)−[−∑zip(zi∣xi;θ(t))log⁡p(zi∣xi;θ(t))]⏟entropy(zi∣xi;θ(t))\begin{aligned} &amp; H \left( \boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)} \right) - H \left( \boldsymbol{\theta}^{(t)} \mid \boldsymbol{\theta}^{(t)} \right) \\ = \ \ &amp; \sum_{i=1}^{n} \underbrace{- \sum_{z_i} p \left( z_i \mid x_i; \boldsymbol{\theta}^{(t)} \right) \log p(z_i \mid x_i ; \boldsymbol{\theta})}_{cross-entropy(z_i \mid x_i; \boldsymbol{\theta}^{(t)}, z_i \mid x_i; \boldsymbol{\theta})} - \underbrace{\left[ - \sum_{z_i} p \left( z_i \mid x_i; \boldsymbol{\theta}^{(t)} \right) \log p(z_i \mid x_i ; \boldsymbol{\theta}^{(t)}) \right]}_{entropy(z_i \mid x_i; \boldsymbol{\theta}^{(t)})} \end{aligned} = ​H(θ∣θ(t))−H(θ(t)∣θ(t))i=1∑n​cross−entropy(zi​∣xi​;θ(t),zi​∣xi​;θ)−zi​∑​p(zi​∣xi​;θ(t))logp(zi​∣xi​;θ)​​−entropy(zi​∣xi​;θ(t))[−zi​∑​p(zi​∣xi​;θ(t))logp(zi​∣xi​;θ(t))]​​​ Based on Gibbs’ inequality: the information entropy of a distribution P is less than or equal to its cross entropy with any other distribution Q, which means H(θ∣θ(t))≥H(θ(t)∣θ(t))H\left(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)} \right) \geq H\left(\boldsymbol{\theta}^{(t)} \mid \boldsymbol{\theta}^{(t)} \right) H(θ∣θ(t))≥H(θ(t)∣θ(t)) Hence, we can conclude that L(θ;X)−L(θ(t);X)≥Q(θ∣θ(t))−Q(θ(t)∣θ(t))L(\boldsymbol{\theta};\mathbf{X}) - L\left( \boldsymbol{\theta}^{(t)};\mathbf{X} \right) \geq Q \left( \boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)} \right) - Q \left( \boldsymbol{\theta}^{(t)} \mid \boldsymbol{\theta}^{(t)} \right) L(θ;X)−L(θ(t);X)≥Q(θ∣θ(t))−Q(θ(t)∣θ(t)) In words, choosing θ\boldsymbol{\theta}θ to improve Q(θ∣θ(t))Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)})Q(θ∣θ(t)) causes L(θ;X)L(\boldsymbol{\theta};\mathbf{X})L(θ;X) to improve at least as much. Alternatively, you can also prove the correctness by using Jensen’s equality. More details in this note. Gaussian Mixture Model (GMM) A typical application of EM algorithm is the gaussian mixture model (GMM) which is a clustering problem as the following: A training set X={x1,…,xn}\mathbf{X} = \{ x_1, \ldots, x_n \}X={x1​,…,xn​} which is a sample of nnn independent observations from a mixture of kkk multivariate normal distributions of dimension ddd. (since we are in the unsupervised learning setting, these points do not come with any labels.) Associated with the latent variables Z={z1,z2,…,zn}\mathbf{Z} = \{ z_1,z_2,\ldots,z_n \}Z={z1​,z2​,…,zn​} that determine the component from which the observation originates. zi∼ Multinomial (ϕ)z_i \sim \text { Multinomial }(\phi)zi​∼ Multinomial (ϕ) where ϕj≥0,∑j=1kϕj=1\phi_j \geq 0, \sum_{j=1}^{k} \phi_j=1ϕj​≥0,∑j=1k​ϕj​=1 and the parameter ϕj\phi_jϕj​ gives p(zi=j)p(z_i=j)p(zi​=j). The model posits that each xix_ixi​ was generated by randomly choosing ziz_izi​ from 1,…,k{1, \ldots, k}1,…,k, and then xix_ixi​ was drawn from one of kkk Gaussians depending on ziz_izi​. Then, xi∣zi=j∼N(μj,Σj)x_i | z_i=j \sim \mathcal{N}\left(\mu_{j}, \Sigma_{j}\right)xi​∣zi​=j∼N(μj​,Σj​) The parameters to estimate θ={ϕj,μj,Σj∣j∈1,…,k}\boldsymbol{\theta} = \{\phi_j, \mu_j, \Sigma_j | j \in 1,\ldots,k\}θ={ϕj​,μj​,Σj​∣j∈1,…,k}. For simplicity, θj={ϕj,μj,Σj}\theta_j = \{\phi_j, \mu_j, \Sigma_j\}θj​={ϕj​,μj​,Σj​} And we wish to model the data by specifying the joint distribution p(xi,zi)p(x_i, z_i)p(xi​,zi​). Hence, the log likelihood is L(θ;X)= ∑i=1nlog⁡∑j=1kp(xi,zi=j;θj)= ∑i=1nlog⁡∑j=1kp(xi∣zi=j;μj,Σj)⋅p(zi=j;ϕj)= ∑i=1nlog⁡∑j=1kf(xi;μj,Σj)⋅ϕj\begin{aligned} &amp; L(\boldsymbol{\theta} ; \mathbf{X}) \\ = \ \ &amp; \sum_{i=1}^{n} \log \sum_{j=1}^{k} p(x_i, z_i = j; \theta_j) \\ = \ \ &amp; \sum_{i=1}^{n} \log \sum_{j=1}^{k} p(x_i \mid z_i = j; \mu_j, \Sigma_j) \cdot p(z_i = j; \phi_j) \\ = \ \ &amp; \sum_{i=1}^{n} \log \sum_{j=1}^{k} f\left(x_i ; \mu_j, \Sigma_j\right) \cdot \phi_j \end{aligned} = = = ​L(θ;X)i=1∑n​logj=1∑k​p(xi​,zi​=j;θj​)i=1∑n​logj=1∑k​p(xi​∣zi​=j;μj​,Σj​)⋅p(zi​=j;ϕj​)i=1∑n​logj=1∑k​f(xi​;μj​,Σj​)⋅ϕj​​ However, if we set to zero the derivatives of this formula with respect to the parameters and try to solve, we’ll find that it is not possible to find the maximum likelihood estimates of the parameters in closed form. (Try this yourself at home.) This is where the EM algorithm comes in. E-step Define Q(θ∣θ(t))Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)})Q(θ∣θ(t)) L(θ;X,Z)=∑i=1nlog⁡f(xi;μj,Σj)⋅ϕjL(\theta ; \mathbf{X}, \mathbf{Z}) = \sum_{i=1}^{n} \log f\left(x_i ; \mu_j, \Sigma_j\right) \cdot \phi_{j} L(θ;X,Z)=i=1∑n​logf(xi​;μj​,Σj​)⋅ϕj​ where f(xi;μj,Σj)=1(2π)d/2∣Σj∣1/2exp⁡(−12(xi−μj)TΣj−1(xi−μj))f\left(x_i ; \mu_j, \Sigma_j\right) = \frac{1}{(2 \pi)^{d / 2}\left|\Sigma_{j}\right|^{1 / 2}} \exp \left(-\frac{1}{2}\left(x_i-\mu_{j}\right)^{T} \Sigma_{j}^{-1}\left(x_i-\mu_{j}\right)\right) f(xi​;μj​,Σj​)=(2π)d/2∣Σj​∣1/21​exp(−21​(xi​−μj​)TΣj−1​(xi​−μj​)) then Q(θ∣θ(t))=∑i=1n∑j=1kp(zi=j∣xi;θ(t))⋅log⁡f(xi;μj,Σj)⋅ϕjQ(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}) = \sum_{i=1}^{n} \sum_{j=1}^{k} p(z_i = j \mid x_i; \boldsymbol{\theta}^{(t)}) \cdot \log f\left(x_i ; \mu_j, \Sigma_j\right) \cdot \phi_j Q(θ∣θ(t))=i=1∑n​j=1∑k​p(zi​=j∣xi​;θ(t))⋅logf(xi​;μj​,Σj​)⋅ϕj​ Compute wij(t)w_{ij}^{(t)}wij(t)​ Given our current estimate of the parameters θ(t)\theta^{(t)}θ(t), the conditional distribution of the ziz_izi​ is determined by Bayes theorem to be normalized Gaussian density weighted by ϕj\phi_jϕj​: wij(t)= p(zi=j∣xi;θ(t))= p(xi,zi=j;θ(t))p(xi;θ(t))= p(xi,zi=j;θ(t))∑l=1kp(xi,zi=l;θ(t))= f(xi;μj(t),Σj(t))⋅ϕj(t)∑l=1kf(xi;μl(t),Σl(t))⋅ϕl(t)\begin{aligned} &amp; w_{ij}^{(t)} \\ = \ \ &amp; p(z_i = j \mid x_i; \boldsymbol{\theta}^{(t)}) \\ = \ \ &amp; \frac{p(x_i, z_i = j ; \boldsymbol{\theta}^{(t)})}{p(x_i; \boldsymbol{\theta}^{(t)})} \\ = \ \ &amp; \frac{p(x_i, z_i = j ; \boldsymbol{\theta}^{(t)})}{\sum_{l = 1}^{k} p(x_i, z_i = l; \boldsymbol{\theta}^{(t)})} \\ = \ \ &amp; \frac{f\left(x_i ; \mu_j^{(t)}, \Sigma_j^{(t)}\right) \cdot \phi_{j}^{(t)}}{\sum_{l = 1}^{k} f\left(x_i ; \mu_l^{(t)}, \Sigma_l^{(t)}\right) \cdot \phi_{l}^{(t)}} \end{aligned} = = = = ​wij(t)​p(zi​=j∣xi​;θ(t))p(xi​;θ(t))p(xi​,zi​=j;θ(t))​∑l=1k​p(xi​,zi​=l;θ(t))p(xi​,zi​=j;θ(t))​∑l=1k​f(xi​;μl(t)​,Σl(t)​)⋅ϕl(t)​f(xi​;μj(t)​,Σj(t)​)⋅ϕj(t)​​​ M-step We need to maximize, with respect to our parameters θ(t)\boldsymbol{\theta}^{(t)}θ(t), the quantity Q(θ∣θ(t))= ∑i=1n∑j=1kwij(t)log⁡1(2π)d/2∣Σj∣1/2exp⁡(−12(xi−μj)TΣj−1(xi−μj))⋅ϕj= ∑i=1n∑j=1kwij(t)[log⁡ϕj−12log⁡∣Σj∣−12(xi−μj)TΣj−1(xi−μj)−d2log⁡(2π)]\begin{aligned} &amp; Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}) \\ = \ \ &amp; \sum_{i=1}^{n}\sum_{j=1}^{k} w_{ij}^{(t)} \log \frac{1}{(2 \pi)^{d / 2}\left|\Sigma_{j}\right|^{1 / 2}} \exp \left(-\frac{1}{2}\left(x_i-\mu_{j}\right)^{T} \Sigma_{j}^{-1}\left(x_i-\mu_{j}\right)\right) \cdot \phi_{j} \\ = \ \ &amp; \sum_{i=1}^{n}\sum_{j=1}^{k} w_{ij}^{(t)} \left[\log \phi_j - \frac{1}{2} \log \left|\Sigma_{j}\right| - \frac{1}{2}(x_i-\mu_j)^T \Sigma_{j}^{-1}(x_i - \mu_j) - \frac{d}{2} \log(2\pi) \right] \end{aligned} = = ​Q(θ∣θ(t))i=1∑n​j=1∑k​wij(t)​log(2π)d/2∣Σj​∣1/21​exp(−21​(xi​−μj​)TΣj−1​(xi​−μj​))⋅ϕj​i=1∑n​j=1∑k​wij(t)​[logϕj​−21​log∣Σj​∣−21​(xi​−μj​)TΣj−1​(xi​−μj​)−2d​log(2π)]​ Update ϕj\phi_jϕj​ Grouping together only the terms that depend on ϕj\phi_jϕj​, we find that we need to maximize ∑i=1n∑j=1kwij(t)log⁡ϕj\sum_{i=1}^{n}\sum_{j=1}^{k} w_{ij}^{(t)} \log \phi_j i=1∑n​j=1∑k​wij(t)​logϕj​ with subject to ∑j=1kϕj=1\sum_{j=1}^{k} \phi_j = 1 j=1∑k​ϕj​=1 So we construct the Lagrangian L(ϕ)=∑i=1n∑j=1kwij(t)log⁡ϕj+λ(∑j=1kϕj−1)\mathcal{L}(\phi) = \sum_{i=1}^{n}\sum_{j=1}^{k} w_{ij}^{(t)} \log \phi_j + \lambda \left( \sum_{j=1}^{k} \phi_j - 1 \right) L(ϕ)=i=1∑n​j=1∑k​wij(t)​logϕj​+λ(j=1∑k​ϕj​−1) where λ\lambdaλ is the Lagrange multiplier. Taking derivative, we find ∂∂ϕjL(ϕ)=∑i=1nwij(t)ϕj+λ\frac{\partial}{\partial \phi_{j}} \mathcal{L}(\phi)=\sum_{i=1}^{n} \frac{w_{ij}^{(t)}}{\phi_{j}}+\lambda ∂ϕj​∂​L(ϕ)=i=1∑n​ϕj​wij(t)​​+λ Setting this to zero and solving, we get ϕj=∑i=1nwij(t)−λ\phi_{j}=\frac{\sum_{i=1}^{n} w_{ij}^{(t)}}{-\lambda} ϕj​=−λ∑i=1n​wij(t)​​ Using the constraint that ∑j=1kϕj=1\sum_{j=1}^{k} \phi_j = 1∑j=1k​ϕj​=1 and knowing the fact that ∑j=1kwij(t)=1\sum_{j=1}^{k} w_{ij}^{(t)} = 1∑j=1k​wij(t)​=1 (probabilities sum to 1), we easily find: −λ=∑i=1n∑j=1kwij(t)=∑i=1n1=n-\lambda=\sum_{i=1}^{n} \sum_{j=1}^{k} w_{ij}^{(t)} = \sum_{i=1}^{n} 1= n −λ=i=1∑n​j=1∑k​wij(t)​=i=1∑n​1=n We therefore have updates for the parameters ϕj\phi_jϕj​: ϕj:=1n∑i=1nwij(t)\phi_{j} :=\frac{1}{n} \sum_{i=1}^{n} w_{ij}^{(t)} ϕj​:=n1​i=1∑n​wij(t)​ Update μj\mu_jμj​ ∂∂μjQ(θ∣θ(t))= ∂∂μj∑i=1n−12wij(t)(xi−μj)TΣj−1(xi−μj)= 12∑i=1nwij(t)∂∂μj(2μjTΣj−1xi−μjTΣj−1μj)= ∑i=1nwij(t)(Σj−1xi−Σj−1μj)\begin{aligned} &amp; \frac{\partial}{\partial \mu_{j}} Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}) \\ = \ \ &amp; \frac{\partial}{\partial \mu_{j}} \sum_{i=1}^{n} -\frac{1}{2} w_{ij}^{(t)} (x_i-\mu_j)^T \Sigma_{j}^{-1}(x_i - \mu_j) \\ = \ \ &amp; \frac{1}{2} \sum_{i=1}^{n} w_{ij}^{(t)} \dfrac{\partial}{\partial \mu_{j}} \left( 2 \mu_j^T\Sigma_j^{-1}x_i - \mu_j^T\Sigma_j^{-1}\mu_j \right) \\ = \ \ &amp; \sum_{i=1}^{n} w_{ij}^{(t)} \left( \Sigma_j^{-1} x_i - \Sigma_j^{-1}\mu_j \right) \end{aligned} = = = ​∂μj​∂​Q(θ∣θ(t))∂μj​∂​i=1∑n​−21​wij(t)​(xi​−μj​)TΣj−1​(xi​−μj​)21​i=1∑n​wij(t)​∂μj​∂​(2μjT​Σj−1​xi​−μjT​Σj−1​μj​)i=1∑n​wij(t)​(Σj−1​xi​−Σj−1​μj​)​ Setting this to zero and solving for μj\mu_jμj​ therefore yields the update rule μj:=∑i=1nwij(t)xi∑i=1nwij(t)\mu_{j} :=\frac{\sum_{i=1}^{n} w_{ij}^{(t)} x_i}{\sum_{i=1}^{n} w_{ij}^{(t)}} μj​:=∑i=1n​wij(t)​∑i=1n​wij(t)​xi​​ Update Σj\Sigma_jΣj​ ∂∂ΣjQ(θ∣θ(t))= ∂∂Σj∑i=1n−12wij(t)[log⁡∣Σj∣+(xi−μj)TΣj−1(xi−μj)]= −12∑i=1nwij(t)∂∂Σj[log⁡∣Σj∣+(xi−μj)TΣj−1(xi−μj)]= −12∑i=1nwij(t)(Σj−1−(xi−μj)(xi−μj)TΣj−2)\begin{aligned} &amp; \frac{\partial}{\partial \Sigma_{j}} Q(\boldsymbol{\theta} \mid \boldsymbol{\theta}^{(t)}) \\ = \ \ &amp; \frac{\partial}{\partial \Sigma_{j}} \sum_{i=1}^{n} -\frac{1}{2} w_{ij}^{(t)} \left[ \log\left|\Sigma_{j}\right| + (x_i-\mu_j)^T \Sigma_{j}^{-1}(x_i - \mu_j)\right] \\ = \ \ &amp; -\frac{1}{2} \sum_{i=1}^{n} w_{ij}^{(t)} \dfrac{\partial}{\partial \Sigma_{j}} \left[ \log\left|\Sigma_{j}\right| + (x_i-\mu_j)^T \Sigma_{j}^{-1}(x_i - \mu_j)\right] \\ = \ \ &amp; -\frac{1}{2} \sum_{i=1}^{n} w_{ij}^{(t)} \left( \Sigma_{j}^{-1} - (x_i-\mu_j)(x_i - \mu_j)^T \Sigma_{j}^{-2} \right) \end{aligned} = = = ​∂Σj​∂​Q(θ∣θ(t))∂Σj​∂​i=1∑n​−21​wij(t)​[log∣Σj​∣+(xi​−μj​)TΣj−1​(xi​−μj​)]−21​i=1∑n​wij(t)​∂Σj​∂​[log∣Σj​∣+(xi​−μj​)TΣj−1​(xi​−μj​)]−21​i=1∑n​wij(t)​(Σj−1​−(xi​−μj​)(xi​−μj​)TΣj−2​)​ Setting the partial derivative to zero and solving for Σj\Sigma_jΣj​ therefore yields the update rule Σj:=∑i=1nwij(t)(xi−μj)(xi−μj)T∑i=1nwij(t)\Sigma_{j} :=\frac{\sum_{i=1}^{n} w_{ij}^{(t)} (x_i-\mu_j)(x_i - \mu_j)^T}{\sum_{i=1}^{n} w_{ij}^{(t)}} Σj​:=∑i=1n​wij(t)​∑i=1n​wij(t)​(xi​−μj​)(xi​−μj​)T​ More details on the derivative calculus Note that Σj\Sigma_{j}Σj​ is invertible, symmetric, square, positive-definite matrix, then the following holds The inverse of a symmetric matrix is symmetric, then (Σj−1)T=Σj−1(\Sigma_{j}^{-1})^T = \Sigma_{j}^{-1}(Σj−1​)T=Σj−1​ Σj\Sigma_{j}Σj​ is positive-definite, then Σj−1\Sigma_{j}^{-1}Σj−1​ is positive-definite Positive-definite matrix is invertible, then Σj−2\Sigma_{j}^{-2}Σj−2​ exists So we get ∂∂Σjlog⁡∣Σj∣= 1∣Σj∣∂∂Σj∣Σj∣= 1∣Σj∣∣Σj∣⋅(Σj−1)T= Σj−1\begin{aligned} &amp; \dfrac{\partial}{\partial \Sigma_{j}} \log\left|\Sigma_{j}\right| \\ = \ \ &amp; \frac{1}{\left|\Sigma_{j}\right|} \frac{\partial}{\partial \Sigma_{j}} \left|\Sigma_{j}\right| \\ = \ \ &amp; \frac{1}{\left|\Sigma_{j}\right|} \left|\Sigma_{j}\right| \cdot (\Sigma_{j}^{-1})^T \\ = \ \ &amp; \Sigma_{j}^{-1} \end{aligned} = = = ​∂Σj​∂​log∣Σj​∣∣Σj​∣1​∂Σj​∂​∣Σj​∣∣Σj​∣1​∣Σj​∣⋅(Σj−1​)TΣj−1​​ and ∂∂Σj(xi−μj)TΣj−1(xi−μj)= (xi−μj)(xi−μj)T∂∂ΣjΣj−1= −(xi−μj)(xi−μj)TΣj−2\begin{aligned} &amp; \frac{\partial}{\partial \Sigma_{j}} (x_i-\mu_j)^T \Sigma_{j}^{-1}(x_i - \mu_j) \\ = \ \ &amp; (x_i-\mu_j)(x_i - \mu_j)^T \frac{\partial}{\partial \Sigma_{j}} \Sigma_{j}^{-1} \\ = \ \ &amp; - (x_i-\mu_j)(x_i - \mu_j)^T \Sigma_{j}^{-2} \\ \end{aligned} = = ​∂Σj​∂​(xi​−μj​)TΣj−1​(xi​−μj​)(xi​−μj​)(xi​−μj​)T∂Σj​∂​Σj−1​−(xi​−μj​)(xi​−μj​)TΣj−2​​ More about GMM Compared to K-Means which uses hard assignment, GMM uses soft assignment which a good way to represent uncertainty. But GMM is limited in its number of features since it requires storing a covariance matrix which has size quadratic in the number of features. Even when the number of features does not exceed this limit, this algorithm may perform poorly on high-dimensional data. This is due to high-dimensional data: making it difficult to cluster at all (based on statistical/theoretical arguments) numerical issues with Gaussian distributions]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Expectation-Maximization</tag>
        <tag>Gaussian Mixture Model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[无水葱油鸡]]></title>
    <url>%2F2019%2F05%2F14%2Fwu-shui-cong-you-ji%2F</url>
    <content type="text"><![CDATA[用时 1小时 必备厨具 Le Creuset 铸铁炖锅 原料 一只鸡（1.5kg） 盐 胡椒 葱 姜 香菜 花椒 小米椒 料酒 香油 生抽 或 蒸鱼豉油 做法 鸡去脖子和屁股，洗干净不必控水，里外抹上盐、黑胡椒， 腌制半小时。 姜切片，一部分葱切段，锅底抹一层油。放入姜片和葱段铺满锅底。另一部分葱和小米椒切碎备用。 将腌制后的童子鸡放入锅中，淋上三大勺料酒。 盖上锅盖，中火开始煮，煮至有蒸汽出来后，调整火力至小火。 小火20分钟后，如筷子可以轻松戳透鸡肉，说明鸡煮好了。 别关火，倒入少许生抽或者蒸鱼豉油。 倒入切碎的葱和小米椒，一把香菜，均匀撒在鸡身上。 炒锅加热，倒入适量油，加入一小撮花椒粒，加一勺香油，烧到滚烫冒烟，关火，花椒粒弃用。 把滚烫的油浇到鸡上，烫熟葱和小米椒。 成品]]></content>
      <categories>
        <category>Cuisine</category>
      </categories>
      <tags>
        <tag>Plat</tag>
        <tag>Poulet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google Hash Code 2019]]></title>
    <url>%2F2019%2F04%2F13%2Fgoogle-hash-code-2019%2F</url>
    <content type="text"><![CDATA[My thoughts on the qualification round of Google Hash Code 2019. If you are interested in the past Online Qualification Round and Finals problem statements, you should check this page. General information Based on the problem statement, your team of 4 need to solve 5 different cases of the problem. Each case is represented by an input text file. An output file should be generated for each input file and submitted to the Judge System. The Judge System will give a score for each output file. You can submit as many as you can, but only the highest score will be counted for each case. The summation of all the scores is the final score of the team. The top teams will be invited to the final round which will be held at Google Dublin on April 27, 2019. The problem is always NP-hard, which means that you can never find the best solution in polynomial time. Fortunately, you can make the result better and better. The greedy algorithm and dynamic programming are very helpful. Problem statement A slideshow contains a list of slides A slide contains either one horizontal photo or two vertical photos A photo contains a list of tags (string type) The tag set of a slide is the set of all tags of the photos in the slide If the slide containing 1 horizontal photos: S=tags(h)S = tags(h)S=tags(h) If the slide containing 2 vertical photos: S=tags(v1)∪tags(v2)S = tags(v_1) \cup tags(v_2)S=tags(v1​)∪tags(v2​) The slideshow is scored based on how interesting the transitions between each pair of subsequent slides are For two subsequent slides SiS_iSi​ and Si+1S_{i+1}Si+1​, the score is the minimum (the smallest number of the three) of: the number of common tags between SiS_iSi​ and Si+1S_{i+1}Si+1​ the number of tags in SiS_iSi​ but not in Si+1S_{i+1}Si+1​ the number of tags in Si+1S_{i+1}Si+1​ but not in SiS_iSi​ Score formula: min(∣Si∩Si+1∣,∣Si∖Si+1∣,∣Si+1∖Si∣)min(\vert S_i \cap S_{i+1} \vert, \vert S_i \setminus S_{i+1} \vert, \vert S_{i+1} \setminus S_i \vert)min(∣Si​∩Si+1​∣,∣Si​∖Si+1​∣,∣Si+1​∖Si​∣) For simplicity, horizontal slide refers to a slide containing 1 horizontal slide and vertical slide refers to a slide containing 2 vertical slides Solution First of all, let’s look at how the tags are distributed in the input files. This may help us understand the problem better. Orientation Metric A B C D E H # photos 2 80000 500 30000 NA H # distict tags 4 840000 1558 220 NA H # tags per photo 2.5 18.0 9.43 10.04 NA H # photos per tag 1.25 1.71 3.02 1369.09 NA V # photos 2 NA 500 60000 80000 V # distict tags 3 NA 1548 220 500 V # tags per photo 2 NA 9.52 10.01 19.09 V # photos per tag 1.3 NA 3.07 2732.06 3055.96 Some observations: B, D, E will come up with a large amount of score because there are many photos in these cases B contains horizontal photos only, which would be a good start point to try ideas B has many distinct tags which are far more than D, E In B and E, photos have twice more tags than D In term of photos per tag, D and E are thousands of times larger than B E contains vertical photos only which would be the last challenge Intuition Actually, we can consider the problem as building a sequence of slides and maximizing its overall transition score. Given the tag set of the previous slide, we can find a slide which maximizes the current transition. If every subsequent transition is optimized, we could eventually optimize the final score. Greedy Algorithm prevSlide &lt;- create a slide of 1 horizontal unused photo or 2 vertical unused photos result += prevSlide while some photos are unused: (hSlide, hScore) &lt;- Best horizontal slide w.r.t prevSlide (vSlide, vScore) &lt;- Best vertical slide w.r.t prevSlide if both hSlide and vSlide are found, then if hScove &gt;= vScore, then prevSlide &lt;- hScove else prevSlide &lt;- vScove else if hSlide not found, then prevSlide &lt;- vSlide else if vSlide not found, then prevSlide &lt;- hSlide else preSlide &lt;- create a slide of 1 horizontal unused photo or 2 vertical unused photos res += prevSlide done return result How to find the best horizontal slide w.r.t the previous slide? Obviously, we just iterate over the horizontal photos hih_ihi​ and take the photo with the highest score after the previous slide ppp. hi∗=argmax⁡hi∈Hscore(p,hi)h_{i}^{*} = \underset{h_i \in H}{\operatorname{argmax}} score(p, h_i) hi∗​=hi​∈Hargmax​score(p,hi​) Actually, we don’t need to search all the photos in the pool of horizontal photos hPool, instead, we only need to look for those having at least one common tag with the previous slide, otherwise, the score must be zero. The search space is restricted in those candidates Essentially, we can keep track of the photos sharing the same tag. We store this information in a HashMap[String, HashSet[Photo]] call hDict, which is a mapping from tag to a list of photos containing the tag. How to find the best vertical slide w.r.t the previous slide? This is the most tricky part of the problem. One solution: we can iterate over all the combination of 2 vertical photos and choose the best vertical slide as we did for horizontal slide. However, the time complexity for each step is very high, O(Nv2)O(N_v^2)O(Nv2​). If you compute the candidates size for each previous slides, we will find that it is, in average, about 7200 vertical photos in case D and 60000 in case E. The brute force solution may work for D, but never works for E. A more reasonable solution is heuristic. Given a previous slide, we can first take the best vertical photo, denoted as viv_ivi​, and then find the second vertical photo vjv_jvj​ maximizing the score between the previous slide ppp and the vertical slide of viv_ivi​ and vjv_jvj​. vi∗=argmax⁡vi∈Vscore(p,vi)v_{i}^{*} = \underset{v_i \in V}{\operatorname{argmax}} score(p, v_i) vi∗​=vi​∈Vargmax​score(p,vi​) vj∗=argmax⁡vj∈(V−vi∗)score(p,vi∗+vj)v_{j}^{*} = \underset{v_j \in (V - v_i^*)}{\operatorname{argmax}} score(p, v_{i}^{*} + v_j) vj∗​=vj​∈(V−vi∗​)argmax​score(p,vi∗​+vj​) Can we do better? We notice that the sequence can be built in parallel. More precisely, we can hash partition the input photos into MMM blocks. Each block of photos can be used to build a sub-sequence independently, and then all the sub-sequences could be joined into a sequence. This is a classical map-reduce pattern which cam largely improve the performance. Code You may find the code here (comments included) https://github.com/invkrh/google-hash-code/blob/master/src/main/scala/q2019/SlideShowSolver.scala Result With the heuristic algorithm and parallelization trick, we can obtain the following result. The final score is 1112255 which around top 20 in the qualification round, see here. Moreover, the time for D and E are no more than 5 mins which are also acceptable during the competition. Case: a_example Time: 119 ms Score: 2 Case: b_lovely_landscapes Time: 17569 ms Score: 201792 Case: c_memorable_moments Time: 165 ms Score: 1736 Case: d_pet_pictures Time: 228491 ms Score: 428041 Case: e_shiny_selfies Time: 294582 ms Score: 480684 Final score = 1112255 Conclusion The competition has been well organized as usual. However, I am a little disappointed at the problem itself. The provided data set is very easy to be cracked brute forced. One can just randomize the slide sequence several times and return the best one. Some teams did that and had a good result. Fortunately, these randomized brute force solutions are not good enough for the finalist. Anyway, this reminds me again the maxim Done is better than perfect. Auxiliary The CPU information of the computer on which the programme has been run. $ lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 8 On-line CPU(s) list: 0-7 Thread(s) per core: 2 Core(s) per socket: 4 Socket(s): 1 NUMA node(s): 1 Vendor ID: GenuineIntel CPU family: 6 Model: 94 Model name: Intel(R) Xeon(R) CPU E3-1270 v5 @ 3.60GHz Stepping: 3 CPU MHz: 800.015 CPU max MHz: 4000.0000 CPU min MHz: 800.0000 ...]]></content>
      <categories>
        <category>Competition</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Coding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow Specialization Learning Note]]></title>
    <url>%2F2019%2F03%2F11%2Ftensorflow-specialization-learning-note%2F</url>
    <content type="text"><![CDATA[A learning note of the coursera specialization TensorFlow: From Basics to Mastery given by deeplearning.ai. Course 1: Introduction to TensorFlow for AI, ML and DL Course 2: Convolutional Neural Networks in TensorFlow Coming soon … C1W1: A New Programming Paradigm Note New programming paradigm input output Triditional Programming Rules, Data Answers Machine Learning Answers, Data Rules Code How to fit a line import tensorflow as tf import numpy as np from tensorflow import keras model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])]) model.compile(optimizer=&#39;sgd&#39;, loss=&#39;mean_squared_error&#39;) xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float) ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float) model.fit(xs, ys, epochs=500) print(model.predict([10.0])) The predicted value is not 19.0 but a little under. It is because neural networks deal with probabilities, so given the data that we fed the NN with, it calculated that there is a very high probability that the relationship between XXX and YYY is Y=2X−1Y=2X-1Y=2X−1, but with only 6 data points we can’t know for sure. As a result, the result for 10 is very close to 19, but not necessarily 19. C1W2: Introduction to Computer Vision Note Why are the labels numbers instead of words Using a number is a first step in avoiding bias – instead of labelling it with words in a specific language and excluding people who don’t speak that language! You can learn more about bias and techniques to avoid it here. What is cross entropy (CE) CE=−∑i=0C−1yi⋅log(f(xi⃗))CE = - \sum_{i=0}^{C - 1} y_i \cdot log( f(\vec{x_i}) ) CE=−i=0∑C−1​yi​⋅log(f(xi​​)) where CCC: the number of classes xi⃗\vec{x_i}xi​​: the feature vector of the example iii yiy_iyi​: the label of the example iii fff: the learned prediction function which takes the feacture vector xi⃗\vec{x_i}xi​​ and returns the probability of being class yiy_iyi​ When c=2c = 2c=2 CE=−[yi⋅log(pi)+(1−yi)⋅log(1−pi)]CE = - \big[ y_i \cdot log( p_i ) + (1 - y_i) \cdot log( 1 - p_i ) \big] CE=−[yi​⋅log(pi​)+(1−yi​)⋅log(1−pi​)] Difference between categorical_crossentropy and sparse_categorical_crossentropy If your targets are one-hot encoded, use categorical_crossentropy. Examples of one-hot encodings: [1,0,0] [0,1,0] [0,0,1] But if your targets are integers, use sparse_categorical_crossentropy. Examples of integer encodings (for the sake of completion): 1 2 3 Code # Early stopping class myCallback(tf.keras.callbacks.Callback): def on_epoch_end(self, epoch, logs={}): if(logs.get(&#39;loss&#39;)&lt;0.4): print(&quot;\nReached 60% accuracy so cancelling training!&quot;) self.model.stop_training = True callbacks = myCallback() mnist = tf.keras.datasets.fashion_mnist (training_images, training_labels), (test_images, test_labels) = mnist.load_data() # Data normalization training_images = training_images &#x2F; 255.0 test_images = test_images &#x2F; 255.0 model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=tf.nn.relu), tf.keras.layers.Dense(10, activation=tf.nn.softmax)]) model.compile(optimizer = &#39;adam&#39;, loss = &#39;sparse_categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks]) model.evaluate(test_images, test_labels) C1W3: Enhancing Vision with Convolutional Neural Networks Note Convolution Layer Each kernal is an edge detector which is perfect for computer vision, because often it’s features that can get highlighted like this that distinguish one item for another, and the amount of information needed is then much less…because you’ll just train on the highlighted features. MaxPooling Layer The convolution layer is followed by a MaxPooling layer which is then designed to compress the image, while maintaining the content of the features that were highlighted by the convolution Why CNN works CNN tries different filters on the image and learning which ones work when looking at the training data. As a result, when it works, you’ll have greatly reduced information passing through the network, but because it isolates and identifies features, you can also get increased accuracy Code Model # Reshape to a 4D tensor, otherwise the Convolutions do not recognize the shape training_images=training_images.reshape(60000, 28, 28, 1) training_images=training_images &#x2F; 255.0 test_images = test_images.reshape(10000, 28, 28, 1) test_images=test_images&#x2F;255.0 # 2-convolution-layer NN model = tf.keras.models.Sequential([ # default: strides = 1, padding = &#39;valid&#39; tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;, input_shape=(28, 28, 1)), # default: strides = None (same as pool_size), padding = &#39;valid&#39; tf.keras.layers.MaxPooling2D(2, 2), tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(128, activation=&#39;relu&#39;), tf.keras.layers.Dense(10, activation=&#39;softmax&#39;) ]) _________________________________________________________________ || Layer (type) Output Shape Param # || Comments ================================================================= || conv2d (Conv2D) (None, 26, 26, 64) 640 || = 64 x (3 x 3 x 1 + 1) _________________________________________________________________ || max_pooling2d (MaxPooling2D) (None, 13, 13, 64) 0 || _________________________________________________________________ || conv2d_1 (Conv2D) (None, 11, 11, 64) 36928 || = 64 x (3 x 3 x 64 + 1) _________________________________________________________________ || max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64) 0 || _________________________________________________________________ || flatten_1 (Flatten) (None, 1600) 0 || _________________________________________________________________ || dense_2 (Dense) (None, 128) 204928 || = 128 x (1600 + 1) _________________________________________________________________ || dense_3 (Dense) (None, 10) 1290 || = 10 * (128 + 1) ================================================================= || Total params: 243,786 Trainable params: 243,786 Non-trainable params: 0 How to compute output size Convolution layer (n+2p−f+1)×(n+2p−f+1)(n + 2p - f + 1) \times (n + 2p - f + 1) (n+2p−f+1)×(n+2p−f+1) MaxPooling layer Floor(height−fs+1)×Floor(weight−fs+1)Floor(\frac{height - f}{s} + 1) \times Floor(\frac{weight - f}{s} + 1) Floor(sheight−f​+1)×Floor(sweight−f​+1) nnn: input size ppp: padding size fff: filter size Two kinds of padding: Valid: no padding p=0p = 0 p=0 Same: results in padding the input such that the output has the same length as the original input n+2p−f+1=n ⟹ p=(f−1)/2n + 2p - f + 1 = n \implies p = (f - 1) / 2 n+2p−f+1=n⟹p=(f−1)/2 where fff is almost always odd number How to compute number of parameters NF×(f×f×NCinput+1)NF \times (f \times f \times NC_{input} + 1 ) NF×(f×f×NCinput​+1) NFNFNF: number of filters NCinputNC_{input}NCinput​: number of input channels Each filter has a bias term Convolutions Over Volume Visualizing the Convolutions and Pooling Each row represents an itea. There are 3 shoes images here. The 4 columns represent the output of the first 4 layers (conv2d, max_pooling2d, conv2d_1, max_pooling2d_1). We can find the commonality for the same kind of items. C1W4: Using Real-world Images Note ImageGenerator ImageGenerator can flow images from a directory and perform operations such as resizing them on the fly. You can point it at a directory and then the sub-directories of that will automatically generate labels for you images |-- training | |-- horse | | |-- 1.jpg | | |-- 2.jpg | | `-- 3.jpg | `-- human | |-- 1.jpg | |-- 2.jpg | `-- 3.jpg `-- validation |-- horse | |-- 1.jpg | |-- 2.jpg | `-- 3.jpg `-- human |-- 1.jpg |-- 2.jpg `-- 3.jpg If you point ImageGenerator to training directory, it will generate a stream of images labelled with horse or human Mini-batch Why mini-batch For large neural networks with very large and highly redundant training sets, it is nearly always best to use mini-batch learning. The mini-batches may need to be quite big when adapting fancy methods. Big mini-batches are more computationally efficient. Optimization Momentum RMSProp Adam Code Model import tensorflow as tf from tensorflow.keras.optimizers import RMSprop model = tf.keras.models.Sequential([ # Note the input shape is the desired size of the image 300x300 with 3 bytes color # This is the first convolution tf.keras.layers.Conv2D(16, (3,3), activation=&#39;relu&#39;, input_shape=(300, 300, 3)), tf.keras.layers.MaxPooling2D(2, 2), # The second convolution tf.keras.layers.Conv2D(32, (3,3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2,2), # The third convolution tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2,2), # The fourth convolution tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2,2), # The fifth convolution tf.keras.layers.Conv2D(64, (3,3), activation=&#39;relu&#39;), tf.keras.layers.MaxPooling2D(2,2), # Flatten the results to feed into a DNN tf.keras.layers.Flatten(), # 512 neuron hidden layer tf.keras.layers.Dense(512, activation=&#39;relu&#39;), # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class (&#39;horses&#39;) and 1 for the other (&#39;humans&#39;) tf.keras.layers.Dense(1, activation=&#39;sigmoid&#39;) ]) # Train our model with the binary_crossentropy loss, # because it&#39;s a binary classification problem and our final activation is a sigmoid. # [More details](http:&#x2F;&#x2F;www.cs.toronto.edu&#x2F;~tijmen&#x2F;csc321&#x2F;slides&#x2F;lecture_slides_lec6.pdf) model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=RMSprop(lr=0.001), metrics=[&#39;acc&#39;]) model.summary() Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 298, 298, 16) 448 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 149, 149, 16) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 147, 147, 32) 4640 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 73, 73, 32) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 71, 71, 64) 18496 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 35, 35, 64) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 33, 33, 64) 36928 _________________________________________________________________ max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64) 0 _________________________________________________________________ conv2d_4 (Conv2D) (None, 14, 14, 64) 36928 _________________________________________________________________ max_pooling2d_4 (MaxPooling2 (None, 7, 7, 64) 0 _________________________________________________________________ flatten (Flatten) (None, 3136) 0 _________________________________________________________________ dense (Dense) (None, 512) 1606144 _________________________________________________________________ dense_1 (Dense) (None, 1) 513 ================================================================= Total params: 1,704,097 Trainable params: 1,704,097 Non-trainable params: 0 The convolutions reduce the shape from 90000 (300 x 300) down to 3136 ImageDataGenerator # All images will be rescaled by 1.&#x2F;255 train_datagen = ImageDataGenerator(rescale=1&#x2F;255) validation_datagen = ImageDataGenerator(rescale=1&#x2F;255) # Flow training images in batches of 128 using train_datagen generator train_generator = train_datagen.flow_from_directory( &#39;&#x2F;tmp&#x2F;horse-or-human&#x2F;&#39;, # This is the source directory for training images target_size=(300, 300), # All images will be resized to 150x150 batch_size=128, # number of images for each batch # Since we use binary_crossentropy loss, we need binary labels class_mode=&#39;binary&#39;) # Flow training images in batches of 128 using train_datagen generator validation_generator = validation_datagen.flow_from_directory( &#39;&#x2F;tmp&#x2F;validation-horse-or-human&#x2F;&#39;, # This is the source directory for validation images target_size=(300, 300), # All images will be resized to 150x150 batch_size=32, # number of images for each batch # Since we use binary_crossentropy loss, we need binary labels class_mode=&#39;binary&#39;) history = model.fit_generator( train_generator, steps_per_epoch=8, # number of batches for each epoch durning training epochs=15, verbose=1, validation_data = validation_generator, validation_steps=8) # number of batches for each epoch durning validation Visualizing Intermediate Representations As you can see we go from the raw pixels of the images to increasingly abstract and compact representations. The representations downstream start highlighting what the network pays attention to, and they show fewer and fewer features being “activated”; most are set to zero. This is called “sparsity.” Representation sparsity is a key feature of deep learning. These representations carry increasingly less information about the original pixels of the image, but increasingly refined information about the class of the image. You can think of a convnet (or a deep network in general) as an information distillation pipeline. C2W1: Exploring a Larger Dataset Note Data: https://www.kaggle.com/c/dogs-vs-cats model.layers API allows you to inspect the impact of convolutions on the images. Code import numpy as np import random from tensorflow.keras.preprocessing.image import img_to_array, load_img # Let&#39;s define a new Model that will take an image as input, and will output # intermediate representations for all layers in the previous model after # the first. successive_outputs = [layer.output for layer in model.layers[1:]] #visualization_model = Model(img_input, successive_outputs) visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs) # Let&#39;s prepare a random input image of a cat or dog from the training set. cat_img_files = [os.path.join(train_cats_dir, f) for f in train_cat_fnames] dog_img_files = [os.path.join(train_dogs_dir, f) for f in train_dog_fnames] img_path = random.choice(cat_img_files + dog_img_files) img = load_img(img_path, target_size=(150, 150)) # this is a PIL image x = img_to_array(img) # Numpy array with shape (150, 150, 3) x = x.reshape((1,) + x.shape) # Numpy array with shape (1, 150, 150, 3) # Rescale by 1&#x2F;255 x &#x2F;= 255.0 # Let&#39;s run our image through our network, thus obtaining all # intermediate representations for this image. successive_feature_maps = visualization_model.predict(x) # These are the names of the layers, so can have them as part of our plot layer_names = [layer.name for layer in model.layers] # ----------------------------------------------------------------------- # Now let&#39;s display our representations # ----------------------------------------------------------------------- for layer_name, feature_map in zip(layer_names, successive_feature_maps): if len(feature_map.shape) == 4: #------------------------------------------- # Just do this for the conv &#x2F; maxpool layers, not the fully-connected layers #------------------------------------------- n_features = feature_map.shape[-1] # number of features in the feature map size = feature_map.shape[ 1] # feature map shape (1, size, size, n_features) # We will tile our images in this matrix display_grid = np.zeros((size, size * n_features)) #------------------------------------------------- # Postprocess the feature to be visually palatable #------------------------------------------------- for i in range(n_features): x = feature_map[0, :, :, i] x -= x.mean() x &#x2F;= x.std () x *= 64 x += 128 x = np.clip(x, 0, 255).astype(&#39;uint8&#39;) display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid #----------------- # Display the grid #----------------- scale = 20. &#x2F; n_features plt.figure( figsize=(scale * n_features, scale) ) plt.title ( layer_name ) plt.grid ( False ) plt.imshow( display_grid, aspect=&#39;auto&#39;, cmap=&#39;viridis&#39; ) C2W2: Augmentation: A technique to avoid overfitting Note Image augmentation Image augmentation implementation in Keras: https://keras.io/preprocessing/image/ Image generator library lets you load the images into memory, process the images and then steam that to the training set to the neural network we will ultimatedly learn on.The preprocessing doesn’t require you to edit your raw images, nor does it amend them for you on-disk. It does it in-memory as it’s performing the training, allowing you to experiment without impacting your dataset. As we start training, we’ll initially see that the accuracy is lower than with the non-augmented version. This is because of the random effects of the different image processing that’s being done. As it runs for a few more epochs, you’ll see the accuracy slowly climbing. The image augmentation introduces a random element to the training images but if the validation set doesn’t have the same randomness, then its results can fluctuate. You don’t just need a broad set of images for training, you also need them for testing or the image augmentation won’t help you very much.(which does NOT mean that you should augment your validation set, see below) Validation dataset should not be augmented: the validation set is used to estimate how your method works on real world data, thus it should only contain real world data. Adding augmented data will not improve the accuracy of the validation. It will at best say something about how well your method responds to the data augmentation, and at worst ruin the validation results and interpretability. As the validation accuracy is no longer a good proxy for the accuracy on new unseen data if you augment the validation data Code train_datagen = ImageDataGenerator( rescale=1.&#x2F;255, rotation_range=40, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode=&#39;nearest&#39;) C2W3: Transfer Learning Note What is transfer learning You can take an existing model, freeze many of its layers to prevent them being retrained, and effectively ‘remember’ the convolutions it was trained on to fit images, then added your own DNN underneath this so that you could retrain on your images using the convolutions from the other model. Why dropout can do the regularization The idea behind Dropouts is that they remove a random number of neurons in your neural network. This works very well for two reasons: The first is that neighboring neurons often end up with similar weights, which can lead to overfitting, so dropping some out at random can remove this. The second is that often a neuron can over-weigh the input from a neuron in the previous layer, and can over specialize as a result. It can not rely on any of the input which will be randomly dropped, instead, it will spread the weights, by which the weights will be shrinked. Code from tensorflow.keras import layers from tensorflow.keras import Model from tensorflow.keras.optimizers import RMSprop from tensorflow.keras.applications.inception_v3 import InceptionV3 local_weights_file = &#39;&#x2F;tmp&#x2F;inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5&#39; pre_trained_model = InceptionV3(input_shape = (150, 150, 3), include_top = False, # whether to include the fully-connected layer at the top of the network. weights = None) # one of None (random initialization) or &#39;imagenet&#39; (pre-training on ImageNet). for layer in pre_trained_model.layers: layer.trainable = False last_layer = pre_trained_model.get_layer(&#39;mixed7&#39;) last_output = last_layer.output # Flatten the output layer to 1 dimension x = layers.Flatten()(last_output) # Add a fully connected layer with 1,024 hidden units and ReLU activation x = layers.Dense(1024, activation=&#39;relu&#39;)(x) # Add a dropout rate of 0.2 x = layers.Dropout(0.2)(x) # Add a final sigmoid layer for classification x = layers.Dense (1, activation=&#39;sigmoid&#39;)(x) model = Model( pre_trained_model.input, x) model.compile(optimizer = RMSprop(lr=0.0001), loss = &#39;binary_crossentropy&#39;, metrics = [&#39;acc&#39;]) C2W4: Multiclass Classification Note Use CGI to generate images for Rock, Paper, Scissors Code train_generator = training_datagen.flow_from_directory( TRAINING_DIR, target_size=(150,150), class_mode=&#39;categorical&#39; ) # Same for validation model = tf.keras.models.Sequential([ # Convolution layers # ... # Flatten the results to feed into a DNN tf.keras.layers.Flatten(), tf.keras.layers.Dropout(0.5), # 512 neuron hidden layer tf.keras.layers.Dense(512, activation=&#39;relu&#39;), # 3 nodes with softmax tf.keras.layers.Dense(3, activation=&#39;softmax&#39;) ]) Another way of using fit_generator API via (images, labels), instead of via directory history = model.fit_generator(train_datagen.flow(training_images, training_labels, batch_size=32), steps_per_epoch=len(training_images) &#x2F; 32, epochs=15, validation_data=validation_datagen.flow(testing_images, testing_labels, batch_size=32), validation_steps=len(testing_images) &#x2F; 32)]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>Coding</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[蒜香蜂蜜鸡]]></title>
    <url>%2F2019%2F02%2F23%2Fsuan-xiang-feng-mi-ji%2F</url>
    <content type="text"><![CDATA[用时 30分钟 原料 四块鸡上腿肉 盐 胡椒 四瓣蒜 蜂蜜（2） 米醋（1） 酱油（1） 大蒜粉（opt） 白芝麻（opt） 葱（opt） 做法 鸡肉洗干净，抹上盐，黑胡椒，大蒜粉，腌制10分钟。 四瓣蒜做成蒜末，最好蒜泥。 用中大火预热不粘锅，不用放一滴油（鸡皮出油）。鸡皮朝下放入锅中，每2分钟翻面，共8分钟。 锅中央放入蒜泥、蜂蜜、米醋、酱油、80毫升水。火开大点。 收汁，翻面，等汁浓稠，出锅（可以撒上白芝麻和葱）。 成品]]></content>
      <categories>
        <category>Cuisine</category>
      </categories>
      <tags>
        <tag>Plat</tag>
        <tag>Poulet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interview Questions in 2018]]></title>
    <url>%2F2018%2F12%2F19%2Fintervew-questions-in-2018%2F</url>
    <content type="text"><![CDATA[A list of coding interview questions that I was asked in 2018. Question 1: Add Binary Statement (leetcode: 791) Given two binary strings, return their sum (also a binary string). The input strings are both non-empty and contains only characters 1 or 0. Example 1: Input: a = &quot;11&quot;, b = &quot;1&quot; Output: &quot;100&quot; Example 2: Input: a = &quot;1010&quot;, b = &quot;1011&quot; Output: &quot;10101&quot; Solution String addBinary(String a, String b) { int i = a.length() - 1, j = b.length() - 1; int carry = 0; StringBuilder sb = new StringBuilder(); while (i &gt;= 0 || j &gt;= 0) { int sum = carry; if (i &gt;= 0) { sum += a.charAt(i) - &#39;0&#39;; i--; } if (j &gt;= 0) { sum += b.charAt(j) - &#39;0&#39;; j--; } carry = sum &#x2F; 2; sb.insert(0, sum % 2); } if (carry != 0) sb.insert(0, carry); return sb.toString(); } Complexity Comments Time O(n) / Space O(n) StringBuilder Extention What if the given strings can be numbers of any base ? String addBinary(String a, String b) { int i = a.length() - 1, j = b.length() - 1; int carry = 0; StringBuilder sb = new StringBuilder(); while (i &gt;= 0 || j &gt;= 0) { int sum = carry; if (i &gt;= 0) { sum += a.charAt(i) - &#39;0&#39;; i--; } if (j &gt;= 0) { sum += b.charAt(j) - &#39;0&#39;; j--; } carry = sum &#x2F; 2; sb.insert(0, sum % 2); } if (carry != 0) sb.insert(0, carry); return sb.toString(); } Question 2: cd command Statement Write a function to simluate linux command cd Example 1: Input: cur = &quot;&#x2F;etc&quot;, path = &quot;&#x2F;bin&#x2F;&quot; Output: &quot;&#x2F;bin&quot; Example 2: Input: a = &quot;&#x2F;etc&quot;, b = &quot;hadoop&quot; Output: &quot;&#x2F;etc&#x2F;hadoop&quot; Example 3: Input: a = &quot;&#x2F;etc&#x2F;hadoop&#x2F;conf&quot;, b = &quot;..&#x2F;..&#x2F;hive&quot; Output: &quot;&#x2F;etc&#x2F;hive&quot; Example 4: Input: a = &quot;&#x2F;etc&#x2F;hadoop&#x2F;conf&quot;, b = &quot;..&#x2F;.&#x2F;conf&quot; Output: &quot;&#x2F;etc&#x2F;hadoop&#x2F;conf&quot; Solution String cd(String cur, String path) { if (path.startsWith(&quot;&#x2F;&quot;)) return path; Stack&lt;String&gt; stack = new Stack&lt;&gt;(); for (String dir : cur.split(&quot;&#x2F;&quot;)) if (!dir.isEmpty()) stack.push(dir); for (String dir : path.split(&quot;&#x2F;&quot;)) if (dir.equals(&quot;..&quot;)) { if (!stack.isEmpty()) stack.pop(); } else if (!dir.equals(&quot;.&quot;)) { stack.push(dir); } String res = String.join(&quot;&#x2F;&quot;, stack); return res.startsWith(&quot;&#x2F;&quot;) ? res : &quot;&#x2F;&quot; + res; } Complexity Comments Time O(n) / Space O(n) Stack Question 3: Custom Sort String Statement (leetcode: 791) S and T are strings composed of lowercase letters. In S, no letter occurs more than once. S was sorted in some custom order previously. We want to permute the characters of T so that they match the order that S was sorted. More specifically, if x occurs before y in S, then x should occur before y in the returned string. Return any permutation of T (as a string) that satisfies this property. Example : Input: S = &quot;cba&quot;, T = &quot;abcd&quot; Output: &quot;cbad&quot; Explanation: &quot;a&quot;, &quot;b&quot;, &quot;c&quot; appear in S, so the order of &quot;a&quot;, &quot;b&quot;, &quot;c&quot; should be &quot;c&quot;, &quot;b&quot;, and &quot;a&quot;. Since &quot;d&quot; does not appear in S, it can be at any position in T. &quot;dcba&quot;, &quot;cdba&quot;, &quot;cbda&quot; are also valid outputs. Note: S has length at most 26, and no character is repeated in S. T has length at most 200. S and T consist of lowercase letters only. Solution public String customSortString(String S, String T) { int[] dict = new int[26]; for (char c : T.toCharArray()) { dict[c - &#39;a&#39;] += 1; } StringBuilder sb = new StringBuilder(); for (char c : S.toCharArray()) { for (int i = 0; i &lt; dict[c - &#39;a&#39;]; i++) sb.append(c); dict[c - &#39;a&#39;] = 0; } for (char c = &#39;a&#39;; c &lt;= &#39;z&#39;; c++) for (int i = 0; i &lt; dict[c - &#39;a&#39;]; i++) sb.append(c); return sb.toString(); } Complexity Comments Time O(n) / Space O(n) StringBuilder Question 4: Position of the leftmost one Statement Given a binary matrix (containing only 0 and 1) of order n * n. All rows are sorted already. We need to find position of the left most 1. Note: in case of tie, return the position of the smallest row number. Example: Input matrix 0 1 1 1 0 0 1 1 1 1 1 1 &#x2F;&#x2F; this row has maximum 1s 0 0 0 0 Output: [2, 0] Solution int[] findPosition(int[][] matrix) { int r = matrix.length; if (r == 0) return null; int c = matrix[0].length; if (c == 0) return null; int[] res = new int[] {}; int j = c - 1; for (int i = 0; i &lt; r; i++) { while (j &gt;= 0 &amp;&amp; matrix[i][j] == 1) { j--; res = new int[] {i, j + 1}; } } return res; } Complexity Comments Time O(r + c) ends on the boundary Space O(1) / Question 5: Validate Binary Search Tree Statement (leetcode: 98) Given a binary tree, determine if it is a valid binary search tree (BST). Assume a BST is defined as follows: The left subtree of a node contains only nodes with keys less than the node’s key. The right subtree of a node contains only nodes with keys greater than the node’s key. Both the left and right subtrees must also be binary search trees. Example 1: Input: 2 &#x2F; \ 1 3 Output: true Example 2: 5 &#x2F; \ 1 4 &#x2F; \ 3 6 Output: false Explanation: The input is: [5,1,4,null,null,3,6]. The root node&#39;s value is 5 but its right child&#39;s value is 4. Solution boolean validate(TreeNode node, long min, long max) { if (node == null) { return true; } else { if (node.val &gt; min &amp;&amp; node.val &lt; max) { return validate(node.left, min, node.val) &amp;&amp; validate(node.right, node.val, max); } else { return false; } } } boolean isValidBST(TreeNode root) { return validate(root, Long.MIN_VALUE, Long.MAX_VALUE); } Complexity Comments Time O(n) visit all the nodes Space O(log n) recursice call stack Question 6: Search word in the dictionary Statement (leetcode: 211) Design a data structure that supports the following two operations: class WordDictionary { &#x2F;** Initialize data structure *&#x2F; public WordDictionary() &#x2F;** Adds a word into the data structure. *&#x2F; public void addWord(String word) &#x2F;** Returns if the word is in the data structure. A word could contain the dot character &#39;.&#39; to represent any one letter. *&#x2F; public boolean search(String word) } void addWord(word) bool search(word) search(word) can search a literal word or a regular expression string containing only letters a-z or .. A . means it can represent any one letter. Example: addWord(&quot;bad&quot;) addWord(&quot;dad&quot;) addWord(&quot;mad&quot;) search(&quot;pad&quot;) -&gt; false search(&quot;bad&quot;) -&gt; true search(&quot;.ad&quot;) -&gt; true search(&quot;b..&quot;) -&gt; true Solution class WordDictionary { class TrieNode { TrieNode[] next = new TrieNode[26]; String word = null; } TrieNode root; public WordDictionary() { this.root = new TrieNode(); } &#x2F;** Adds a word into the data structure. *&#x2F; public void addWord(String word) { TrieNode node = root; for (int i = 0; i &lt; word.length(); i++) { char c = word.charAt(i); if (node.next[c - &#39;a&#39;] == null) node.next[c - &#39;a&#39;] = new TrieNode(); node = node.next[c - &#39;a&#39;]; } node.word = word; } &#x2F;** Returns if the word is in the data structure. A word could contain the dot character &#39;.&#39; to represent any one letter. *&#x2F; public boolean search(String word) { return match(word, 0, root); } private boolean match(String word, int i, TrieNode node) { if (i == word.length()) return node.word != null; char c = word.charAt(i); if (c == &#39;.&#39;) { for (TrieNode nextNode : node.next) { if (nextNode != null &amp;&amp; match(word, i + 1, nextNode)) { return true; } } return false; } else { TrieNode nextNode = node.next[c - &#39;a&#39;]; return nextNode != null &amp;&amp; match(word, i + 1, nextNode); } } } add Complexity Comments Time O(n) / Space O(n) node creation search Complexity Comments Time O(n) / Space O(n) recursive call stack Question 7: Valid Palindrome Statement (leetcode: 125) Given a string, determine if it is a palindrome, considering only alphanumeric characters and ignoring cases. Note: For the purpose of this problem, we define empty string as valid palindrome. Example 1: Input: &quot;A man, a plan, a canal: Panama&quot; Output: true Example 2: Input: &quot;race a car&quot; Output: false Solution boolean isValid(char c) { return (c &gt;= &#39;a&#39; &amp;&amp; c &lt;= &#39;z&#39;) || (c &gt;= &#39;A&#39; &amp;&amp; c &lt;= &#39;Z&#39;) || (c &gt;= &#39;0&#39; &amp;&amp; c &lt;= &#39;9&#39;); } boolean isPalindrome(String s) { int i = 0; int j = s.length() - 1; while(i &lt;= j) { if (!isValid(s.charAt(i))) { i++; continue; } if (!isValid(s.charAt(j))) { j--; continue; } if (Character.toLowerCase(s.charAt(i)) == Character.toLowerCase(s.charAt(j))) { i++; j--; } else { return false; } } return true; } Complexity Comments Time O(n) / Space O(1) / Question 8: Shortest Distance To All Stations Statement Given a metro map of London, find the station which is closest to all the others stations. Solution (Floyd–Warshall algorithm) &#x2F;** graph is a weighted undirected adjacency matrix *&#x2F; int solve(double[][] graph) { int n = graph.length; double[][] dist = new double[n][n]; for (int i = 0; i &lt; n; i++) { for (int j = 0; j &lt; n; j++) { dist[i][j] = graph[i][j]; } } &#x2F;** Floyd–Warshall algorithm *&#x2F; for (int k = 0; k &lt; n; k++) { for (int i = 0; i &lt; n; i++) { for (int j = 0; j &lt; n; j++) { if (dist[i][j] &gt; dist[i][k] + dist[k][j]) { dist[i][j] = dist[i][k] + dist[k][j]; } } } } double min = Integer.MAX_VALUE; int res = -1; for (int i = 0; i &lt; n; i++) { double sum = 0 for (double d : dist[i]) sum += d; if (sum &lt; min) { res = i; min = sum; } } return res; } Complexity Comments Time O(n ^ 3) / Space O(n ^ 2) / Question 9: Equilibrium Point Statement (leetcode: 724) Given an array of integers nums, write a method that returns the “pivot” index of this array. We define the pivot index as the index where the sum of the numbers to the left of the index is equal to the sum of the numbers to the right of the index. If no such index exists, we should return -1. If there are multiple pivot indexes, you should return the left-most pivot index. Example 1: Input: nums = [1, 7, 3, 6, 5, 6] Output: 3 Explanation: The sum of the numbers to the left of index 3 (nums[3] = 6) is equal to the sum of numbers to the right of index 3. Also, 3 is the first index where this occurs. Example 2: Input: nums = [1, 2, 3] Output: -1 Explanation: There is no index that satisfies the conditions in the problem statement. Solution pivotIndex(int[] nums) { int sum = 0, leftsum = 0; for (int x: nums) sum += x; for (int i = 0; i &lt; nums.length; ++i) { if (leftsum == sum - leftsum - nums[i]) return i; leftsum += nums[i]; } return -1; } Complexity Comments Time O(n) / Space O(1) / Question 10: Complete Binary Tree Statement Given a complete binary tree in which each node marked with a number in level order (root = 1) and several connections are removed. Find if the given number is still reachable from the root of the tree. Example 1: Input: tree = root, num = 5 1 -&gt; root &#x2F; \ &#x2F; \ &#x2F; \ &#x2F; \ &#x2F; \ 2 3 &#x2F; &#x2F; \ &#x2F; &#x2F; \ 4 5 6 7 &#x2F; \ &#x2F; \ &#x2F; \ &#x2F; \ 8 9 10 11 12 13 14 15 Output: false Example 2: Input: tree = root, num = 6 1 -&gt; root \ \ \ \ \ 2 3 &#x2F; \ &#x2F; \ &#x2F; \ &#x2F; \ 4 5 6 7 &#x2F; \ &#x2F; \ &#x2F; \ &#x2F; \ 8 9 10 11 12 13 14 15 Output: true Solution boolean findInCompleteTree(TreeNode root, int n) { List&lt;Boolean&gt; path = new LinkedList&lt;&gt;(); while (n &gt; 1) { if (n % 2 == 0) { path.add(0, true); } else { path.add(0, false); } n &#x2F;= 2; } for (boolean p : path) { if (p) root = root.left; else root = root.right; if (root == null) return false; } return true; } Complexity Comments Time O(log n) / Space O(log n) / Extension (leetcode: 222) Count the number of node in a complete binary tree. Example 1: Input: tree = root 1 -&gt; root &#x2F; \ &#x2F; \ &#x2F; \ &#x2F; \ &#x2F; \ 2 3 &#x2F; \ &#x2F; \ &#x2F; \ &#x2F; \ 4 5 6 7 &#x2F; \ &#x2F; \ &#x2F; 8 9 10 11 12 Output: 12 int countInCompleteTree(TreeNode root) { TreeNode node = root; int depthLeft = 0; while (node != null) { depthLeft++; node = node.left; } node = root; int depthRight = 0; while (node != null) { depthRight++; node = node.right; } return depthLeft == depthRight ? (1 &lt;&lt; depthLeft) - 1 : 1 + countInCompleteTree(root.left) + countInCompleteTree(root.right); } Complexity Comments Time O(log n * log n) log n calls and each call takes log n to compute depth Space O(log n) recursive call stack Question 11: UTF-8 Encoding Statement A character in UTF8 can be from 1 to 4 bytes long, subjected to the following rules: For 1-byte character, the first bit is a 0, followed by its unicode code. For n-bytes character, the first n-bits are all one’s, the n+1 bit is 0, followed by n-1 bytes with most significant 2 bits being 10. This is how the UTF-8 encoding would work: Number of bytes Bits for code point First code point Last code point Byte 1 Byte 2 Byte 3 Byte 4 1 7 U+0000 U+007F 0xxxxxxx 2 11 U+0080 U+07FF 110xxxxx 10xxxxxx 3 16 U+0800 U+FFFF 1110xxxx 10xxxxxx 10xxxxxx 4 21 U+10000 U+10FFFF 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx Given a byte array which contains only UTF-8 encoded characters and an integer limit, return the max number of bytes contains only valid UTF-8 encordings in the first limit bytes. Example 1: Input: stream = | 0xxxxxxx | 110xxxxx | 10xxxxxx | 1110xxxx | 10xxxxxx | 10xxxxxx | 11110xxx | 10xxxxxx ||| 10xxxxxx | 10xxxxxx | limit = 8 Output: 5 Example 2: Input: stream = | 0xxxxxxx | 110xxxxx | 10xxxxxx | limit = 5 Output: 2 Solution int countUTF8Byte(byte[] stream, int limit) { if (stream.length &lt;= limit) { return stream.length; } else { while (limit &gt; 0 &amp;&amp; (stream[limit] &amp; 0xFF) &gt;&gt; 6 == 2) { limit--; } return limit; } } Complexity Comments Time O(1) No more than 6 bytes Space O(1) / Question 12: Design Rate limiter Statement (inspired by leetcode: 362) Design rate limiter API based on the count limit per minute and per hour. The granularity of timestamp is in second if needed. class RateLimiter { &#x2F;** Initialize data structure *&#x2F; public RateLimiter(long minuteCount, long hourCount) &#x2F;** Return true if the function calls exceeded either minuteCount or hourCount, otherwise return false *&#x2F; public boolean isLimited() } RateLimiter rl = new RateLimit(100, 6000); rl.isLimited() &#x2F;&#x2F; return false; Solution public class RateLimiter { class HitCounter { private int numBucket; private int[] time; private int[] hit; public HitCounter(int numBucket) { this.numBucket = numBucket; this.time = new int[numBucket]; this.hit = new int[numBucket]; } public void hit(int ts) { int bucket = ts % this.numBucket; if (time[bucket] == ts) { hit[bucket]++; } else { time[bucket] = ts; hit[bucket] = 1; } } public int count(int ts) { int cnt = 0; for (int i = 0; i &lt; this.numBucket; i++) { if (ts - time[i] &lt; this.numBucket) { cnt += hit[i]; } } return cnt; } } private long minuteLimit; private long hourLimit; private HitCounter minuteCounter; private HitCounter hourCounter; public RateLimiter(long minuteLimit, long hourLimit) { this.minuteLimit = minuteLimit; this.hourLimit = hourLimit; this.minuteCounter = new HitCounter(60); this.hourCounter = new HitCounter(3600); } public boolean isLimited() { int tsInSec = (int) (System.currentTimeMillis() &#x2F; 1000); if (this.minuteCounter.count(tsInSec) &lt; this.minuteLimit &amp;&amp; this.hourCounter.count(tsInSec) &lt; this.hourLimit) { minuteCounter.hit(tsInSec); hourCounter.hit(tsInSec); return false; } else { return true; } } public static void main(String[] args) throws InterruptedException { RateLimiter rl = new RateLimiter(10, 600); int count = 0; while (true) { Thread.sleep(1000); if (rl.isLimited()) { break; } else { count++; System.out.println(&quot;Limit not reached: &quot; + count); } } System.out.println(&quot;Limit exceeded: &quot; + count); } } hit Complexity Comments Time O(1) / Space O(n) number of the buckets count Complexity Comments Time O(n) number of the buckets Space O(n) number of the buckets Question 13: Design Task Scheduler (cron) Statement Implement the following 3 methods. Start with scheduling part and then execution part. public class CronScheduler { void schedule(TimerTask task, long delay) {} void repeat(TimerTask t, long delay, long period) {} void daily(TimerTask t, long delay) {} } Solution Reference: java.util.Timer and java.util.TimerTask &#x2F;&#x2F; TODO]]></content>
      <categories>
        <category>Interview</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Coding</tag>
      </tags>
  </entry>
</search>
